<!DOCTYPE html>



  


<html class="theme-next gemini use-motion" lang="zh-Hans">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">



  
  
    
    
  <script src="/lib/pace/pace.min.js?v=1.0.2"></script>
  <link href="/lib/pace/pace-theme-center-circle.min.css?v=1.0.2" rel="stylesheet">
  <style>
    .pace .pace-progress {
        background: #181f25; /*进度条颜色*/
    }
  </style>







<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">







  <script>
  (function(i,s,o,g,r,a,m){i["DaoVoiceObject"]=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;a.charset="utf-8";m.parentNode.insertBefore(a,m)})(window,document,"script",('https:' == document.location.protocol ? 'https:' : 'http:') + "//widget.daovoice.io/widget/0f81ff2f.js","daovoice")
  daovoice('init', {
      app_id: "3f8cade8"
    });
  daovoice('update');
  </script>












  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Machine Learning,">





  <link rel="alternate" href="/atom.xml" title="空博客" type="application/atom+xml">






<meta name="description" content="正向传播和机器学习|吴恩达机器学习之神经网络中的内容差不多，都是在给出$\Theta_{(1)}$和$\Theta_{(2)}$的情况下通过正向传播求个代价值或是验证一下准确率。">
<meta name="keywords" content="Machine Learning">
<meta property="og:type" content="article">
<meta property="og:title" content="机器学习|吴恩达机器学习之神经网络反向传播">
<meta property="og:url" content="http://yoursite.com/2019/04/12/机器学习-吴恩达机器学习之神经网络反向传播/index.html">
<meta property="og:site_name" content="空博客">
<meta property="og:description" content="正向传播和机器学习|吴恩达机器学习之神经网络中的内容差不多，都是在给出$\Theta_{(1)}$和$\Theta_{(2)}$的情况下通过正向传播求个代价值或是验证一下准确率。">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="https://s2.ax1x.com/2019/04/11/A7xwiq.png">
<meta property="og:image" content="https://s2.ax1x.com/2019/04/06/AW7Z40.md.png">
<meta property="og:image" content="https://s2.ax1x.com/2019/04/06/AW7hrQ.png">
<meta property="og:image" content="https://s2.ax1x.com/2019/04/08/A4jwvR.png">
<meta property="og:image" content="https://s2.ax1x.com/2019/04/09/AI71Yj.png">
<meta property="og:image" content="https://s2.ax1x.com/2019/04/09/A5HaS1.png">
<meta property="og:updated_time" content="2019-04-12T14:25:47.805Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="机器学习|吴恩达机器学习之神经网络反向传播">
<meta name="twitter:description" content="正向传播和机器学习|吴恩达机器学习之神经网络中的内容差不多，都是在给出$\Theta_{(1)}$和$\Theta_{(2)}$的情况下通过正向传播求个代价值或是验证一下准确率。">
<meta name="twitter:image" content="https://s2.ax1x.com/2019/04/11/A7xwiq.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2019/04/12/机器学习-吴恩达机器学习之神经网络反向传播/">





  <title>机器学习|吴恩达机器学习之神经网络反向传播 | 空博客</title>
  








</head>

<body itemscope="" itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope="" itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">空博客</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">总不能浪费个副标题吧</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br>
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br>
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-留言板">
          <a href="/guestbook" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-newspaper-o"></i> <br>
            
            留言板
          </a>
        </li>
      

      
    </ul>
  

  
</nav>


 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/04/12/机器学习-吴恩达机器学习之神经网络反向传播/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="JQK/许阳航">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="空博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">机器学习|吴恩达机器学习之神经网络反向传播</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-04-12T17:01:04+08:00">
                2019-04-12
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h2 id="正向传播"><a href="#正向传播" class="headerlink" title="正向传播"></a>正向传播</h2><p>和<a href="https://nullblog.top/2019/04/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" target="_blank" rel="noopener">机器学习|吴恩达机器学习之神经网络</a>中的内容差不多，都是在给出$\Theta_{(1)}$和$\Theta_{(2)}$的情况下通过正向传播求个代价值或是验证一下准确率。</p>
<a id="more"></a>
<h3 id="导入数据"><a href="#导入数据" class="headerlink" title="导入数据"></a>导入数据</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_mat</span><span class="params">(path)</span>:</span></span><br><span class="line">    <span class="string">'''读取数据'''</span></span><br><span class="line">    data = loadmat(<span class="string">'ex4data1.mat'</span>)  <span class="comment"># return a dict</span></span><br><span class="line">    X = data[<span class="string">'X'</span>]</span><br><span class="line">    y = data[<span class="string">'y'</span>].flatten()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> X, y  </span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_weight</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">'''读取权重'''</span></span><br><span class="line">    weight = loadmat(<span class="string">'ex4weights.mat'</span>)</span><br><span class="line">    theta1 = weight[<span class="string">'Theta1'</span>]</span><br><span class="line">    theta2 = weight[<span class="string">'Theta2'</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> theta1, theta2</span><br></pre></td></tr></table></figure>
<p>这里需要对y向量做一个处理：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">expand_y</span><span class="params">(y)</span>:</span></span><br><span class="line">    result = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> y:</span><br><span class="line">        y_array = np.zeros(<span class="number">10</span>)</span><br><span class="line">        y_array[i - <span class="number">1</span>] = <span class="number">1</span></span><br><span class="line">        result.append(y_array)</span><br><span class="line">    <span class="keyword">return</span> np.array(result)</span><br></pre></td></tr></table></figure>
<p>原来的y是用数字表示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[<span class="number">10</span> <span class="number">10</span> <span class="number">10</span> ...  <span class="number">9</span>  <span class="number">9</span>  <span class="number">9</span>]</span><br></pre></td></tr></table></figure>
<p>转换为矩阵，用1的位置来表示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[[<span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> ... <span class="number">0.</span> <span class="number">0.</span> <span class="number">1.</span>]</span><br><span class="line"> [<span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> ... <span class="number">0.</span> <span class="number">0.</span> <span class="number">1.</span>]</span><br><span class="line"> [<span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> ... <span class="number">0.</span> <span class="number">0.</span> <span class="number">1.</span>]</span><br><span class="line"> ...</span><br><span class="line"> [<span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> ... <span class="number">0.</span> <span class="number">1.</span> <span class="number">0.</span>]</span><br><span class="line"> [<span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> ... <span class="number">0.</span> <span class="number">1.</span> <span class="number">0.</span>]</span><br><span class="line"> [<span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> ... <span class="number">0.</span> <span class="number">1.</span> <span class="number">0.</span>]]</span><br></pre></td></tr></table></figure>
<h4 id="可视化数据"><a href="#可视化数据" class="headerlink" title="可视化数据"></a>可视化数据</h4><p>随机选100张图，可视化观察一波</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_data</span><span class="params">(X)</span>:</span></span><br><span class="line">    <span class="string">'''随机画100个数字'''</span></span><br><span class="line">    index = np.random.choice(range(<span class="number">5000</span>),</span><br><span class="line">                             <span class="number">100</span>)  <span class="comment"># np.random.choice(arrange,size),返回ndarray</span></span><br><span class="line">    images = X[index]  <span class="comment"># 随机选择100个样本</span></span><br><span class="line">    fig, ax_array = plt.subplots(</span><br><span class="line">        <span class="number">10</span>, <span class="number">10</span>, sharex=<span class="keyword">True</span>, sharey=<span class="keyword">True</span>, figsize=(<span class="number">8</span>, <span class="number">8</span>))  <span class="comment"># ax_array为Axes对象</span></span><br><span class="line">    <span class="keyword">for</span> r <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">        <span class="keyword">for</span> c <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">            ax_array[r, c].matshow(</span><br><span class="line">                images[r * <span class="number">10</span> + c].reshape(<span class="number">20</span>, <span class="number">20</span>), cmap=<span class="string">'gray_r'</span></span><br><span class="line">            )  <span class="comment"># matshow() 第一个参数为要显示的矩阵（Display an array as a matrix in a new 				 figure window）</span></span><br><span class="line">    plt.yticks([])</span><br><span class="line">    plt.xticks([])</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure>
<p>关于<code>plt.subplots</code>展开，参考<a href="https://nullblog.top/2019/04/07/subplots%E7%94%BB%E5%9B%BE/#more" target="_blank" rel="noopener">subplots画图</a>。</p>
<h3 id="计算"><a href="#计算" class="headerlink" title="计算"></a>计算</h3><p>因为反向传播和正向传播的公式，代价函数，正则化都是一样的，在后面反向传播再展开讲。</p>
<h4 id="正向传播-1"><a href="#正向传播-1" class="headerlink" title="正向传播"></a>正向传播</h4><p>1.公式：</p>
<p><img src="https://s2.ax1x.com/2019/04/11/A7xwiq.png" alt="A7xwiq.png"></p>
<p>2.代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">feed_forward</span><span class="params">(theta1, theta2, X)</span>:</span></span><br><span class="line">    z2 = X @ theta1.T</span><br><span class="line">    a2 = sg.sigmoid(z2)  <span class="comment">#(5000,25)</span></span><br><span class="line">    a2 = np.insert(a2, <span class="number">0</span>, <span class="number">1</span>, axis=<span class="number">1</span>)  <span class="comment">#(5000,26)</span></span><br><span class="line">    z3 = a2 @ theta2.T</span><br><span class="line">    a3 = sg.sigmoid(z3)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> z2, a2, z3, a3</span><br></pre></td></tr></table></figure>
<h4 id="代价函数"><a href="#代价函数" class="headerlink" title="代价函数"></a>代价函数</h4><p>1.公式：$J(\theta)=\frac{1}{m} \sum_{i=1}^{m} \sum_{k=1}^{K}\left[-y_{k}^{(i)} \log \left(\left(h_{\theta}\left(x^{(i)}\right)\right)_{k}\right)-\left(1-y_{k}^{(i)}\right) \log \left(1-\left(h_{\theta}\left(x^{(i)}\right)\right)_{k}\right)\right.$</p>
<p>2.代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cost</span><span class="params">(theta1, theta2, X, y)</span>:</span></span><br><span class="line">    z2, a2, z3, h = feed_forward(theta1, theta2, X)</span><br><span class="line">    <span class="comment"># 这里的y是矩阵而不是向量了</span></span><br><span class="line">    first = -y * np.log(h)</span><br><span class="line">    second = (<span class="number">1</span> - y) * np.log(<span class="number">1</span> - h)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> (np.sum(first - second)) / len(X)  <span class="comment"># 这里不能用np.mean()，否则会相差10倍</span></span><br></pre></td></tr></table></figure>
<h4 id="代价函数正则化"><a href="#代价函数正则化" class="headerlink" title="代价函数正则化"></a>代价函数正则化</h4><p>1.公式：</p>
<p>$\begin{aligned} J(\theta)=&amp; \frac{1}{m} \sum_{i=1}^{m} \sum_{k=1}^{K}\left[-y_{k}^{(i)} \log \left(\left(h_{\theta}\left(x^{(i)}\right)\right)_{k}\right)-\left(1-y_{k}^{(i)}\right) \log \left(1-\left(h_{\theta}\left(x^{(i)}\right)\right)_{k}\right)\right]+\\ &amp; \frac{\lambda}{2 m}\left[\sum_{j=1}^{25} \sum_{k=1}^{400}\left(\Theta_{j, k}^{(1)}\right)^{2}+\sum_{j=1}^{10} \sum_{k=1}^{25}\left(\Theta_{j, k}^{(2)}\right)^{2}\right] \end{aligned}$</p>
<p>2.代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cost_reg</span><span class="params">(theta1, theta2, X, y, lmd)</span>:</span></span><br><span class="line">    c = cost(theta1, theta2, X, y)</span><br><span class="line">    reg = (lmd / (<span class="number">2</span> * len(X))) * (</span><br><span class="line">        np.sum(theta1[:, <span class="number">1</span>:]**<span class="number">2</span>) + np.sum(theta2[:, <span class="number">1</span>:]**<span class="number">2</span>))</span><br><span class="line">    <span class="keyword">return</span> reg + c</span><br></pre></td></tr></table></figure>
<h4 id="主函数"><a href="#主函数" class="headerlink" title="主函数"></a>主函数</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line">    path = <span class="string">'ex4data1.mat'</span></span><br><span class="line">    X, y = load_mat(path)</span><br><span class="line">    X = np.insert(X, <span class="number">0</span>, <span class="number">1</span>, axis=<span class="number">1</span>)</span><br><span class="line">    y = expand_y(y)</span><br><span class="line">    theta1, theta2 = load_weight()</span><br><span class="line">    print(cost_reg(theta1, theta2, X, y, <span class="number">1</span>))  <span class="comment">#0.38376985909092354</span></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure>
<h2 id="反向传播"><a href="#反向传播" class="headerlink" title="反向传播"></a>反向传播</h2><h4 id="随机初始化"><a href="#随机初始化" class="headerlink" title="随机初始化"></a>随机初始化</h4><p>之前我们是把$\theta​$初始化为0向量，但是在神经网络中如果把$\theta_{1}​$初始化为0，那么$S_{2}​$中的激活单元都为相同值。同理，只要初始化为相同的数，那么结果都一样。</p>
<p>因此我们通常随机初始化，即在（-$\varepsilon$，$\varepsilon$）之间随机取值，为了保证效率，需要取值足够小，所以选择$\varepsilon=0.12$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">random_init</span><span class="params">(size)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> np.random.uniform(<span class="number">-0.12</span>, <span class="number">0.12</span>, size)</span><br></pre></td></tr></table></figure>
<h4 id="处理参数"><a href="#处理参数" class="headerlink" title="处理参数"></a>处理参数</h4><p>使用优化参数<code>opt.minimize()</code>，需要把参数展开。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">serialize</span><span class="params">(a, b)</span>:</span></span><br><span class="line">    <span class="string">'''展开参数'''</span></span><br><span class="line">    <span class="keyword">return</span> np.r_[a.flatten(), b.flatten()]  <span class="comment"># 按行拼接</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">deserialize</span><span class="params">(seq)</span>:</span></span><br><span class="line">    <span class="string">'''提取参数'''</span></span><br><span class="line">    <span class="keyword">return</span> seq[:<span class="number">25</span> * <span class="number">401</span>].reshape(<span class="number">25</span>, <span class="number">401</span>), seq[<span class="number">25</span> * <span class="number">401</span>:].reshape(<span class="number">10</span>, <span class="number">26</span>)</span><br></pre></td></tr></table></figure>
<h4 id="代价函数（带正则项）"><a href="#代价函数（带正则项）" class="headerlink" title="代价函数（带正则项）"></a>代价函数（带正则项）</h4><p>以多分类为例</p>
<p>1.公式：<img src="https://s2.ax1x.com/2019/04/06/AW7Z40.md.png" alt="AW7Z40.md.png"></p>
<p>其中</p>
<ul>
<li>L：神经网络的层数</li>
<li>$s_{l}​$：$l​$层中的神经元个数（不包括bias unit）</li>
<li>K：输出层中的神经元个数</li>
<li>m：样本个数</li>
</ul>
<p><img src="https://s2.ax1x.com/2019/04/06/AW7hrQ.png" alt="AW7hrQ.png"></p>
<p>累加项中表示从第一项累加到第k项（why？）</p>
<p>正则项表示神经网络中所有权重的平方和。</p>
<h4 id="梯度项"><a href="#梯度项" class="headerlink" title="梯度项"></a>梯度项</h4><p><img src="https://s2.ax1x.com/2019/04/08/A4jwvR.png" alt="A4jwvR.png"></p>
<p>一般情况下，我们只知道Input Layer和Output Layer两层的神经元个数，中间的Hidden Layer很难确定，不过对于初学者而言，都是参考别人算法里的，所以这里也直接给出了Hidden Layer的层数（1层）以及$\theta_{1}$（25，401）和$\theta_{2}$（10，26）的维度，一个神经元为一列。</p>
<h5 id="计算前馈-feedforward"><a href="#计算前馈-feedforward" class="headerlink" title="计算前馈(feedforward)"></a>计算前馈(feedforward)</h5><p>参数含义及传递过程如下</p>
<p>1.参数含义：</p>
<ul>
<li>$\Theta^{i}​$第$i​$层的参数矩阵</li>
<li>$z^{(l)}$第$l$层的输入</li>
<li>$a^{(l)}$第$l$层的输出</li>
</ul>
<p>2.传递过程：</p>
<ul>
<li>$a^{(1)}=x​$</li>
<li>$z^{(2)}=\Theta^{(1)} a^{(1)}​$</li>
<li>$a^{(2)}=g\left(z^{(2)}\right)\left(a d d \ bias \ \ a_{0}^{(2)}\right)​$</li>
<li>$z^{(3)}=\Theta^{(2)} a^{(2)}​$</li>
<li>$h=a^{(3)}=g\left(z^{(3)}\right)​$</li>
</ul>
<p>3.前馈代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">feed_forward</span><span class="params">(theta1, theta2, X)</span>:</span></span><br><span class="line">    z2 = X @ theta1.T</span><br><span class="line">    a2 = sg.sigmoid(z2)  <span class="comment">#(5000,25)</span></span><br><span class="line">    a2 = np.insert(a2, <span class="number">0</span>, <span class="number">1</span>, axis=<span class="number">1</span>)  <span class="comment">#(5000,26)</span></span><br><span class="line">    z3 = a2 @ theta2.T</span><br><span class="line">    a3 = sg.sigmoid(z3) </span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> z2, a2, z3, a3</span><br></pre></td></tr></table></figure>
<h5 id="计算梯度项"><a href="#计算梯度项" class="headerlink" title="计算梯度项"></a>计算梯度项</h5><p>计算梯度项，也就是代价函数的偏导数$\frac{\partial}{\partial\Theta^{(l)}_{ij}}J\left(\Theta\right)​$。通过前馈的计算我们得到了$h​$，接下来计算“误差”，这里之所以用引号，是因为误差的实质是$\delta^{(l)}=\frac{\partial J}{\partial z^{(l)}}​$</p>
<ul>
<li>$\delta^{(3)}=h-y​$ ……(1)</li>
<li>$\delta^{(2)}=(\Theta^{(2)})^{T}\delta^{(3)}g^{\prime}\left(z^{(2)}\right)$……(2)</li>
</ul>
<p>第一层没有误差，接下去计算每层参数矩阵的<strong>梯度值</strong>，用$\Delta^{(l)}$表示</p>
<ul>
<li>$\Delta^{(2)}=a^{(2)} \delta^{(3)}$……(3)</li>
<li>$\Delta^{(1)}=a^{(1)} \delta^{(2)}​$……(4)</li>
</ul>
<p>最后网络的总梯度为：</p>
<p>（这里并不是真的相加，而是将$\Delta^{(1)}$和$\Delta^{(2)}$合成为一个向量，方便后面计算）</p>
<p>$D=\frac{1}{m}\left(\Delta^{(1)}+\Delta^{(2)}\right)$</p>
<p>求梯度项代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradient</span><span class="params">(theta1,theta2,X,y)</span>:</span></span><br><span class="line">    z2,a2,z3,h=feed_forward(theta1,theta2,X)</span><br><span class="line">    d3=h-y <span class="comment"># (5000,10) </span></span><br><span class="line">    d2=d3@theta2[:,<span class="number">1</span>:]*sg.sigmoid_gradient(z2) <span class="comment"># (5000,25)</span></span><br><span class="line">    D2=d3.T@a2 <span class="comment"># (10,26)</span></span><br><span class="line">    D1=d2.T@X <span class="comment"># (25,401)</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 这里合并成1-D array是为了方便后面用优化函数处理</span></span><br><span class="line">    D=(<span class="number">1</span>/len(X))*serialize(D1,D2) <span class="comment">#(10285,)</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> D</span><br></pre></td></tr></table></figure>
<p><strong>正则化</strong></p>
<p>1.原理</p>
<p><img src="https://s2.ax1x.com/2019/04/09/AI71Yj.png" alt="AI71Yj.png"></p>
<p>2.代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">regularized_gradient</span><span class="params">(theta,X,y,lmd=<span class="number">1</span>)</span>:</span></span><br><span class="line">    theta1,theta2=deserialize(theta)</span><br><span class="line">    D1,D2=deserialize(gradient(theta1,theta2,X,y))</span><br><span class="line">    theta1[:,<span class="number">0</span>]=<span class="number">0</span></span><br><span class="line">    theta2[:,<span class="number">0</span>]=<span class="number">0</span></span><br><span class="line">    reg_D1=D1+(lmd/len(X))*theta1</span><br><span class="line">    reg_D2=D2+(lmd/len(X))*theta2</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> serialize(reg_D1,reg_D2)</span><br></pre></td></tr></table></figure>
<h5 id="推导-delta-和-Delta"><a href="#推导-delta-和-Delta" class="headerlink" title="*推导$\delta$和$\Delta$"></a>*推导$\delta$和$\Delta$</h5><p>从代价函数入手，假设我们只有一个输入样本，那么代价函数为：$J(\theta)=-y \operatorname{logh}(x)-(1-y) \log (1-h)$，所谓梯度项，就是将代价函数对参数求导，即$\frac{\partial}{\partial \Theta^{(2)}} J(\theta), \frac{\partial}{\partial \Theta^{(1)}} J(\theta)$。而由传递过程函数：</p>
<ul>
<li>$a^{(1)}=x$</li>
<li>$z^{(2)}=\Theta^{(1)} a^{(1)}$</li>
<li>$a^{(2)}=g\left(z^{(2)}\right)\left(a d d \ bias \ \ a_{0}^{(2)}\right)$</li>
<li>$z^{(3)}=\Theta^{(2)} a^{(2)}$</li>
<li>$h=a^{(3)}=g\left(z^{(3)}\right)​$</li>
</ul>
<p>我们可以使用链式求导法则，因此有$\frac{\partial J}{\partial \Theta^{(2)}}=\frac{\partial J}{\partial a^{(3)}} \frac{\partial a^{(3)}}{\partial z^{(3)}} \frac{\partial z^{(3)}}{\partial \Theta^{(2)}}=(h-y)a^{(2)}​$</p>
<p>其中令$\delta^{(3)}=h-y​$得到公式(1)；令$\Delta^{(2)}=\frac{\partial J}{\partial \Theta^{(2)}}​$则得到公式(3)。</p>
<p>接着求$\frac{\partial J}{\partial \Theta^{(1)}}​$=$\frac{\partial J}{\partial a^{(3)}} \frac{\partial a^{(3)}}{\partial z^{(3)}} \frac{\partial z^{(3)}}{\partial a^{(2)}} \frac{\partial a^{(2)}}{\partial z^{(2)}} \frac{\partial z^{(2)}}{\partial \Theta^{(1)}}​$</p>
<p>​            =$\delta^{(3)} \Theta^{(2)} g^{\prime}\left(Z^{(2)}\right) a^{(1)}​$</p>
<p>​            =$\delta^{(2)} a^{(1)}​$</p>
<p>同样可以看出，令$\delta^{(3)}=\frac{\partial J}{\partial a^{(3)}} \frac{\partial a^{(3)}}{\partial z^{(3)}}$ ，$\delta^{(2)}=\delta^{(3)} \Theta^{(2)} g^{\prime}\left(Z^{(2)}\right)$则得到公式(2)。令$\frac{\partial J}{\partial \Theta^{(1)}}=\Delta^{(1)}$得到公式(4)。</p>
<h4 id="梯度检验"><a href="#梯度检验" class="headerlink" title="梯度检验"></a>梯度检验</h4><p>目的：在反向传播的过程中，因为需要计算的参数很多，因此容易导致误差，使得最终的结果并非最优解。因此为了确定反向传播计算的梯度是否正确，需要用到梯度检验(<strong>gradient check</strong>)。</p>
<p>原理：通过计算$\frac{\partial}{\partial \Theta}J(\Theta)=\lim _{\varepsilon \rightarrow 0} \frac{J(\theta+\varepsilon)-J(\theta-\varepsilon)}{2 \varepsilon}$，估计出$J(\theta)$在$\theta$的值，和反向传播计算的梯度值$\Delta$进行对比。具体来说，对于某个特定的 $\theta$，我们计算出在 $\theta$-$\varepsilon $ 处和 $\theta$+$\varepsilon $ 的代价值（$\varepsilon $是一个非常小的值，通常选取 0.001），然后求两个代价的平均，用以估计在 $\theta$ 处的代价值。</p>
<p><img src="https://s2.ax1x.com/2019/04/09/A5HaS1.png" alt="A5HaS1.png"></p>
<p>具体做法：先将$\Theta_{(1)}$和$\Theta_{(2)}$展开成一个向量$\theta$，维度（10285，）。然后循环，对每个$\theta_{j}$求$\frac{\partial}{\partial \theta_{j}}J(\theta)$，以$\theta_{1}$为例，$\frac{\partial}{\partial \theta_{1}}=\frac{J\left(\theta_{1}+\varepsilon_{1}, \theta_{2}, \theta_{3} \dots \theta_{n}\right)-J\left(\theta_{1}-\varepsilon_{1}, \theta_{2}, \theta_{3} \ldots \theta_{n}\right)}{2 \varepsilon}$，注意这里虽然只有$\theta_{j}$每次都要把整个$J(\theta)$带进去，因此需要每次都复制整个$\theta$。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradient_check</span><span class="params">(theta1,theta2,X,y,e)</span>:</span></span><br><span class="line">    theta_temp=serialize(theta1,theta2) <span class="comment"># (10285,)</span></span><br><span class="line">    numeric_grad=[]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(theta_temp)):</span><br><span class="line">        plus=copy.copy(theta_temp) <span class="comment"># 复制theta</span></span><br><span class="line">        minus=copy.copy(theta_temp) <span class="comment"># 复制theta</span></span><br><span class="line">        plus[i]+=e <span class="comment"># 只改变当前的一个值</span></span><br><span class="line">        minus[i]-=e <span class="comment"># 只改变当前的一个值</span></span><br><span class="line">        </span><br><span class="line">        grad_i=(cost_reg(plus,X,y,<span class="number">1</span>)-cost_reg(minus,X,y,<span class="number">1</span>))/(e*<span class="number">2</span>)</span><br><span class="line">        numeric_grad.append(grad_i)</span><br></pre></td></tr></table></figure>
<p>再和$\Theta_{(1)}$和$\Theta_{(2)}$比较求得准确度。</p>
<p>这里简单介绍一下数值梯度(<strong>numerical gradient</strong>)和解析梯度(<strong>analytic gradient</strong>)，数值梯度的优点是编程可以直接实现，不要求函数可微，缺点是运行速度非常慢，也就是上面中<code>numeric_grad</code>，且只能求出近似解；解析梯度能求出近似解，也是我们通常使用的方法，即<code>analytic_grad</code>。</p>
<p>得到数值梯度和解析梯度之后，要求他们的进行相似性度量(<strong>Similarity Measurement</strong>)，这里用标准化欧氏距离（存疑？）$diff=\frac{ | \text { numeric_grad }-\text {analytic_grad}\left|_{2}\right.}{ | \text { numeric_grad }\left|_{2}+\right| \text { analytic_grad }\left|_{2}\right.}$，当距离(diff)小于<code>10e-9</code>时为计算正确。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">reg_D1,reg_D2=regularized_gradient(theta_temp,X,y)</span><br><span class="line">analytic_grad=serialize(reg_D1,reg_D2)</span><br><span class="line">   </span><br><span class="line">   diff = np.linalg.norm(numeric_grad - analytic_grad) / np.linalg.norm(numeric_grad 			+ analytic_grad)</span><br><span class="line"></span><br><span class="line">   print(<span class="string">'If your backpropagation implementation is correct,\nthe relative difference 			will be smaller than 10e-9 (assume epsilon=0.0001).\nRelative Difference: 			 &#123;&#125;\n'</span>.format(diff))</span><br></pre></td></tr></table></figure>
<p><code>np.linalg.norm()</code>：linalg=linear(线性)+algebra(代数)，norm()表示范数$^{[1]}$。</p>
<h4 id="优化参数"><a href="#优化参数" class="headerlink" title="优化参数"></a>优化参数</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">nn_trainning</span><span class="params">(X,y)</span>:</span></span><br><span class="line"></span><br><span class="line">    init_theta = random_init(<span class="number">10285</span>) <span class="comment">#随机初始化theta1和theta2</span></span><br><span class="line">    res = opt.minimize(</span><br><span class="line">        fun=cost_reg,</span><br><span class="line">        x0=init_theta,</span><br><span class="line">        args=(X, y, <span class="number">1</span>),</span><br><span class="line">        method=<span class="string">'TNC'</span>,</span><br><span class="line">        jac=regularized_gradient,</span><br><span class="line">        options=&#123;<span class="string">'maxiter'</span>: <span class="number">400</span>&#125;)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>
<h4 id="可视化隐藏层"><a href="#可视化隐藏层" class="headerlink" title="可视化隐藏层"></a>可视化隐藏层</h4><p>一种明白神经网络是如何学习的方法就是将隐藏层捕获的内容可视化，通俗来说就是输入一个x，激活这个隐藏层。在我们的这个训练样本中，$\theta_{1}$（25,401）有401个参数，去掉偏置单元，将剩下的400个参数reshape为(20,20)。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_hidden</span><span class="params">(theta)</span>:</span></span><br><span class="line">    t1,_=deserialize(theta)</span><br><span class="line">    t1=t1[:,<span class="number">1</span>:]</span><br><span class="line">    fig,ax_array=plt.subplots(<span class="number">5</span>,<span class="number">5</span>,sharex=<span class="keyword">True</span>,sharey=<span class="keyword">True</span>,figsize=(<span class="number">6</span>,<span class="number">6</span>))</span><br><span class="line">    <span class="keyword">for</span> r <span class="keyword">in</span> range(<span class="number">5</span>):</span><br><span class="line">        <span class="keyword">for</span> c <span class="keyword">in</span> range(<span class="number">5</span>):</span><br><span class="line">            ax_array[r,c].matshow(t1[r*<span class="number">5</span>+c].reshape(<span class="number">20</span>,<span class="number">20</span>),cmap=<span class="string">'gray_r'</span>)</span><br><span class="line">    plt.xticks([])</span><br><span class="line">    plt.yticks([])</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure>
<h4 id="计算精确度"><a href="#计算精确度" class="headerlink" title="计算精确度"></a>计算精确度</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">accuracy</span><span class="params">(theta, X, y)</span>:</span></span><br><span class="line">    theta1, theta2 = deserialize(theta)</span><br><span class="line">    _, _, _, h = feed_forward(theta1, theta2, X)</span><br><span class="line">    y_pred = np.argmax(h, axis=<span class="number">1</span>) + <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    print(classification_report(y, y_pred))</span><br></pre></td></tr></table></figure>
<h4 id="主函数-1"><a href="#主函数-1" class="headerlink" title="主函数"></a>主函数</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line">    path = <span class="string">'ex4data1.mat'</span></span><br><span class="line">    X, raw_y = ff.load_mat(path)</span><br><span class="line">    X = np.insert(X, <span class="number">0</span>, <span class="number">1</span>, axis=<span class="number">1</span>)</span><br><span class="line">    y = ff.expand_y(raw_y)  <span class="comment"># y的一行表示一个样本</span></span><br><span class="line">    <span class="comment"># theta1, theta2 = ff.load_weight()</span></span><br><span class="line">    <span class="comment"># gradient_check(theta1,theta2,X,y,0.0001)</span></span><br><span class="line">    theta_unroll = nn_training(X, y)</span><br><span class="line">    accuracy(theta_unroll.x, X, raw_y)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p>[1] <a href="https://blog.csdn.net/hqh131360239/article/details/79061535" target="_blank" rel="noopener">np.linalg.norm(求范数)</a></p>
<p>[2] <a href="https://blog.csdn.net/Cowry5/article/details/80399350" target="_blank" rel="noopener">吴恩达机器学习作业Python实现(四)：神经网络(反向传播)</a></p>

      
    </div>
    
    
    
	
	
	<div>
  
    <div>
    
        <div style="text-align:center;color: #ccc;font-size:14px;">-------------End-------------</div>
    
</div>

  
	</div>
    
	
	

    
      <div>
        <div style="padding: 10px 0; margin: 20px auto; width: 90%; text-align: center;">
  <div>梦想总是要有的，万一有人有钱呢？</div>
  <button id="rewardButton" disable="enable" onclick="var qr = document.getElementById('QR'); if (qr.style.display === 'none') {qr.style.display='block';} else {qr.style.display='none'}">
    <span>包养</span>
  </button>
  <div id="QR" style="display: none;">

    
      <div id="wechat" style="display: inline-block">
        <img id="wechat_qr" src="/images/wechatpay.png" alt="JQK/许阳航 微信支付">
        <p>微信支付</p>
      </div>
    

    
      <div id="alipay" style="display: inline-block">
        <img id="alipay_qr" src="/images/alipay.png" alt="JQK/许阳航 支付宝">
        <p>支付宝</p>
      </div>
    

    

  </div>
</div>

      </div>
    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/Machine-Learning/" rel="tag"><i class="fa fa-tag"></i> Machine Learning</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2019/04/07/subplots画图/" rel="next" title="subplots画图">
                <i class="fa fa-chevron-left"></i> subplots画图
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2019/04/15/matplotlib坐标原点不重合/" rel="prev" title="matplotlib坐标原点不重合">
                matplotlib坐标原点不重合 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
        
  <script>
    window._bd_share_config = {
      "common": {
        "bdText": "",
        "bdMini": "1",
        "bdMiniList": false,
        "bdPic": ""
      },
      "image": {
        "viewList": ["tsina", "douban", "sqq", "qzone", "weixin", "twi", "fbook"],
        "viewText": "分享到：",
        "viewSize": "16"
      },
      "slide": {
        "bdImg": "5",
        "bdPos": "left",
        "bdTop": "100"
      }
    }
  </script>

<script>
  with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];
</script>

      
    </div>
  </div>


          </div>
          


          

  
    <div id="gitalk-container"></div>
    <link rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css">
	  
  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope="" itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/images/avatar.jpg" alt="JQK/许阳航">
            
              <p class="site-author-name" itemprop="name">JQK/许阳航</p>
              <p class="site-description motion-element" itemprop="description">The early worm is catched by birds</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">39</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                
                  <span class="site-state-item-count">3</span>
                  <span class="site-state-item-name">分类</span>
                
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">19</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          
            <div class="feed-link motion-element">
              <a href="/atom.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/cesarexyh" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="https://weibo.com/2442021982/profile?topnav=1&wvr=6&is_all=1" target="_blank" title="Weibo">
                      
                        <i class="fa fa-fw fa-weibo"></i>Weibo</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="https://www.zhihu.com/people/qing-lou-man-zuo/activities" target="_blank" title="Zhihu">
                      
                        <i class="fa fa-fw fa-custom zhihu"></i>Zhihu</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="https://www.douban.com/people/123226876/" target="_blank" title="Douban">
                      
                        <i class="fa fa-fw fa-custom douban"></i>Douban</a>
                  </span>
                
            </div>
          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#正向传播"><span class="nav-number">1.</span> <span class="nav-text">正向传播</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#导入数据"><span class="nav-number">1.1.</span> <span class="nav-text">导入数据</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#可视化数据"><span class="nav-number">1.1.1.</span> <span class="nav-text">可视化数据</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#计算"><span class="nav-number">1.2.</span> <span class="nav-text">计算</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#正向传播-1"><span class="nav-number">1.2.1.</span> <span class="nav-text">正向传播</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#代价函数"><span class="nav-number">1.2.2.</span> <span class="nav-text">代价函数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#代价函数正则化"><span class="nav-number">1.2.3.</span> <span class="nav-text">代价函数正则化</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#主函数"><span class="nav-number">1.2.4.</span> <span class="nav-text">主函数</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#反向传播"><span class="nav-number">2.</span> <span class="nav-text">反向传播</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#随机初始化"><span class="nav-number">2.0.1.</span> <span class="nav-text">随机初始化</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#处理参数"><span class="nav-number">2.0.2.</span> <span class="nav-text">处理参数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#代价函数（带正则项）"><span class="nav-number">2.0.3.</span> <span class="nav-text">代价函数（带正则项）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#梯度项"><span class="nav-number">2.0.4.</span> <span class="nav-text">梯度项</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#计算前馈-feedforward"><span class="nav-number">2.0.4.1.</span> <span class="nav-text">计算前馈(feedforward)</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#计算梯度项"><span class="nav-number">2.0.4.2.</span> <span class="nav-text">计算梯度项</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#推导-delta-和-Delta"><span class="nav-number">2.0.4.3.</span> <span class="nav-text">*推导$\delta$和$\Delta$</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#梯度检验"><span class="nav-number">2.0.5.</span> <span class="nav-text">梯度检验</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#优化参数"><span class="nav-number">2.0.6.</span> <span class="nav-text">优化参数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#可视化隐藏层"><span class="nav-number">2.0.7.</span> <span class="nav-text">可视化隐藏层</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#计算精确度"><span class="nav-number">2.0.8.</span> <span class="nav-text">计算精确度</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#主函数-1"><span class="nav-number">2.0.9.</span> <span class="nav-text">主函数</span></a></li></ol></li></ol><li class="nav-item nav-level-2"><a class="nav-link" href="#参考资料"><span class="nav-number">3.</span> <span class="nav-text">参考资料</span></a></li></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
<div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">JQK/许阳航</span>

  
</div>


	<div class="powered-by">
	<i class="fa fa-user-md"></i><span id="busuanzi_container_site_uv">
		本站访客数:<span id="busuanzi_value_site_uv"></span>
	</span>
	</div>
<!--
  <div class="powered-by">个人专属</div>
-->



  <span class="post-meta-divider">|</span>



  <div class="theme-info">博客-nullblog </div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  


  











  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  

  
  
    <script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












    
        
        <script src="https://unpkg.com/gitalk/dist/gitalk.min.js"></script>
        <script src="https://cdn.bootcss.com/blueimp-md5/2.10.0/js/md5.js"></script>
        <script type="text/javascript">
            const gitalk = new Gitalk({
            clientID: '6724e07f1e7ddc3c67ae',
            clientSecret: 'ed0bb21b8d06979b07b9453d473167f1024efc97',
            repo: 'scp-1024.github.io',
            owner: 'scp-1024',
            admin: 'scp-1024'.split(','),
            pagerDirection: 'first',
            id: md5(window.location.pathname),
            distractionFreeMode: 'true'
            })
            gitalk.render('gitalk-container')
        </script>
        
    

  





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>

<!--崩溃欺骗-->
<script type="text/javascript" src="/js/src/crash_cheat.js"></script>
