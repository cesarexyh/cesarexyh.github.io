<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>空博客</title>
  
  <subtitle>总不能浪费个副标题吧</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2019-06-20T08:28:12.298Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>JQK/许阳航</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>机器学习基石7|The VC Dimension</title>
    <link href="http://yoursite.com/2019/06/19/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B37-The-VC-Dimension/"/>
    <id>http://yoursite.com/2019/06/19/机器学习基石7-The-VC-Dimension/</id>
    <published>2019-06-19T10:09:11.000Z</published>
    <updated>2019-06-20T08:28:12.298Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://imgchr.com/i/VHVk0s" target="_blank" rel="noopener"><img src="https://s2.ax1x.com/2019/06/17/VHVk0s.md.png" alt="VHVk0s.md.png"></a></p><a id="more"></a><p>—————————红色石头机器学习课程笔记7 &lt;节选&gt;—————————————</p><p> 在前几节课中，我们推导出机器要能够学习，必须满足两个条件：</p><ul><li>hypothesis $H$中的size M是有限的，N足够大，那么对于$H$中的任意一个假设$h$，$E_{o u t} \approx E_{i n}$</li><li>利用算法$A$从hypothesis中，挑选出一个$g$，使$E_{in}(g) \approx 0$，则$E_{o u t} \approx 0$。</li></ul><p>这第二个条件对应了train和test，train的目的是让损失期望$E_{i n}(g) \approx 0$；test目的是将算法运用到新样本时，损失期望也尽可能小，即$E_{o u t} \approx 0$。</p><p>正因为如此，上次课引入了break point，并推导出只要break point存在，则M有上界，一定存在$E_{o u t} \approx E_{i n}$。</p><p>这周的课程介绍VC Dimension，也是总结VC Dimension与$E_{i n}(g) \approx 0$，$E_{o u t} \approx 0$，Model Complexity Penalty的关系。</p><hr><h2 id="Definition-of-VC-Dimension"><a href="#Definition-of-VC-Dimension" class="headerlink" title="Definition of VC Dimension"></a>Definition of VC Dimension</h2><p>dichotomy的上限是成长函数，成长函数的上限是$B(N,k)$，$B(N,k)$的上限是$N^{k-1}$。</p><p><a href="https://imgchr.com/i/VH5N01" target="_blank" rel="noopener"><img src="https://s2.ax1x.com/2019/06/17/VH5N01.md.png" alt="VH5N01.md.png"></a></p><p>由下图可以更明显的看出，当$k \geq 3，N \geq 2$时，$N^{k-1}$大于$B(N,k)$。</p><p><img src="https://s2.ax1x.com/2019/06/17/VHI84f.png" alt="VHI84f.png"></p><p>因此这条结论的不等式为：</p><p>​                                                          $m_{H}(\mathrm{N})\leq B(\mathrm{N}, \mathrm{k})=\sum_{i=0}^{k-1} C_{N}^{i} \leq N^{k-1}$</p><p>因此上一周的VC bound就可以转换为只和K，N有关的式子：</p><p><a href="https://imgchr.com/i/VHTZOH" target="_blank" rel="noopener"><img src="https://s2.ax1x.com/2019/06/17/VHTZOH.md.png" alt="VHTZOH.md.png"></a></p><p>通常情况N都足够大，因此只需要k满足条件即可。这时候我们就说$h$泛化能力不错，即$E_{o u t} \approx E_{i n}$。如果再有一个好的演算法$A$使得$E_{in} \approx 0$，那么理论上就这个模型就具有学习能力了。（真的好不好用还得要点运气….）</p><h3 id="VC-dimension"><a href="#VC-dimension" class="headerlink" title="VC dimension"></a>VC dimension</h3><p>通常来说break point越大的$H$，其复杂度也越高，这个复杂度比较抽象，通俗理解，就是$H$中的$h$，特征多，数量多，复杂度越高。</p><blockquote><p>这篇文章<a href="https://www.jianshu.com/p/cbe8e0fe7b2c" target="_blank" rel="noopener">《机器学习（周志华）》学习笔记（一）</a>里对假设空间$H$的说明可以参考一下，能够帮助直观理解$H$的复杂度。</p></blockquote><p>根据VC Theory，还可以用<strong>VC-dimension</strong>来描述$H$的复杂度，记为$d_{v c}(\mathcal{H})$。更重要的是</p><p>$k=d_{vc}(\mathcal{H})+1$，也就是说，$d_{vc}(\mathcal{H})$是$\mathcal{H}$能够shatter掉的数目，也就是比break point小1的点。如果不管多少个点$\mathcal{H}$都能shatter，那么$d_{v c}(H)=\infty$。</p><p>因此之前的条件，我们都可以把k替换成$d_{v c}(\mathcal{H})$：</p><p><img src="https://s2.ax1x.com/2019/06/17/VHHTwd.png" alt="VHHTwd.png"></p><p>那么无论采取什么样的$\mathcal{A}$，输入，目标函数，$d_{v c}(\mathcal{H})$都只和$H$有关，只要它finite，那么至少能保证$E_{o u t} \approx E_{i n}$。</p><h2 id="VC-Dimension-of-Perceptrons"><a href="#VC-Dimension-of-Perceptrons" class="headerlink" title="VC Dimension of Perceptrons"></a>VC Dimension of Perceptrons</h2><p>通过前面的证明，我们知道在2D情况下$d_{v c}=3$，PLA算法是可以学习的：</p><p><a href="https://imgchr.com/i/VbixXj" target="_blank" rel="noopener"><img src="https://s2.ax1x.com/2019/06/17/VbixXj.md.png" alt="VbixXj.md.png"></a></p><p>那么多个feature或者说多维的情况下，对应的$d_{v c}$又是多少？我们先给出结论（1D perceptron（pos/neg rays）$d_{vc}=2$；2D perceptrons $d_{vc}=3$，其中$d$为维数）：</p><p>​                                                                    $d_{v c}=d+1$</p><p>证明”=”通常我们分两步：</p><ul><li>$d_{v c} \geq d+1$</li><li>$d_{v c} \leq d+1$</li></ul><p>首先证明第一个不等式：$d_{v c} \geq d+1$，因为$d_{vc}$是break point-1，也就是$m_{H}(N)=2^{N}$成立且最大时的N值，所以要证明这个不等式，只需要证明存在<strong>某一个</strong>含有$d+1$个数据的数据集能够被shatter即可，即</p><p>$m_{H}(d+1)=2^{d+1}$。</p><p>假设我们有一个$d$维的矩阵$X$，含有$d+1$个inputs，每个inputs加上偏置项1，得到的新的矩阵$X$如下：</p><p><a href="https://imgchr.com/i/VLzFD1" target="_blank" rel="noopener"><img src="https://s2.ax1x.com/2019/06/19/VLzFD1.md.png" alt="VLzFD1.md.png"></a></p><p>可以明显看出$X$是可逆的，shatter的本质是假设空间$\mathcal{H}$里的每个$h$对$X$都能进行分类，也就是说总能找到权重$W$，满足$\operatorname{sign}(\mathbf{X} \mathbf{w})=\mathbf{y}$，而只要使得$\mathbf{X}_{\mathbf{W}}=\mathbf{y}$成立，$sign(\mathbf{X}_{\mathbf{W}})=\mathbf{y}$就一定成立，那么这个权重$W$存在吗？答案显而易见，只要令$W=X^{-1}y$（别忘了这里$X$可逆），那么$\mathbf{X}_{\mathbf{W}}=\mathbf{y}$就一定成立。</p><p>结论：$d$维 perceptrons，存在$N=d+1$时被shattered，因此$d_{v c} \geq d+1$。</p><p>接着证明第二个不等式$d_{v c} \leq d+1$。在$d$维里，对于<strong>任意</strong>的d+2个inputs都不能被shattered，则不等式成立。</p><p><a href="https://imgchr.com/i/VOCffK" target="_blank" rel="noopener"><img src="https://s2.ax1x.com/2019/06/19/VOCffK.md.png" alt="VOCffK.md.png"></a></p><p>添加偏置项后$X$为$d+1$列，矩阵秩不满，因此$\mathbf{X}_{d+2}$可以由$a_{1} \mathbf{x}_{1}+a_{2} \mathbf{x}_{2}+\ldots+a_{d+1} \mathbf{x}_{d+1}$线性表示：</p><p>$\mathbf{x}_{d+2}=a_{1} \mathbf{x}_{1}+a_{2} \mathbf{x}_{2}+\ldots+a_{d+1} \mathbf{x}_{d+1}$………………….(1)</p><p>现在用反证法来证明，我们假设$X$能被shattered，且存在一个$W$，使得$W^{T}X$的值(也就是y值，+1或-1)和系数$a_{i}$(i=1,2,….,d+1)符号一致。结合公式(1)得出：</p><p><img src="https://s2.ax1x.com/2019/06/19/VOk2uD.png" alt="VOk2uD.png"></p><p>其中，蓝色$a_{i}$为正，红色$a_{i}$为负。</p><p>但是由上面可以知道，此时$\mathbf{w}^{T} \mathbf{x}_{d+2} &gt;0$是恒成立的，也就是说第$\mathbf{w}^{T}\mathbf{x}_{d+2}$只能为”o”不能为”x”。同理，当</p><p>$\mathbf{w}^{T} \mathbf{x}_{d+2}&lt;0$，则$\mathbf{w}^{T}\mathbf{x}_{d+2}$只能为“x”，不能为“o”。</p><p>这就违背了shattered的原则，假设不成立，任意$d+2$个inputs都不可能被shattered。所以$d_{v c} \leq d+1$。</p><p>综上所述：d维perceptrons，$d_{v c}=d+1$。</p><h2 id="Physical-Intuition-of-VC-Dimension"><a href="#Physical-Intuition-of-VC-Dimension" class="headerlink" title="Physical Intuition of VC Dimension"></a>Physical Intuition of VC Dimension</h2><p>从上一part我们得知VC Dimension中的Dimension和perceptron中数据的Dimension是有关的，这也是VC维中维字的由来。而在perceptron中，数据样本的dimension加上偏置，就和权值的dimension一致，且权值又构成了假设函数。因此如果把hypothesis set想象成一个空间（即假设空间），那么其中的假设函数可以用权值参数向量$\mathbf{w}^{T}=\left[w_{0}, w_{1, \cdots} w_{d}\right]$来表示，向量的个数d就是假设空间的维度，即模型学习的参数个数，也称为”自由度”(degree of freedom)。</p><blockquote><p>对于$d_{vc}$较小的$\mathcal{H}$，可以从它最多能够shatter的点的数量，得到$d_{vc}$，但对于一些较为复杂的模型，寻找能够shatter掉的点的数量，就不太容易了。此时我们可以通过模型的自由度，来近似的得到模型的$d_{vc}$。</p></blockquote><p>因为假设空间的数量$|H|$是无限的，所以自由度也是无限的。当然如果我们从感知器在二元分类上这一限制条件入手，是可以使用VC维作为自由度的衡量。</p><p>举几个直观例子：</p><p><img src="https://s2.ax1x.com/2019/06/19/VO0MS1.png" alt="VO0MS1.png"></p><p>因此，</p><p><a href="https://imgchr.com/i/VO01OK" target="_blank" rel="noopener"><img src="https://s2.ax1x.com/2019/06/19/VO01OK.md.png" alt="VO01OK.md.png"></a></p><p><img src="https://s2.ax1x.com/2019/06/19/VO0znK.png" alt="VO0znK.png"></p><p>最后整理一下，通过这图，我们可以将假设空间中的假设函数个数$M$用$d_{vc}$来代替。这张图里说明了很多问题，比如large $d_{vc}$就证明了在特征很多的情况下，只要样本数量N够多，学习得到的模型$E_{in}(g)$都比较小，错误率也较低。</p><h2 id="Interpreting-VC-Dimension"><a href="#Interpreting-VC-Dimension" class="headerlink" title="Interpreting VC Dimension"></a>Interpreting VC Dimension</h2><p>之前定义过VC Bound：</p><p><a href="https://imgchr.com/i/VO2oBn" target="_blank" rel="noopener"><img src="https://s2.ax1x.com/2019/06/19/VO2oBn.md.png" alt="VO2oBn.md.png"></a></p><p>BAD发生的概率$\leq \delta$，反过来说good，即$\left|E_{\mathrm{in}}(g)-E_{\mathrm{out}}(g)\right|&lt;\epsilon$发生的概率$\geq 1-\delta$。因此我们可以推导出$E_{in}(g)$和$E_{out}(g)$的接近程度$\epsilon$：</p><p><a href="https://imgchr.com/i/VORNbn" target="_blank" rel="noopener"><img src="https://s2.ax1x.com/2019/06/19/VORNbn.md.png" alt="VORNbn.md.png"></a></p><p>我们又把接近程度$\left|E_{\mathrm{in}}(g)-E_{\mathrm{out}}(g)\right|$称为泛化误差(generalization error)，$\epsilon$称为泛化误差界。再进一步，可以确定$E_{out}(g)$的范围：</p><p><a href="https://imgchr.com/i/VOfntP" target="_blank" rel="noopener"><img src="https://s2.ax1x.com/2019/06/19/VOfntP.md.png" alt="VOfntP.md.png"></a></p><p>其中带根号这一项就被称为模型复杂度(model complexity)，由样本数量N，假设空间$\mathcal{H}$($d_{vc}$)，$\delta$有关。我们通常不关注左边灰色的式子，因此得到：</p><p><a href="https://imgchr.com/i/VOfcA1" target="_blank" rel="noopener"><img src="https://s2.ax1x.com/2019/06/19/VOfcA1.md.png" alt="VOfcA1.md.png"></a></p><p>可以绘制出$E_{out}(g)$、模型复杂度、$E_{in}(g)$随$d_{vc}$变化的函数图像：</p><p><a href="https://imgchr.com/i/VOhXI1" target="_blank" rel="noopener"><img src="https://s2.ax1x.com/2019/06/19/VOhXI1.md.png" alt="VOhXI1.md.png"></a></p><p>从图中可以看出，$d_{vc}$越大，假设空间就越大，就有可能选到更小的$E_{in}$；模型复杂度从公式中也可以看出和$d_{vc}$正相关。而我们希望的是能够让$E_{out}$最小，因此在学习中如何选择到$d_{vc}^{*}$就非常关键。</p><h3 id="sample-complexity"><a href="#sample-complexity" class="headerlink" title="sample complexity"></a>sample complexity</h3><p>VC Dimension还可以表示样本复杂度(sample complexity)。如果选定了$d_{vc}$，那么数据样本D选择多少合适？</p><p>假设你帮老板进行股票分析，老板要求：$E_{in}(g)$和$E_{out}(g)$之间的误差上限$\epsilon=0.1$；置信区间90%，即$\delta=0.1$；所用模型的VC Dimension $d_{vc}=3$。你经过一番计算，跟老板汇报：</p><p><a href="https://imgchr.com/i/VOTsBD" target="_blank" rel="noopener"><img src="https://s2.ax1x.com/2019/06/19/VOTsBD.md.png" alt="VOTsBD.md.png"></a></p><p>如果想要满足条件，大概需要30000条数据作为训练集，也就是10000$d_{vc}$。这么多条数据，显然老板也无法解决这个问题。</p><p>好消息是，实际经验告诉我们，其实需要的数据量大概是10$d_{vc}$。那么为什么导致理论值和实际值差别如此大？原因是我们再推导VC Bound的时候考虑的都是比较泛化的情况，换句话说VC Bound过于”宽松“了，回顾我们的推导过程：</p><p><a href="https://imgchr.com/i/VOHTYj" target="_blank" rel="noopener"><img src="https://s2.ax1x.com/2019/06/19/VOHTYj.md.png" alt="VOHTYj.md.png"></a></p><p>加个中文版的：</p><p><a href="https://imgchr.com/i/VO7vid" target="_blank" rel="noopener"><img src="https://s2.ax1x.com/2019/06/19/VO7vid.md.png" alt="VO7vid.md.png"></a></p><p>为了能够适用于这么多的‘any’，导致了VC Bound的理论值变得非常宽松。因此我们得到的是一个比实际上大得多的上界。尽管VC Bound看起来非常宽松，但也很难找到一个比它更好的模型。值得欣慰的是，对于不同模型，VC Bound的宽松程度还是基本一致的。尽管在实际使用时我们不会严格根据VC Bound来选择，但它的推导过程还是能够帮助进一步理解为什么机器学习是成立的。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;https://imgchr.com/i/VHVk0s&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;&lt;img src=&quot;https://s2.ax1x.com/2019/06/17/VHVk0s.md.png&quot; alt=&quot;VHVk0s.md.png&quot;&gt;&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="Machine Learning" scheme="http://yoursite.com/tags/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>机器学习基石4|机器学习的可行性</title>
    <link href="http://yoursite.com/2019/06/14/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B34-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%8F%AF%E8%A1%8C%E6%80%A7/"/>
    <id>http://yoursite.com/2019/06/14/机器学习基石4-机器学习的可行性/</id>
    <published>2019-06-14T14:33:56.000Z</published>
    <updated>2019-06-14T14:33:56.609Z</updated>
    
    <content type="html"><![CDATA[<p>机器学习基石在证明过程中的思想，可以类比于集合论的思想，比如$f$是最完美的模型(函数)，那么$h$是<strong>假设函数空间(hypothesis  set)</strong>$H$中的某个假设函数，$g$则是最接近$f$的那个$h$。</p><h2 id="机器学习真的可行吗？"><a href="#机器学习真的可行吗？" class="headerlink" title="机器学习真的可行吗？"></a>机器学习真的可行吗？</h2><p>我们举个例子：</p><p><img src="https://s2.ax1x.com/2019/06/13/Vfj4oV.png" alt="Vfj4oV.png"></p><a id="more"></a><p>输入特征x是二进制、三维的，对应有8种输入，输出为y。现在假设其中5个为训练样本，有8个hypothesis在训练集D上，对应的输出都完全正确。因此在已知数据D上，$g \approx f$。但是在D以外的另外3个数据集上，我们完全无法预测对应的输出应该是哪个。</p><p>这样看起来我们似乎只能保证在D上有很好的分类效果，机器学习的这种特性又被称为没有免费午餐定理(NFL,No Free Launch)。关于NFL定理：</p><blockquote><p>NFL定理最重要意义是，在脱离实际意义情况下，空泛地谈论哪种算法好毫无意义，要谈论算法优劣必须针对具体学习问题。——百度百科</p></blockquote><p>上面的例子的确很绝望，模型根本没有泛化能力。当然这只是针对这个例子，在研究其他问题时，模型是可以泛化的不错的，接下来就要证明这个。</p><h2 id="推断未知世界"><a href="#推断未知世界" class="headerlink" title="推断未知世界"></a>推断未知世界</h2><p>换一个场景，我们现在在摸球：</p><p><img src="https://s2.ax1x.com/2019/06/13/VfzaL9.png" alt="VfzaL9.png"></p><ul><li>bin为总体，其中$P(\text {orange})=\mu$，$P(\text {green})=1-\mu$。但我们不知道$\mu$具体指。</li><li>sample从bin中提取的样本，样本个数为$N$，其中orange的比例为$\mathcal{V}$，green的比例为$1-\nu$，$\nu$是可以计算的。</li></ul><p>那么$\mu \approx \nu$？在概率论中，需要满足<a href="https://en.wikipedia.org/wiki/Hoeffding&#39;s_inequality" target="_blank" rel="noopener">Hoeffding`s Inequality</a>：</p><p>​                                                                $\mathbb{P}[|\nu-\mu|&gt;\epsilon] \leq 2 \exp \left(-2 \epsilon^{2} N\right)$</p><p>注：$\epsilon$是容忍度，当$\mu$和$\nu$的差别小于容忍度时，我们称$\mu$和$\nu$”差不多“(PAC，Probably approximately correct)。公式中描述的是$\mu$和$\nu$的差别大于容忍度的概率，我们当然希望它越小越好。观察右边发现，$\epsilon^{2}$增大(looser gap)或是$N$增大(增加样本)都能使得概率的上限减小，从而使得$\mu \approx \nu$。</p><h2 id="Connection-to-Learning"><a href="#Connection-to-Learning" class="headerlink" title="Connection to Learning"></a>Connection to Learning</h2><p>我们把learning和抓球问题结合起来。</p><p><img src="https://s2.ax1x.com/2019/06/13/VhAezt.png" alt="VhAezt.png"></p><p>罐子相当于世界上所有有关该问题的<strong>样本空间</strong>，里面的每个球代表着每个样本，从样本中抽取的N个球类比于我们所拥有的<strong>训练样本</strong>。橙色的球代表着用模型预测数据后$h\left(x\right) \neq f\left(x\right)$，绿色的球代表$h\left(x\right)=f\left(x\right)$。根据上一part结论，如果抽样样本N够大，那么就可以从抽样样本中的$h(x) \neq f(x)$的概率来推导<strong>抽样样本以外的所有样本</strong></p><p>$h(x) \neq f(x)$的概率。</p><p>从数学角度上看，这里的$h(x) \neq f(x)$可以看成是一个error，则可以称$h(x)$在sample中出现error的比例记为$E_{i n}$(in-sample-error)，在总体上的error所占比例记为$E_{o u t}$(out-of-sample-error)，则有：</p><p><img src="https://s2.ax1x.com/2019/06/13/VhnXmq.png" alt="VhnXmq.png"></p><p>同样，它的Hoeffding`s Inequality可以表示为：</p><p>$P\left[\left|E_{i n}(h)-E_{o u t}(h)\right|&gt;\epsilon\right] \leq 2 \exp \left(-2 \epsilon^{2} N\right)$</p><p>因此，$E_{i n}(h)$和$E_{o u t}(h)$也满足PAC，这时候机器学习的模型预测会比较准确。</p><p>但这里仅仅针对一个固定的$h$(fixed h)而言，$E_{i n}(h)$和$E_{o u t}(h)$很接近。但这不能说是一个好的learning，因为$E_{in}(h)$可能很大，导致$E_{o u t}(h)$也很大。因此我们的算法$A$应该能从$H$中选择出最好的$h$，记为$g$(final hypothesis)因此需要添加一个验证流程(verification flow)，也就是用非训练集D里的数据来对$h$进行验证。</p><p><img src="https://s2.ax1x.com/2019/06/13/VhQX2F.md.png" alt="VhQX2F.md.png"></p><h2 id="Bad-Sample"><a href="#Bad-Sample" class="headerlink" title="Bad Sample"></a>Bad Sample</h2><p>$A$能够自由在$H$中挑选最合适的$h$，因此每个$h$($h_{1}、h_{2}…$)都有可能成为我们想要的$g$。但是我们的$D$只是来自于总体的一个抽样样本，因此必然存在抽样误差。比如你想知道抛硬币正面的概率，但有时候你连续抛了20次正面，能说明正面的概率为1吗？显然不行，因此这10次硬币，就是一个bad sample。</p><p>或者我们看不同的瓶子：</p><p><img src="https://s2.ax1x.com/2019/06/13/VhDMo8.md.png" alt="VhDMo8.md.png"></p><p>每次的$h$都不相同，而对于$h_{M}$来说，它在抽样数据$D_{M}$上的准确率为100%（都是绿球）。能说明样本空间中绿球的比例是1吗？答案显然是不行。对于learning 而言，其实Bad Sample就是$E_{in}$和$E_{out}$差别很大的情况。</p><p>我们之前说过，Hoeffding’s inequality保证了大多数的$D$都满足$E_{i n} \approx E_{o u t}$。但凡事总有个意外：</p><p><img src="https://s2.ax1x.com/2019/06/13/VhrCXn.md.png" alt="VhrCXn.md.png"></p><p>可以看出，对不同的$D_{n}$和$h_{n}$，都有可能成为Bad Sample，其上界可以表示为连级(union bound)的形式：</p><p><img src="https://s2.ax1x.com/2019/06/13/Vhrt9e.md.png" alt="Vhrt9e.md.png"></p><p>其中M为hypothesis的个数，N是样本$D$的数量，$\epsilon$是容忍度。union bound表明，当M有限，N足够大时。Bad Sample出现的概率比较低。如果这时能找到一个$g$，使得$E_{i n} \approx 0$，PAC就能保证$E_{o u t} \approx 0$。证明机器学习是可行的。</p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p>[1] <a href="http://beader.me/mlnotebook/section2/is-learning-feasible.html" target="_blank" rel="noopener">机器学习的可行性</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;机器学习基石在证明过程中的思想，可以类比于集合论的思想，比如$f$是最完美的模型(函数)，那么$h$是&lt;strong&gt;假设函数空间(hypothesis  set)&lt;/strong&gt;$H$中的某个假设函数，$g$则是最接近$f$的那个$h$。&lt;/p&gt;
&lt;h2 id=&quot;机器学习真的可行吗？&quot;&gt;&lt;a href=&quot;#机器学习真的可行吗？&quot; class=&quot;headerlink&quot; title=&quot;机器学习真的可行吗？&quot;&gt;&lt;/a&gt;机器学习真的可行吗？&lt;/h2&gt;&lt;p&gt;我们举个例子：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://s2.ax1x.com/2019/06/13/Vfj4oV.png&quot; alt=&quot;Vfj4oV.png&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="Machine Learning" scheme="http://yoursite.com/tags/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>PLA修正过程</title>
    <link href="http://yoursite.com/2019/06/13/PLA%E4%BF%AE%E6%AD%A3%E8%BF%87%E7%A8%8B/"/>
    <id>http://yoursite.com/2019/06/13/PLA修正过程/</id>
    <published>2019-06-13T13:51:50.000Z</published>
    <updated>2019-06-13T14:51:27.589Z</updated>
    
    <content type="html"><![CDATA[<p>最近在看<a href="https://www.bilibili.com/video/av12463015/?p=8" target="_blank" rel="noopener">林轩田机器学习基石</a>，作为对ML基础的进一步学习。在2-Learning to Answer Yes_No中，以一个例子来总结解释一下PLA（感知学习算法）的修正过程，以及一些证明。</p><h2 id="PLA"><a href="#PLA" class="headerlink" title="PLA"></a>PLA</h2><p>PLA的主要思想：逐步修正。</p><p>图中坐标原点位于正方形中心位置，样本数据线性可分，决策边界满足$w^{T}x=0$，注意下面谈到的$w$和$x$都是向量。</p><p><strong>step-0</strong>：初始化$w=0$</p><p><img src="https://s2.ax1x.com/2019/06/11/VcQCTg.png" alt="VcQCTg.png"></p><a id="more"></a><p><strong>step-1</strong>：第一次更新，选择样本点$x_{1}$如图所示，记为黑点。更新$w_{t+1}=w_{t}+y*x$，这里圈圈代表y为正，$w_{t}=0$，因此$w_{t+1}=x$。</p><p><img src="https://s2.ax1x.com/2019/06/11/Vcllb8.png" alt="Vcllb8.png"></p><p>又因为$w_{t}x=0$，所以可以画出第一条决策边界如下图。</p><p><strong>step-2</strong>：接下来根据$x_{2}$进行修正，显然它是错误分类的样本点，因此$w_{t}x_{2}&lt;0$，也就是夹角大于90$^{o}$。然后根据上面说的$w$更新规则，可以得到如下图所示紫色的向量$w_{t+1}$，以它作为决策边界的法向量，再次更新决策边界。</p><p><img src="https://s2.ax1x.com/2019/06/11/VclHGd.png" alt="VclHGd.png"></p><p><img src="https://s2.ax1x.com/2019/06/11/Vc1Dyt.png" alt="Vc1Dyt.png"></p><p>之后就是不断循环更新分类错误的点，直到找到合适的分类边界为止。</p><p>但这里还存在一个问题：PLA何时停止？或者说数学上怎样判定找到了合适的分类边界？</p><h2 id="PLA的终止条件"><a href="#PLA的终止条件" class="headerlink" title="PLA的终止条件"></a>PLA的终止条件</h2><p>在线性可分的情况下，假设所有样本正确分类。如果有一条直线，能够将正类和负类完全分开，令这时候的目标权重为$w_{f}$，对于每个点来说，必然满足$y_{n}=\operatorname{sign}\left(w_{f}^{T} x_{n}\right)$，对于任一点：</p><p>$y_{n(t)} \mathbf{w}_{f}^{T} \mathbf{x}_{n(t)} \geq \min _{n} y_{n} \mathbf{w}_{f}^{T} \mathbf{x}_{n}&gt;0$……………………………………………………….(1)</p><p>也就是任一点到分类边界的距离，都大于离分类边界最近点到分类边界的距离。</p><p>上面曾经提到过，PLA是通过逐步修正$w_{t}$，让其逼近$w_{f}$，在数学上体现为$w_{t+1}$和$w_{f}$的内积越来越大，也就是说$\mathbf{w}_{t+1}$和$\mathbf{w}_{f}$之间的夹角要越来越小。因此我们计算：</p><p>$\mathbf{w}_{f}^{T} \mathbf{w}_{t+1}=\mathbf{w}_{f}^{T}\left(\mathbf{w}_{t}+y_{n(t)} \mathbf{x}_{n(t)}\right)\geq\mathbf{w}_{f}^{T} \mathbf{w}_{t}+\min _{n} y_{n} \mathbf{w}_{f}^{T} \mathbf{x}_{n}&gt;\mathbf{w}_{f}^{T} \mathbf{w}_{t}+0$</p><p>可以看出内积的确在增大，但问题是，也有可能是$\mathbf{w}_{t+1}$的模长在增大而并非角度越来越小，因此还需要证明一下$\mathbf{w}_{t+1}$和$\mathbf{w}_{f}$向量长度的关系。首先我们知道：</p><p><img src="https://s2.ax1x.com/2019/06/11/VgPFdP.md.png" alt="VgPFdP.md.png"></p><p>只有当分类错误时，$\mathbf{w}_{t}$才会改变：</p><p>$\operatorname{sign}\left(\mathbf{w}_{t}^{T} \mathbf{x}_{n(t)}\right) \neq y_{n(t)} \Leftrightarrow y_{n(t)} \mathbf{w}_{t}^{T} \mathbf{x}_{n(t)} \leq 0$…………………………………………(2)</p><p>然后计算$\mathbf{w}_{t+1}$的长度，这里我们计算$\left|\mathbf{w}_{t+1}\right|^{2}$，方便很多：</p><p>$\begin{aligned}\left|\mathbf{w}_{t+1}\right|^{2} &amp;=\left|\mathbf{w}_{t}+y_{n(t)} \mathbf{x}_{n(t)}\right|^{2} \\ &amp;=\left|\mathbf{w}_{t}\right|^{2}+2 y_{n(t)} \mathbf{w}_{t}^{T} \mathbf{x}_{n(t)}+\left|y_{n(t)} \mathbf{x}_{n(t)}\right|^{2} \\ &amp; \leq\left|\mathbf{w}_{t}\right|^{2}+0+\left|y_{n(t)} \mathbf{x}_{n(t)}\right|^{2} \\ &amp; \leq\left|\mathbf{w}_{t}\right|^{2}+\max _{n}\left|y_{n} \mathbf{x}_{n}\right|^{2} \end{aligned}$ ……………………………………………..(3)</p><p>因此可以看到长度的增量的限度为：$\max _{n}\left|y_{n} \mathbf{x}_{n}\right|^{2}$。也就是说向量长度的增长非常缓慢，而且有上限。</p><p>至此，我们证明了$\mathbf{w}_{t+1}$的确是在不断逼近$\mathbf{w}_{f}$的，但是要如何证明$\mathbf{w}_{t+1}$何时停止呢？</p><p>这里假设循环次数为$t$，因此我们只需要证明$t$小于某个数即可，而第一步，我们先计算$\mathbf{w}^{T}_{f}$和$\mathbf{w}_{t}$单位向量的内积</p><p>$\frac{w_{f}^{T} }{\left|w_{f}\right|} \frac{w_{t} }{\left|w_{t}\right|}$。根据公式(1)可得：</p><p><img src="https://s2.ax1x.com/2019/06/11/VggQnx.png" alt="VggQnx.png"></p><p>再根据公式(2)可得</p><p><img src="https://s2.ax1x.com/2019/06/11/Vgg8AO.png" alt="Vgg8AO.png"></p><p>(这里的证明其实跟上面几乎是一样的，只是因为要证明的内容不同，所以有些不一样的地方。)</p><p>整理一下可得出结论：</p><p>$\frac{w_{f}^{T} }{\left|w_{f}\right|} \frac{w_{t} }{\left|w_{t}\right|}\geqslant\frac{t \cdot \min y_{n} w_{f}^{T} x_{n} }{\left|w_{f}^{T}\right| \cdot \sqrt{t} \cdot \max _{n}\left|x_{n}\right|}\geq \sqrt{t}\cdot\frac{  \min _{n} y_{n} w_{f}^{T} x_{n} }{\left|w_{f}^{T}\right| \cdot \max _{n}\left|x_{n}\right|}=\sqrt{t}\cdot c$</p><p>(后面的分数等于常数c，因为和$w_{t}$无关)</p><p>$\frac{w_{f}^{T} }{\left|w_{f}\right|} \frac{w_{t} }{\left|w_{t}\right|}$随着迭代次数的增长，会越来越靠近，当完全重合时，内积等于1。因此</p><p>$\frac{w_{f}^{T} }{\left|w_{f}\right|} \frac{w_{t} }{\left|w_{t}\right|}\le1$=&gt;$\sqrt{t} \cdot c\leq1$=&gt;$t\leq\frac{1}{c^{2} }$，存在上界。如果我们做出如下定义：</p><p><img src="https://s2.ax1x.com/2019/06/11/V2CdYT.md.png" alt="V2CdYT.md.png"></p><p>则$t\le \frac{R^{2} }{\rho^{2} }$。</p><h2 id="非线性可分数据"><a href="#非线性可分数据" class="headerlink" title="非线性可分数据"></a>非线性可分数据</h2><p>PLA适用于线性可分的数据，而对于线性不可分数据。PLA可能无法停止。即便是存在个别”噪音“的数据集，PLA都几乎无法工作。因此我们可以PLA修改，得出一个新算法Packet Algorithm。它的算法流程与PLA基本类似，首先初始化权重，计算出在这条初始化的直线中，分类错误点的个数。然后对错误点进行修正，更新w，得到一条新的直线，在计算其对应的分类错误的点的个数，并与之前错误点个数比较，取个数较小的直线作为我们当前选择的分类直线。之后，再经过n次迭代，不断比较当前分类错误点个数与之前最少的错误点个数比较，选择最小的值保存。直到迭代次数完成后，选取个数最少的直线对应的w，即为我们最终想要得到的权重值。</p><h2 id="参考资料："><a href="#参考资料：" class="headerlink" title="参考资料："></a>参考资料：</h2><p>[1]红色石头的机器学习基石笔记</p><p>链接：<a href="https://pan.baidu.com/s/1uauOV9xyhJkMFao31yWZXg" target="_blank" rel="noopener">https://pan.baidu.com/s/1uauOV9xyhJkMFao31yWZXg</a><br>提取码：kffo </p><p>[2] <a href="https://blog.csdn.net/m0_37411189/article/details/79744912" target="_blank" rel="noopener">感知器学习算法PLA的收敛性证明</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;最近在看&lt;a href=&quot;https://www.bilibili.com/video/av12463015/?p=8&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;林轩田机器学习基石&lt;/a&gt;，作为对ML基础的进一步学习。在2-Learning to Answer Yes_No中，以一个例子来总结解释一下PLA（感知学习算法）的修正过程，以及一些证明。&lt;/p&gt;
&lt;h2 id=&quot;PLA&quot;&gt;&lt;a href=&quot;#PLA&quot; class=&quot;headerlink&quot; title=&quot;PLA&quot;&gt;&lt;/a&gt;PLA&lt;/h2&gt;&lt;p&gt;PLA的主要思想：逐步修正。&lt;/p&gt;
&lt;p&gt;图中坐标原点位于正方形中心位置，样本数据线性可分，决策边界满足$w^{T}x=0$，注意下面谈到的$w$和$x$都是向量。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;step-0&lt;/strong&gt;：初始化$w=0$&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://s2.ax1x.com/2019/06/11/VcQCTg.png&quot; alt=&quot;VcQCTg.png&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="Machine Learning" scheme="http://yoursite.com/tags/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Python-for-Data-Analysis|3.2 函数</title>
    <link href="http://yoursite.com/2019/05/15/3-2Functions/"/>
    <id>http://yoursite.com/2019/05/15/3-2Functions/</id>
    <published>2019-05-15T12:46:26.000Z</published>
    <updated>2019-05-15T12:49:11.822Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://s2.ax1x.com/2019/05/15/E7phpF.png" alt="E7phpF.png"></p><a id="more"></a><h1 id="Functions"><a href="#Functions" class="headerlink" title="Functions"></a>Functions</h1><blockquote><p>如果函数不带有返回值，那么会自动返回一个<code>None</code>。</p></blockquote><p>每个函数都含有位置参数和关键字参数，关键字参数通常指明了默认值。下面例子中的<code>z=0.7</code>就是关键字参数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">my_function</span><span class="params">(x, y, z=<span class="number">1.5</span>)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> z &gt; <span class="number">1</span>:</span><br><span class="line">        <span class="keyword">return</span> z * (x + y)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> z / (x + y)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">my_function(<span class="number">5</span>, <span class="number">6</span>, z=<span class="number">0.7</span>)</span><br><span class="line">my_function(<span class="number">3.14</span>, <span class="number">7</span>, <span class="number">3.5</span>)</span><br><span class="line">my_function(<span class="number">10</span>, <span class="number">20</span>)</span><br></pre></td></tr></table></figure><pre><code>45.0</code></pre><h2 id="Namespaces-Scope-and-Local-Functions"><a href="#Namespaces-Scope-and-Local-Functions" class="headerlink" title="Namespaces,Scope,and Local Functions"></a>Namespaces,Scope,and Local Functions</h2><p>在函数体中的变量为局部变量，函数被调用时，局部命名空间(local namespaces)被创建，但函数结束时被释放。</p><h2 id="Returning-Multiple-Values"><a href="#Returning-Multiple-Values" class="headerlink" title="Returning Multiple Values"></a>Returning Multiple Values</h2><p>返回多个值的原理：实际上返回的是一个tuple，这些元组被unpack后赋给不同的变量</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">()</span>:</span></span><br><span class="line">    a = <span class="number">5</span></span><br><span class="line">    b = <span class="number">6</span></span><br><span class="line">    c = <span class="number">7</span></span><br><span class="line">    <span class="keyword">return</span> a, b, c</span><br><span class="line"></span><br><span class="line">a, b, c = f()</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">return_value=f()</span><br><span class="line">print(return_value)</span><br></pre></td></tr></table></figure><pre><code>(5, 6, 7)</code></pre><h2 id="Functions-Are-Objects"><a href="#Functions-Are-Objects" class="headerlink" title="Functions Are Objects"></a>Functions Are Objects</h2><p>可以用<strong>列表函数</strong>的方法作用于指定的对象</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">states = [<span class="string">'   Alabama '</span>, <span class="string">'Georgia!'</span>, <span class="string">'Georgia'</span>, <span class="string">'georgia'</span>, <span class="string">'FlOrIda'</span>,</span><br><span class="line">          <span class="string">'south   carolina##'</span>, <span class="string">'West virginia?'</span>]</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">remove_punctuation</span><span class="params">(value)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> re.sub(<span class="string">'[!#?]'</span>, <span class="string">''</span>, value)</span><br><span class="line"></span><br><span class="line">clean_ops = [str.strip, remove_punctuation, str.title]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">clean_strings</span><span class="params">(strings, ops)</span>:</span></span><br><span class="line">    result = []</span><br><span class="line">    <span class="keyword">for</span> value <span class="keyword">in</span> strings:</span><br><span class="line">        <span class="keyword">for</span> function <span class="keyword">in</span> ops:</span><br><span class="line">            value = function(value)</span><br><span class="line">        result.append(value)</span><br><span class="line">    <span class="keyword">return</span> result</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">clean_strings(states,clean_ops)</span><br></pre></td></tr></table></figure><pre><code>[&#39;Alabama&#39;, &#39;Georgia&#39;, &#39;Georgia&#39;, &#39;Georgia&#39;, &#39;Florida&#39;, &#39;South   Carolina&#39;, &#39;West Virginia&#39;]</code></pre><p>你可以用函数作为<code>map</code>函数的参数，<code>map</code>的作用是将函数运用到某个序列中</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> map(remove_punctuation, states):</span><br><span class="line">    print(x)</span><br></pre></td></tr></table></figure><pre><code>   Alabama GeorgiaGeorgiageorgiaFlOrIdasouth   carolinaWest virginia</code></pre><h2 id="Anonymous-Lambda-Functions"><a href="#Anonymous-Lambda-Functions" class="headerlink" title="Anonymous(Lambda) Functions"></a>Anonymous(Lambda) Functions</h2><p><code>lambda</code>关键字除了表示“我们申明了一个匿名函数”之外没有任何其他含义</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">apply_to_list</span><span class="params">(some_list, f)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> [f(x) <span class="keyword">for</span> x <span class="keyword">in</span> some_list]</span><br><span class="line"></span><br><span class="line">ints = [<span class="number">4</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">5</span>, <span class="number">6</span>]</span><br><span class="line">apply_to_list(ints, <span class="keyword">lambda</span> x: x * <span class="number">2</span>)</span><br></pre></td></tr></table></figure><pre><code>[8, 0, 2, 10, 12]</code></pre><h2 id="Currying：Partial-Argument-Application"><a href="#Currying：Partial-Argument-Application" class="headerlink" title="Currying：Partial Argument Application"></a>Currying：Partial Argument Application</h2><p>柯里化（currying）是一个有趣的计算机科学术语，它指的是通过“部分参数应用”（partial argument application）从现有函数派生出新函数的技术。</p><h2 id="Generators"><a href="#Generators" class="headerlink" title="Generators"></a>Generators</h2><p>用一致的方式在序列中进行迭代，是<code>python</code>的重要特征。这个过程是由迭代协议(<strong>iterator protocol</strong>)，一种原始的使对象迭代的方法。</p><p>关于迭代器更多内容：<a href="https://foofish.net/how-for-works-in-python.html" target="_blank" rel="noopener">for循环在Python中是怎么工作的</a></p><p>生成器延时返回一个序列，每返回一个值便暂停直到下一个值被请求。这和普通函数执行并且返回一个单一的值正好相反。</p><h2 id="Errors-and-Exception-Handing"><a href="#Errors-and-Exception-Handing" class="headerlink" title="Errors and Exception Handing"></a>Errors and Exception Handing</h2><p>python中的<code>float</code>函数用来把一个<code>string</code>类型的值转换成<code>float</code>，注意观察下面的第二个例子中的错误是一<code>ValueError</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">float(<span class="string">'1.2345'</span>)</span><br></pre></td></tr></table></figure><pre><code>1.2345</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">float(<span class="string">'something'</span>)</span><br></pre></td></tr></table></figure><pre><code>---------------------------------------------------------------------------ValueError                                Traceback (most recent call last)&lt;ipython-input-22-2649e4ade0e6&gt; in &lt;module&gt;----&gt; 1 float(&#39;something&#39;)ValueError: could not convert string to float: &#39;something&#39;</code></pre><p>可以用<code>except</code>来处理错误信息。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">attempt_float</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        <span class="keyword">return</span> float(x)</span><br><span class="line">    <span class="keyword">except</span> (ValueError,TypeError):</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure><p>此外还有<code>else</code>：当try的内容成功执行后执行<code>else</code>；<code>finally</code>：无论<code>try</code>是否成功执行，都会执行<code>finally</code></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;https://s2.ax1x.com/2019/05/15/E7phpF.png&quot; alt=&quot;E7phpF.png&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="Machine Learning" scheme="http://yoursite.com/tags/Machine-Learning/"/>
    
      <category term="Python" scheme="http://yoursite.com/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>Python-for-Data-Analysis|3.1数据结构和序列</title>
    <link href="http://yoursite.com/2019/05/15/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E5%92%8C%E5%BA%8F%E5%88%97/"/>
    <id>http://yoursite.com/2019/05/15/数据结构和序列/</id>
    <published>2019-05-15T08:43:39.000Z</published>
    <updated>2019-05-15T08:49:00.756Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Data-Structures-and-Sequences"><a href="#Data-Structures-and-Sequences" class="headerlink" title="Data Structures and Sequences"></a>Data Structures and Sequences</h1><p><img src="https://s2.ax1x.com/2019/05/15/E7phpF.png" alt="E7phpF.png"></p><a id="more"></a><p>从python中基础的数据结构开始：元组(tuple)、列表(list)、字典(dict)和集合(set)</p><h2 id="Tuple"><a href="#Tuple" class="headerlink" title="Tuple"></a>Tuple</h2><p>元组的长度固定，序列不可更多。可以将任何序列或是迭代器转换成元组。</p><p>即便元组存储的是可变对象，一旦元组被创建，可变对象也将变得不可修改。例如：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tup = tuple([<span class="string">'foo'</span>, [<span class="number">1</span>, <span class="number">2</span>], <span class="keyword">True</span>])</span><br><span class="line">tup[<span class="number">2</span>] = <span class="keyword">False</span></span><br></pre></td></tr></table></figure><pre><code>---------------------------------------------------------------------------TypeError                                 Traceback (most recent call last)&lt;ipython-input-2-11b694945ab9&gt; in &lt;module&gt;      1 tup = tuple([&#39;foo&#39;, [1, 2], True])----&gt; 2 tup[2] = FalseTypeError: &#39;tuple&#39; object does not support item assignment</code></pre><p>但是如果tuple中的某个元素是可变对象，那么可以修改这个对象中的内容。例如：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tup[<span class="number">1</span>].append(<span class="number">3</span>)</span><br><span class="line">tup</span><br></pre></td></tr></table></figure><pre><code>(&#39;foo&#39;, [1, 2, 3], True)</code></pre><h3 id="Unpacking-tuples"><a href="#Unpacking-tuples" class="headerlink" title="Unpacking tuples"></a>Unpacking tuples</h3><p>元组可以解包，即使是嵌套的元组：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tup=<span class="number">4</span>,<span class="number">5</span>,(<span class="number">6</span>,<span class="number">7</span>)</span><br><span class="line">a,b,(c,d)=tup</span><br><span class="line">d</span><br></pre></td></tr></table></figure><pre><code>7</code></pre><p>python允许从元组的开头抓取几个元素，剩余舍去的元素存在<code>rest</code>中，例如：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">values=<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span></span><br><span class="line">a,b,*rest=values</span><br><span class="line">rest</span><br></pre></td></tr></table></figure><pre><code>[3, 4, 5]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a,b</span><br></pre></td></tr></table></figure><pre><code>(1, 2)</code></pre><p>可以看出，舍去的元素存储在列表中，抓取的元素存储在元组中</p><h3 id="Tuple-methods"><a href="#Tuple-methods" class="headerlink" title="Tuple methods"></a>Tuple methods</h3><p>鉴于元组的内容不可修改，一个特别有用的方法是<code>count</code>,用来计算某个value出现的次数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a=(<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">2</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a.count(<span class="number">2</span>)</span><br></pre></td></tr></table></figure><pre><code>4</code></pre><h2 id="List"><a href="#List" class="headerlink" title="List"></a>List</h2><h3 id="Adding-and-removing-elements"><a href="#Adding-and-removing-elements" class="headerlink" title="Adding and removing elements"></a>Adding and removing elements</h3><p>插入方法有两种：<code>append</code>和<code>insert</code>，其中<code>append</code>在尾部插入而<code>insert</code>可以指定位置插入。</p><blockquote><p><code>insert</code>方法比起<code>append</code>需要花费更多的计算开销，因为任意位置插入需要移动后续的元素。</p></blockquote><p><code>insert</code>的逆运算是<code>pop</code>，它移除并返回指定位置的元素。还可以用<code>remove</code>来去除，它会去除寻找到的第一个值。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">b_list=[<span class="string">'foo'</span>,<span class="string">'red'</span>,<span class="string">'baz'</span>,<span class="string">'dwarf'</span>]</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">b_list.append(<span class="string">'foo'</span>)</span><br><span class="line">b_list</span><br></pre></td></tr></table></figure><pre><code>[&#39;foo&#39;, &#39;red&#39;, &#39;baz&#39;, &#39;dwarf&#39;, &#39;foo&#39;]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">b_list.remove(<span class="string">'foo'</span>)</span><br><span class="line">b_list</span><br></pre></td></tr></table></figure><pre><code>[&#39;red&#39;, &#39;baz&#39;, &#39;dwarf&#39;, &#39;foo&#39;]</code></pre><h3 id="Concatenating-and-combining-lists"><a href="#Concatenating-and-combining-lists" class="headerlink" title="Concatenating and combining lists"></a>Concatenating and combining lists</h3><p>可以用<code>+</code>和<code>extend</code>的方法串联列表，<code>+</code>的计算开销较大，因为需要先创建一个新列表，再将需要串联的两个列表中的元素复制进去。而<code>extend</code>则是在后面追加。</p><h3 id="Binary-search-and-maintaining-a-sorted-list"><a href="#Binary-search-and-maintaining-a-sorted-list" class="headerlink" title="Binary search and maintaining a sorted list"></a>Binary search and maintaining a sorted list</h3><p>内置的<code>bisect</code>模块可以找到插入值的位置，这个值在该位置插入后不会影响排序</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> bisect</span><br><span class="line">c = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">7</span>]</span><br><span class="line">bisect.bisect(c, <span class="number">2</span>)</span><br><span class="line">bisect.bisect(c, <span class="number">5</span>)</span><br><span class="line">bisect.insort(c, <span class="number">6</span>)</span><br><span class="line">c</span><br></pre></td></tr></table></figure><pre><code>[1, 2, 2, 2, 3, 4, 6, 7]</code></pre><blockquote><p>需要注意的是，bisec不会去检测列表是否已经有序，因此如果对无序列表进行该操作会引发逻辑错误。</p></blockquote><h3 id="Slicing"><a href="#Slicing" class="headerlink" title="Slicing"></a>Slicing</h3><p>切分后元素的个数可以用stop-start得到。</p><h2 id="Bulid-in-Sequence-Functions"><a href="#Bulid-in-Sequence-Functions" class="headerlink" title="Bulid-in Sequence Functions"></a>Bulid-in Sequence Functions</h2><h3 id="enumerate"><a href="#enumerate" class="headerlink" title="enumerate"></a>enumerate</h3><p><code>enumerate</code>枚举可以返回一个字典，其中的key-value对应列表中的元素位置和元素值。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">some_list=[<span class="string">'foo'</span>,<span class="string">'bar'</span>,<span class="string">'baz'</span>]</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mapping=&#123;&#125;</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i,v <span class="keyword">in</span> enumerate(some_list):</span><br><span class="line">    mapping[v]=i</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mapping</span><br></pre></td></tr></table></figure><pre><code>{&#39;foo&#39;: 0, &#39;bar&#39;: 1, &#39;baz&#39;: 2}</code></pre><h3 id="sorted"><a href="#sorted" class="headerlink" title="sorted"></a>sorted</h3><p><code>sorted</code>函数排序任意序列的元素值并返回一个<strong>新的排序序列</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">test=[<span class="number">7</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">6</span>,<span class="number">0</span>,<span class="number">3</span>,<span class="number">2</span>]</span><br><span class="line">sorted(test)</span><br></pre></td></tr></table></figure><pre><code>[0, 1, 2, 2, 3, 6, 7]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(test)</span><br></pre></td></tr></table></figure><pre><code>[7, 1, 2, 6, 0, 3, 2]</code></pre><h3 id="zip"><a href="#zip" class="headerlink" title="zip"></a>zip</h3><p>zip可以将多个列表、元组或其它序列成对组合成一个元组列表：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">seq1 = [<span class="string">'foo'</span>, <span class="string">'bar'</span>, <span class="string">'baz'</span>]</span><br><span class="line">seq2 = [<span class="string">'one'</span>, <span class="string">'two'</span>, <span class="string">'three'</span>]</span><br><span class="line">zipped = zip(seq1, seq2)</span><br><span class="line">list(zipped)</span><br></pre></td></tr></table></figure><pre><code>[(&#39;foo&#39;, &#39;one&#39;), (&#39;bar&#39;, &#39;two&#39;), (&#39;baz&#39;, &#39;three&#39;)]</code></pre><h3 id="reversed"><a href="#reversed" class="headerlink" title="reversed"></a>reversed</h3><blockquote><p>reversed 是一个生成器，只有实体化（即列表或for循环）之后才能创建翻转的序列。</p></blockquote><h2 id="dict"><a href="#dict" class="headerlink" title="dict"></a>dict</h2><p>字典也叫哈希表，有key-value形式存储数据。</p><p>可以使用<code>update</code>方法将两个字典混合</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">d1=&#123;<span class="string">'a'</span>:<span class="string">'some value'</span>&#125;</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">d1.update(&#123;<span class="string">'b'</span>:<span class="string">'foo'</span>,<span class="string">'c'</span>:<span class="number">12</span>&#125;)</span><br><span class="line">d1</span><br></pre></td></tr></table></figure><pre><code>{&#39;a&#39;: &#39;some value&#39;, &#39;b&#39;: &#39;foo&#39;, &#39;c&#39;: 12}</code></pre><h3 id="Valid-dict-key-types"><a href="#Valid-dict-key-types" class="headerlink" title="Valid dict key types"></a>Valid dict key types</h3><p>字典的key通常是不可修改的对象，行话来说，就是“可哈希性”。你可以通过<code>hash</code>函数来检测是否对象可以被哈希（是否可以被用作key）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hash(<span class="string">'string'</span>)</span><br></pre></td></tr></table></figure><pre><code>7186817693226238043</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hash([<span class="number">1</span>,<span class="number">2</span>])<span class="comment"># list是可修改的值，因此失败。</span></span><br></pre></td></tr></table></figure><pre><code>---------------------------------------------------------------------------TypeError                                 Traceback (most recent call last)&lt;ipython-input-45-9ce67481a686&gt; in &lt;module&gt;----&gt; 1 hash([1,2])TypeError: unhashable type: &#39;list&#39;</code></pre><h2 id="set"><a href="#set" class="headerlink" title="set"></a>set</h2><p><code>set</code>是无序的不可重复的元素的集合。可以看成是只有key没有value的字典。通用可以有<code>{}</code>创建。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="number">2</span>,<span class="number">2</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>&#125;</span><br></pre></td></tr></table></figure><pre><code>{1, 2, 3}</code></pre><p>当且仅当集合内容相等是，集合相等</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>&#125;==&#123;<span class="number">3</span>,<span class="number">2</span>,<span class="number">1</span>&#125;</span><br></pre></td></tr></table></figure><pre><code>True</code></pre><h2 id="List-Set-and-Dict-Comprehensions"><a href="#List-Set-and-Dict-Comprehensions" class="headerlink" title="List,Set and Dict Comprehensions"></a>List,Set and Dict Comprehensions</h2><p>列表、集合、字典推导式允许用户方便的从一个集合过滤元素，形成列表，在传递参数的过程中还可以修改元素。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[expr <span class="keyword">for</span> val <span class="keyword">in</span> collection <span class="keyword">if</span> condition]</span><br></pre></td></tr></table></figure><p>等同于for循环</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">result = []</span><br><span class="line"><span class="keyword">for</span> val <span class="keyword">in</span> collection:</span><br><span class="line">    <span class="keyword">if</span> condition:</span><br><span class="line">        result.append(expr)</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;Data-Structures-and-Sequences&quot;&gt;&lt;a href=&quot;#Data-Structures-and-Sequences&quot; class=&quot;headerlink&quot; title=&quot;Data Structures and Sequences&quot;&gt;&lt;/a&gt;Data Structures and Sequences&lt;/h1&gt;&lt;p&gt;&lt;img src=&quot;https://s2.ax1x.com/2019/05/15/E7phpF.png&quot; alt=&quot;E7phpF.png&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="Machine Learning" scheme="http://yoursite.com/tags/Machine-Learning/"/>
    
      <category term="Python" scheme="http://yoursite.com/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>Jupyter-notebook路径</title>
    <link href="http://yoursite.com/2019/05/13/Jupyter-notebook%E8%B7%AF%E5%BE%84/"/>
    <id>http://yoursite.com/2019/05/13/Jupyter-notebook路径/</id>
    <published>2019-05-13T12:43:28.000Z</published>
    <updated>2019-05-13T12:54:23.400Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Jupyter-notebook的路径"><a href="#Jupyter-notebook的路径" class="headerlink" title="Jupyter notebook的路径"></a>Jupyter notebook的路径</h3><p>1、你打开jupyter notebook时的路径，即为工作路径。注意下图中的黄色字体部分。<br><img src="https://s2.ax1x.com/2019/05/13/E5DD41.png" alt="E5DD41.png" border="0"></p><a id="more"></a><p>2、注意路径要用斜杠’/‘而不是反斜杠’\\’。</p><h3 id="os-path-join-函数用于路径拼接："><a href="#os-path-join-函数用于路径拼接：" class="headerlink" title="os.path.join()函数用于路径拼接："></a><code>os.path.join()</code>函数用于路径拼接：</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line">m=os.path.join(<span class="string">'path'</span>,<span class="string">'file_name.txt'</span>)</span><br><span class="line">m</span><br></pre></td></tr></table></figure><pre><code>&#39;path\\file_name.txt&#39;</code></pre><p>可以看到输出的是双反斜杠。而我们需要的是斜杠’/‘</p><h4 id="解法一："><a href="#解法一：" class="headerlink" title="解法一："></a>解法一：</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">m.replace(<span class="string">'\\'</span>,<span class="string">'/'</span>)</span><br></pre></td></tr></table></figure><pre><code>&#39;path/file_name.txt&#39;</code></pre><h4 id="解法二"><a href="#解法二" class="headerlink" title="解法二"></a>解法二</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">m=os.path.join(<span class="string">'path/'</span>,<span class="string">'file_name.txt'</span>)</span><br><span class="line">m</span><br></pre></td></tr></table></figure><pre><code>&#39;path/file_name.txt&#39;</code></pre>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;Jupyter-notebook的路径&quot;&gt;&lt;a href=&quot;#Jupyter-notebook的路径&quot; class=&quot;headerlink&quot; title=&quot;Jupyter notebook的路径&quot;&gt;&lt;/a&gt;Jupyter notebook的路径&lt;/h3&gt;&lt;p&gt;1、你打开jupyter notebook时的路径，即为工作路径。注意下图中的黄色字体部分。&lt;br&gt;&lt;img src=&quot;https://s2.ax1x.com/2019/05/13/E5DD41.png&quot; alt=&quot;E5DD41.png&quot; border=&quot;0&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="python" scheme="http://yoursite.com/tags/python/"/>
    
      <category term="Jupyter notebook" scheme="http://yoursite.com/tags/Jupyter-notebook/"/>
    
  </entry>
  
  <entry>
    <title>机器学习笔记|第三周</title>
    <link href="http://yoursite.com/2019/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E7%AC%AC%E4%B8%89%E5%91%A8/"/>
    <id>http://yoursite.com/2019/05/08/机器学习笔记-第三周/</id>
    <published>2019-05-08T03:42:21.000Z</published>
    <updated>2019-05-08T03:48:14.739Z</updated>
    
    <content type="html"><![CDATA[<h2 id="逻辑回归"><a href="#逻辑回归" class="headerlink" title="逻辑回归"></a>逻辑回归</h2><p><img src="https://s2.ax1x.com/2019/05/08/Eyzf58.md.jpg" alt="Eyzf58.md.jpg"></p><a id="more"></a><p><img src="https://s2.ax1x.com/2019/05/08/Eyz4PS.md.jpg" alt="Eyz4PS.md.jpg"></p><p><img src="https://s2.ax1x.com/2019/05/08/EyzI2Q.md.jpg" alt="EyzI2Q.md.jpg"></p><p><img src="https://s2.ax1x.com/2019/05/08/Eyz7Ks.md.jpg" alt="Eyz7Ks.md.jpg"></p><p><img src="https://s2.ax1x.com/2019/05/08/EyzLV0.md.jpg" alt="EyzLV0.md.jpg"></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;逻辑回归&quot;&gt;&lt;a href=&quot;#逻辑回归&quot; class=&quot;headerlink&quot; title=&quot;逻辑回归&quot;&gt;&lt;/a&gt;逻辑回归&lt;/h2&gt;&lt;p&gt;&lt;img src=&quot;https://s2.ax1x.com/2019/05/08/Eyzf58.md.jpg&quot; alt=&quot;Eyzf58.md.jpg&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="Machine Learning" scheme="http://yoursite.com/tags/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>机器学习笔记|第二周</title>
    <link href="http://yoursite.com/2019/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E7%AC%AC%E4%BA%8C%E5%91%A8/"/>
    <id>http://yoursite.com/2019/05/08/机器学习笔记-第二周/</id>
    <published>2019-05-08T03:35:56.000Z</published>
    <updated>2019-05-08T03:48:05.591Z</updated>
    
    <content type="html"><![CDATA[<h2 id="多元变量线性回归"><a href="#多元变量线性回归" class="headerlink" title="多元变量线性回归"></a>多元变量线性回归</h2><p><img src="https://s2.ax1x.com/2019/05/08/EyxPSJ.md.jpg" alt="EyxPSJ.md.jpg"></p><a id="more"></a><p><img src="https://s2.ax1x.com/2019/05/08/EyzdUK.md.jpg" alt="EyzdUK.md.jpg"></p><p><img src="https://s2.ax1x.com/2019/05/08/Eyz6KA.md.jpg" alt="Eyz6KA.md.jpg"></p><p><img src="https://s2.ax1x.com/2019/05/08/Eyzgbt.md.jpg" alt="Eyzgbt.md.jpg"></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;多元变量线性回归&quot;&gt;&lt;a href=&quot;#多元变量线性回归&quot; class=&quot;headerlink&quot; title=&quot;多元变量线性回归&quot;&gt;&lt;/a&gt;多元变量线性回归&lt;/h2&gt;&lt;p&gt;&lt;img src=&quot;https://s2.ax1x.com/2019/05/08/EyxPSJ.md.jpg&quot; alt=&quot;EyxPSJ.md.jpg&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="Machine Learning" scheme="http://yoursite.com/tags/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>机器学习笔记|第一周</title>
    <link href="http://yoursite.com/2019/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E7%AC%AC%E4%B8%80%E5%91%A8/"/>
    <id>http://yoursite.com/2019/05/08/机器学习笔记-第一周/</id>
    <published>2019-05-08T02:35:39.000Z</published>
    <updated>2019-05-08T03:25:01.521Z</updated>
    
    <content type="html"><![CDATA[<h2 id="绪论和单变量线性回归"><a href="#绪论和单变量线性回归" class="headerlink" title="绪论和单变量线性回归"></a>绪论和单变量线性回归</h2><p><img src="https://s2.ax1x.com/2019/05/08/EyjKAO.md.jpg" alt="EyjKAO.md.jpg"></p><a id="more"></a><p><img src="https://s2.ax1x.com/2019/05/08/Eyjbb6.md.jpg" alt="Eyjbb6.md.jpg"></p><p><img src="https://s2.ax1x.com/2019/05/08/Eyv6RH.md.jpg" alt="Eyv6RH.md.jpg"></p><p><img src="https://s2.ax1x.com/2019/05/08/EyvqQs.md.jpg" alt="EyvqQs.md.jpg"></p><p><img src="https://s2.ax1x.com/2019/05/08/EyvvwV.md.jpg" alt="EyvvwV.md.jpg"></p><p><img src="https://s2.ax1x.com/2019/05/08/EyxPSJ.md.jpg" alt="EyxPSJ.md.jpg"></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;绪论和单变量线性回归&quot;&gt;&lt;a href=&quot;#绪论和单变量线性回归&quot; class=&quot;headerlink&quot; title=&quot;绪论和单变量线性回归&quot;&gt;&lt;/a&gt;绪论和单变量线性回归&lt;/h2&gt;&lt;p&gt;&lt;img src=&quot;https://s2.ax1x.com/2019/05/08/EyjKAO.md.jpg&quot; alt=&quot;EyjKAO.md.jpg&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="Machine Learning" scheme="http://yoursite.com/tags/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>100天机器学习-1</title>
    <link href="http://yoursite.com/2019/05/08/100%E5%A4%A9%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-1/"/>
    <id>http://yoursite.com/2019/05/08/100天机器学习-1/</id>
    <published>2019-05-08T01:56:38.000Z</published>
    <updated>2019-05-08T01:56:38.272Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Day-1"><a href="#Day-1" class="headerlink" title="Day-1"></a>Day-1</h2><h4 id="pd-iloc"><a href="#pd-iloc" class="headerlink" title="pd.iloc()"></a>pd.iloc()</h4><p>用于切分pandas的数据，基于整数位置索引。再通过<code>.values</code>即可返回numpy表示。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x=data.iloc[:,:<span class="number">-1</span>].values</span><br><span class="line">y=data.iloc[:,<span class="number">3</span>].values</span><br></pre></td></tr></table></figure><h4 id="处理缺失值（missing-value）"><a href="#处理缺失值（missing-value）" class="headerlink" title="处理缺失值（missing value）"></a>处理缺失值（missing value）</h4><p>数据集中有些数据的值为<code>NaN</code>。值得一提的是，在numpy中，<code>NaN</code>(Not a Number)属于float类型的。可以使用<code>sklearn.preprocessing</code>中的类<code>Imputer</code>来处理这些缺失值。具体步骤如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> Imputer</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创立一个imputer对象，参数是对缺失值的处理方式</span></span><br><span class="line">imputer=Imputer(missing_values=<span class="string">'NaN'</span>,strategy=<span class="string">'mean'</span>,axis=<span class="number">0</span>)</span><br><span class="line"><span class="comment"># fit()方法是用数组x去"训练"imputer对象</span></span><br><span class="line">imputer=imputer.fit(x[:,<span class="number">1</span>:<span class="number">3</span>])</span><br><span class="line"><span class="comment"># transform()是用训练好的对象再对x进行处理</span></span><br><span class="line">x[:,<span class="number">1</span>:<span class="number">3</span>]=imputer.transform(x[:,<span class="number">1</span>:<span class="number">3</span>])</span><br></pre></td></tr></table></figure><p><a href="https://www.cnblogs.com/chaosimple/p/4153158.html" target="_blank" rel="noopener">关于缺失值（missing value）的处理</a></p><h4 id="解析分类数据（encoding-categorical-data）"><a href="#解析分类数据（encoding-categorical-data）" class="headerlink" title="解析分类数据（encoding categorical data）"></a>解析分类数据（encoding categorical data）</h4><p>分类数据是指不用数字而用标签的变量，比如本例中的“Yes”和“No”。为了方便处理，可以用<code>sklearn.preprocessing</code>模块中的<code>LabelEncoder</code>类把他们转换成数字。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> LabelEncoder,OneHotEncoder</span><br><span class="line"></span><br><span class="line">labelencoder_X=LabelEncoder()</span><br><span class="line"><span class="comment"># fit()给标签编码，并且返回编码后的值</span></span><br><span class="line">x[:,<span class="number">0</span>]=labelencoder_X.fit_transform(x[:,<span class="number">0</span>])</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Day-1&quot;&gt;&lt;a href=&quot;#Day-1&quot; class=&quot;headerlink&quot; title=&quot;Day-1&quot;&gt;&lt;/a&gt;Day-1&lt;/h2&gt;&lt;h4 id=&quot;pd-iloc&quot;&gt;&lt;a href=&quot;#pd-iloc&quot; class=&quot;headerlink&quot; title=
      
    
    </summary>
    
    
      <category term="Machine Learning" scheme="http://yoursite.com/tags/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>机器学习笔记|第十周</title>
    <link href="http://yoursite.com/2019/05/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E7%AC%AC%E5%8D%81%E5%91%A8/"/>
    <id>http://yoursite.com/2019/05/06/机器学习笔记-第十周/</id>
    <published>2019-05-06T11:02:50.000Z</published>
    <updated>2019-05-06T11:02:50.622Z</updated>
    
    <content type="html"><![CDATA[<h2 id="大规模机器学习"><a href="#大规模机器学习" class="headerlink" title="大规模机器学习"></a>大规模机器学习</h2><h3 id="学习大数据"><a href="#学习大数据" class="headerlink" title="学习大数据"></a>学习大数据</h3><p>假设我们有一个高偏差模型，那么大规模的数据的确能够帮助你拟合更好的模型。但是大规模的数据可能导致算法运行的效率低下。</p><p>所以在收集大量数据之前，应该考虑用小样本数据是否也能获得较好的拟合效果。</p><a id="more"></a><h3 id="随机梯度下降法"><a href="#随机梯度下降法" class="headerlink" title="随机梯度下降法"></a>随机梯度下降法</h3><p>如果我们确定需要使用到大量数据，也可以使用随机梯度法(<strong>Stochastic gradient descent</strong>)来代替之前提到过的批量梯度下降法(<strong>batch gradient descent</strong>)。</p><p><img src="https://s2.ax1x.com/2019/05/06/EDS5LV.png" alt="EDS5LV.png"></p><p>由图中可知，随机梯度法的代价函数为一个单一训练实例的代价，注意这里取的是一个样本，计算代价：</p><p>​                                                    $cost\left(  \theta, \left( {x}^{(i)} , {y}^{(i)} \right)  \right) = \frac{1}{2}\left( {h}_{\theta}\left({x}^{(i)}\right)-{y}^{ {(i)} } \right)^{2}$</p><p>随机梯度下降的具体步骤：</p><ul><li><p>第一步：随机打乱数据</p></li><li><p>第二步：Repeat (usually anywhere between1-10){</p><p>  <strong>for</strong> $i = 1:m${</p><p> ​       $\theta:={\theta}_{j}-\alpha\left( {h}_{\theta}\left({x}^{(i)}\right)-{y}^{(i)} \right){ {x}_{j} }^{(i)}$      </p><p>​        (<strong>for</strong> $j=0:n$)</p><p> ​    }<br> }</p><p>注意这一步中，对比批量梯度下降法的每次迭代都需要计算所有样本，随机梯度下降法不需要每次迭代（求$\theta$）时都对所有数据累加求平方和，这大大地减少了计算量。</p></li></ul><p>在大样本情况下，随机梯度下降法加大了迭代过程的效率，但同样，它的下降过程不再沿着“正确的方向”，如下图。最终可能无法得到精确的最小值点，而是在最小值值点附近，但对于模型的预测准确度不会有太大的影响。</p><p><img src="https://s2.ax1x.com/2019/05/06/EDpjhQ.png" alt="EDpjhQ.png"></p><h3 id="小批量梯度下降"><a href="#小批量梯度下降" class="headerlink" title="小批量梯度下降"></a>小批量梯度下降</h3><p><img src="https://s2.ax1x.com/2019/05/06/EDCuGQ.png" alt="EDCuGQ.png"></p><p>在每次迭代时，使用b个样本累加求和，具体来说：</p><p> <strong>Repeat</strong> {</p><p> <strong>for</strong> $i = 1:m${</p><p> ​       $\theta:={\theta}_{j}-\alpha\frac{1}{b}\sum_\limits{k=i}^{i+b-1}\left( {h}_{\theta}\left({x}^{(k)}\right)-{y}^{(k)} \right){ {x}_{j} }^{(k)}$      </p><p>​       (<strong>for</strong> $j=0:n$)</p><p>​      $ i +=10 $   </p><p> ​     }<br> }</p><p>通常b的取值在2-100之间。</p><h3 id="随机梯度下降收敛"><a href="#随机梯度下降收敛" class="headerlink" title="随机梯度下降收敛"></a>随机梯度下降收敛</h3><p>随机梯度下降法还有一个好处就是方便观察优化过程中的收敛情况。</p><p><img src="https://s2.ax1x.com/2019/05/06/EDPz1H.png" alt="EDPz1H.png"></p><p>可以看到，批量梯度下降法需要计算完所有的样本才能画出图像，这在大样本的情况下是非常低效的。而随机梯度下降法，只需要每次更新$\theta$之前，计算出代价值。再画出前1000次迭代情况即可。</p><p><img src="https://s2.ax1x.com/2019/05/06/EDiLKs.png" alt="EDiLKs.png"></p><p>前两张图是正确的迭代过程，通常而言，选择样本数量越多，观测到的曲线就越平缓。</p><p>就最后两张图而言，我们可以得出一个结论，如果你得到的曲线没有体现出收敛的趋势（图三），那么可以尝试增加样本，如果还是没有（粉红色线）。那么可能就是特征、算法方面的问题了。</p><p>如果曲线有递增趋势，也就是发散。那么可以尝试使用小一点的学习速率。</p><p>关于学习速率，我们通常是选取固定值。但其实也可以使用变量：$\alpha = \frac{const1}{iterationNumber + const2}$，这可以迫使我们的算法收敛而不是在最小值周围徘徊。但是一般不需要为了学习速率浪费计算资源，我们也可以得到很好的效果。</p><h3 id="在线学习"><a href="#在线学习" class="headerlink" title="在线学习"></a>在线学习</h3><p>如果你有一个连续的用户流引起的数据流，那么就可以设计一个在线学习机制，来根据数据流学习用户的偏好。</p><p>其实这和之前的随机梯度下降主要区别在于，我们的数据是流动的，算法也在不断的学习而不是只用静态的数据集。</p><p> Repeat forever (as long as the website is running) {<br>  Get $\left(x,y\right)$ corresponding to the current user<br>        $\theta:={\theta}_{j}-\alpha\left( {h}_{\theta}\left({x}\right)-{y} \right){ {x}_{j} }$<br>       (<strong>for</strong> $j=0:n$)<br>    }</p><p>一旦(x,y)训练完成，我们就可以抛弃它，因为数据是动态的，还会有新的数据源源不断的输入。</p><h3 id="减少映射与数据并行"><a href="#减少映射与数据并行" class="headerlink" title="减少映射与数据并行"></a>减少映射与数据并行</h3><p>之前我们提到，如果用批量梯度下降法来训练大规模数据是非常耗费时间的，但如果我们能将数据分为多个子集，分给不同的计算机运行，最后再将结果汇总求和，这样的方法就叫做减少映射。</p><p>例如我们有400个例子，需要使用批量梯度下降法，我们可以把数据分配给4台计算机处理：</p><p><img src="https://s2.ax1x.com/2019/05/06/EDllhd.md.png" alt="EDllhd.md.png"></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;大规模机器学习&quot;&gt;&lt;a href=&quot;#大规模机器学习&quot; class=&quot;headerlink&quot; title=&quot;大规模机器学习&quot;&gt;&lt;/a&gt;大规模机器学习&lt;/h2&gt;&lt;h3 id=&quot;学习大数据&quot;&gt;&lt;a href=&quot;#学习大数据&quot; class=&quot;headerlink&quot; title=&quot;学习大数据&quot;&gt;&lt;/a&gt;学习大数据&lt;/h3&gt;&lt;p&gt;假设我们有一个高偏差模型，那么大规模的数据的确能够帮助你拟合更好的模型。但是大规模的数据可能导致算法运行的效率低下。&lt;/p&gt;
&lt;p&gt;所以在收集大量数据之前，应该考虑用小样本数据是否也能获得较好的拟合效果。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Machine Learning" scheme="http://yoursite.com/tags/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>机器学习笔记|第九周</title>
    <link href="http://yoursite.com/2019/05/04/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E7%AC%AC%E4%B9%9D%E5%91%A8/"/>
    <id>http://yoursite.com/2019/05/04/机器学习笔记-第九周/</id>
    <published>2019-05-04T08:50:04.000Z</published>
    <updated>2019-05-04T08:50:04.973Z</updated>
    
    <content type="html"><![CDATA[<h2 id="异常检测"><a href="#异常检测" class="headerlink" title="异常检测"></a>异常检测</h2><p>异常检测(<strong>Anomaly detection</strong>)是一种特殊的无监督学习，但其和监督问题有些类似之处。</p><h3 id="问题动机"><a href="#问题动机" class="headerlink" title="问题动机"></a>问题动机</h3><p>从下图可以看出，对训练集建立模型后，输入$x_{test}$，通过概率来判断它是否异常。</p><p><img src="https://s2.ax1x.com/2019/05/02/EtQyRg.png" alt="EtQyRg.png"></p> <a id="more"></a><p>异常检测的运用：</p><ul><li>欺诈检测</li><li>工业生产领域</li><li>检测数据中心的计算机</li></ul><h3 id="高斯分布"><a href="#高斯分布" class="headerlink" title="高斯分布"></a>高斯分布</h3><p>就是正态分布，因为其性质和异常检测的性质差不多，由下图可以很明显看出，样本集中的位置同样也是高斯分布中概率（面积）较大的位置。</p><p><img src="https://s2.ax1x.com/2019/05/02/EtlNfU.png" alt="EtlNfU.png"></p><p>参数估计要做的则是确定高斯分布的参数，具体而言就是确定$\mu$和$\sigma^{2}$</p><ul><li>均值：$\mu=\frac{1}{m} \sum_{i=1}^{m} x^{(i)}$</li><li>方差：$\sigma^{2}=\frac{1}{m} \sum_{i=1}^{m}\left(x^{(i)}-\mu\right)^{2}$</li></ul><h3 id="算法："><a href="#算法：" class="headerlink" title="算法："></a>算法：</h3><ul><li><p>第一步：选取你认为能够指示出异常例子的特征$x_{i}$</p></li><li><p>第二步：拟合参数 $\mu_{1}, \dots, \mu_{n}, \sigma_{1}^{2}, \dots, \sigma_{n}^{2}$</p></li><li><p>第三步：给定一个新的输入，计算$p(x)=\prod_{j=1}^{n} p\left(x_{j} ; \mu_{j}, \sigma_{j}^{2}\right)=\prod_{j=1}^{n} \frac{1}{\sqrt{2 \pi} \sigma_{j}} \exp \left(-\frac{\left(x_{j}-\mu_{j}\right)^{2}}{2 \sigma_{j}^{2}}\right)$</p><p>也就是根据不同的特征，计算出<strong>每个特征的概率</strong>，再将所有概率累乘起来。最后如果$p(x)&lt;\varepsilon$，则标注为异常。</p></li></ul><p>以二维特征为例：</p><p><img src="https://s2.ax1x.com/2019/05/02/EtJiZ9.png" alt="EtJiZ9.png"></p><h3 id="开发和评估异常检测系统"><a href="#开发和评估异常检测系统" class="headerlink" title="开发和评估异常检测系统"></a>开发和评估异常检测系统</h3><p>前面所提到的内容都是在只有正常样本（y=0，normal）的情况下训练的吗，也就是无监督学习。但如果你还有异常样本（y=1，anomalous），则可以用于评估算法的准确性。而且，当为样本选择特征时，如果不确定是否该纳入某一个特征，可以分别在两种情况下进行训练，在验证集上分别评估效果，从而决定是否纳入该特征(模型选择问题)。</p><p>假设现在你有一些带标签数据，即含有正常样本和异常样本。可以把数据集划分为训练集、交叉验证集和测试集。</p><ul><li>数据集尽量都为正常样本</li><li>异常样本放入交叉验证集和测试集中。</li></ul><p>举个例子：飞机引擎，10000个正常引擎，20个有瑕疵的引擎。把正常引擎按60%、20%、20%的比例划分为训练集、CV和测试集。同时把瑕疵引擎平均分配到CV和测试集中。</p><p>用6000个正常引擎数据拟合出概率分布$p(x)$。</p><p><img src="https://s2.ax1x.com/2019/05/02/EtTFi9.png" alt="EtTFi9.png"></p><p>推广到一般情况下</p><p><img src="https://s2.ax1x.com/2019/05/02/Et7u60.png" alt="Et7u60.png"></p><ul><li>利用训练集来拟合出概率分布p(x)</li><li>将CV中的x输入p(x)，设置好阈值$\varepsilon$，从而预测出结果y</li><li>根据y，计算TP、FP、FN、TN。准确率和找回率，计算F1分数</li><li>调整$\varepsilon$直到找到最高F1的函数作为最优拟合模型</li><li>把测试集代入最优拟合模型。</li></ul><h3 id="异常检测-vs-监督学习"><a href="#异常检测-vs-监督学习" class="headerlink" title="异常检测 vs 监督学习"></a>异常检测 vs 监督学习</h3><p>在有标签的情况下，异常检测和监督学习具有相似之处，但仍然有许多明显的差别。从图中可以看出，如果样本中含有大量的负样本（y=0，正常样本），少量正样本（y=1，非正常样本）。那么常使用异常检测；而当正样本数量和负样本数量都很多时，采用监督学习。</p><p><img src="https://s2.ax1x.com/2019/05/03/ENccxe.png" alt="ENccxe.png"></p><p>在用途上也略有不同：</p><p><img src="https://s2.ax1x.com/2019/05/03/ENcvaq.png" alt="ENcvaq.png"></p><h3 id="选择要使用的功能"><a href="#选择要使用的功能" class="headerlink" title="选择要使用的功能"></a>选择要使用的功能</h3><p>在应用异常检测之前，有一个很重要的是就是选择合适的特征。下面介绍几个常用的方法：</p><ul><li>特征不服从高斯分布</li></ul><p><img src="https://s2.ax1x.com/2019/05/03/EN2PtP.png" alt="EN2PtP.png"></p><p>如果特征不服从高斯分布，那就需要通过一些转换方法（比如取对数或是开根号）来构造出符合高斯分布的样子。</p><p><img src="https://s2.ax1x.com/2019/05/03/EN2B9K.png" alt="EN2B9K.png"></p><ul><li>异常检测误差分析</li></ul><p>也可以尝试用有标签的样本，先选择一个比较原始的特征，计算出$p(x)$，在验证集上验证，找到不符合验证结果的样本，观察它的特征，看看能否构建出新的特征，从而能够达到区分的目的。</p><p>例如下图中在只有特征$x_{1}$的情况下，很难看出绿色样本是异常样本。</p><p><img src="https://s2.ax1x.com/2019/05/03/EN2x3T.png" alt="EN2x3T.png"></p><p>此时对可以对这个绿色样本进行分析，看能不能构建出一个特征$x_{2}$，把这个异常样本给检测出来：</p><p><img src="https://s2.ax1x.com/2019/05/03/ENRdbj.png" alt="ENRdbj.png"></p><p>以数据中心的计算机为例：</p><p>通常不会选取值特别大或者特别小的特征。</p><p><img src="https://s2.ax1x.com/2019/05/03/ENWiQS.png" alt="ENWiQS.png"></p><p>如果突然我怀疑一台计算机陷入了死循环，那么$x_{3}$的数值会变得很大而$x_{4}$的数值则基本不变。那么我可以通过设置其它特征$x_{5}$和$x_{6}$来判断是哪台计算机出现了该问题。</p><p><img src="https://s2.ax1x.com/2019/05/03/ENWfl8.png" alt="ENWfl8.png"></p><h3 id="多变量高斯分布"><a href="#多变量高斯分布" class="headerlink" title="多变量高斯分布"></a>多变量高斯分布</h3><p>对比单元高斯分布，多元高斯分布在处理某些问题上具有优势。</p><p>以数据中心的计算机为例：</p><p><img src="https://s2.ax1x.com/2019/05/03/EN5X59.png" alt="EN5X59.png"></p><p>我们用到CPU负载和内存使用两个特征，如果使用单元高斯分布，则分别对这两个特征训练出p(x)，再相乘得到最终结果。反映在左图中，其概率的范围应该是粉红色圈。</p><p>但事实上，我们知道CPU负载和内存的使用之间的关系应该呈现出左图蓝色线圈那样的正线性关系。具体来说，如果此时有一个异常样本点——小绿叉，根据单元高斯分布计算出的结果，小绿叉落在的粉红色线圈范围是概率较高的范围，也就是会被判定为正常样本（从右图也可以看出）。但这显然不符合真实情况。</p><p>要解决这个问题，第一种方法是上一节说过的，可以通过构建新的特征，比如这里可以用x3=x1/x2。还有一种方法就是利用多元高斯分布。</p><p>多元高斯分布考虑了每两个特征之间可能存在的关系。</p><p><img src="https://s2.ax1x.com/2019/05/04/EaFTjP.png" alt="EaFTjP.png"></p><p>而多元高斯分布公式：</p><p><img src="https://s2.ax1x.com/2019/05/04/EaFqHS.png" alt="EaFqHS.png"></p><p>其中，粉色圈内的表示行列式。</p><p>在多元高斯分布公式中，$\Sigma$表示的是协方差矩阵，衡量的是方差，也就是$x_{1}$和$x_{2}$之间的变化量。其中对角线上元素上表示维度之间的方差。</p><p>从下面的图中可以看出，当对角线元素相等时，图像的投影为圆形。而当对角线元素不等时，投影为椭圆。</p><p><img src="https://s2.ax1x.com/2019/05/04/EaFjhj.png" alt="EaFjhj.png"></p><p>非对角的元素表示维度之间的协方差，以下是为正值的情况。</p><p><img src="https://s2.ax1x.com/2019/05/04/EakKu6.png" alt="EakKu6.png"></p><p>如果非对角线的元素为负值：</p><p><img src="https://s2.ax1x.com/2019/05/04/EakNvt.png" alt="EakNvt.png"></p><p>我们还可以改变$\mu$值，即移动峰值所在位置：</p><p><img src="https://s2.ax1x.com/2019/05/04/EakaKP.png" alt="EakaKP.png"></p><h3 id="使用多变量高斯分布的异常检测"><a href="#使用多变量高斯分布的异常检测" class="headerlink" title="使用多变量高斯分布的异常检测"></a>使用多变量高斯分布的异常检测</h3><p>以下是多变量高斯模型的计算过程：</p><p><img src="https://s2.ax1x.com/2019/05/04/EaVmgs.png" alt="EaVmgs.png"></p><p>可以看出计算过程和之前的原始模型（单元高斯分布模型）：</p><p>$p(x)=p\left(x_{1} ; \mu_{1}, \sigma_{1}^{2}\right) \times p\left(x_{2} ; \mu_{2}, \sigma_{2}^{2}\right) \times \cdots \times p\left(x_{n} ; \mu_{n}, \sigma_{n}^{2}\right)$</p><p>有相似的地方。其实只要把多元高斯分布中的$\Sigma$非对角线元素全部改为0，就消除了不同纬度的相关性，得到的就是原始模型了。</p><ul><li>原始模型和多变量高斯分布模型比较</li></ul><p><img src="https://s2.ax1x.com/2019/05/04/EaV1ET.png" alt="EaV1ET.png"></p><p>多元高斯分布模型的优点在于能够自动找到不同维度之间的关系；而原始模型的优点在于计算效率高，适用于维度小或是样本少的情况。值得注意的是多元高斯分布模型的$\Sigma$必须可逆，所以必须满足m&gt;n（NG建议m&gt;=10n），而且维度之间必须是线性无关的。</p><h2 id="推荐系统"><a href="#推荐系统" class="headerlink" title="推荐系统"></a>推荐系统</h2><h3 id="问题规划"><a href="#问题规划" class="headerlink" title="问题规划"></a>问题规划</h3><p>后面几个小节都会通过这个预测电影评分的例子来讲解，目的是预测左下角表格中’?’的评分</p><p><img src="https://s2.ax1x.com/2019/05/04/EaVbxs.png" alt="EaVbxs.png"></p><h3 id="基于内容的推荐算法"><a href="#基于内容的推荐算法" class="headerlink" title="基于内容的推荐算法"></a>基于内容的推荐算法</h3><p>假设我们已经知道了衡量电影类型的属性，$x_{1}$和$x_{2}$。再添加一个偏置，即可以用三个特征来表示一个样本，例如《Love at last》的特征就是[1 , 0.9 , 0]。</p><p><img src="https://s2.ax1x.com/2019/05/04/Ean0QH.png" alt="Ean0QH.png"></p><p>对于每个样本而言，接下来就需要求出参数$\theta^{(j)}$（单个用户j的参数向量），再通过$\left(\theta^{(j)}\right)^{T} x^{(i)}$即可求出结果。以第一个粉红色圈圈为例，假设我们已知它的参数$\theta^{(1)}$，那么就能预测出评分为4.95。</p><p><img src="https://s2.ax1x.com/2019/05/04/EauE1e.png" alt="EauE1e.png"></p><p>那么如何求出参数$\theta^{(j)}$呢？</p><p>这相当于一个线性回归问题：</p><p><img src="https://s2.ax1x.com/2019/05/04/EauU7q.png" alt="EauU7q.png"></p><p>得到优化目标之后，可以使用梯度下降法、BFGS等方法来进行优化。</p><p><img src="https://s2.ax1x.com/2019/05/04/EaKt2D.png" alt="EaKt2D.png"></p><p>使用基于内容的推荐算法的前提是，我们已经有了用于描述算法的不同特征，这个例子中，就是用了描述电影成分的$x_{1}$和$x_{2}$。但是大多数情况下，我们不具有这些特征时又应该怎么做呢？</p><h3 id="协同过滤"><a href="#协同过滤" class="headerlink" title="协同过滤"></a>协同过滤</h3><p>特征学习：学习算法能自动习得所需要的所有特征。这个和上一节中学习参数$\theta$放在一起，有点类似于先有蛋还是先有鸡的问题。</p><p><img src="https://s2.ax1x.com/2019/05/04/EaQQtx.png" alt="EaQQtx.png"></p><p>从图中可以得出，需要知道每个用户对于不同类型电影的喜爱程度，从而计算出电影含有爱情的成分$x_{1}$和含有动作$x_{2}$的成分。</p><p>对比之前求$\theta$的步骤如下：</p><p><img src="https://s2.ax1x.com/2019/05/04/EaQGcD.png" alt="EaQGcD.png"></p><p>那么到底是先计算$\theta$还是先计算$x_{j}$呢？</p><p>简单来说，就是先猜测一个$\theta$值，然后不断迭代，从而使得参数和特征都收敛于某个最终值。</p><p><img src="https://s2.ax1x.com/2019/05/04/EaQwNt.png" alt="EaQwNt.png"></p><h3 id="协同过滤算法"><a href="#协同过滤算法" class="headerlink" title="协同过滤算法"></a>协同过滤算法</h3><p>除了上个例子中的反复计算特征和参数的方法，我们还可以把特征和参数放在同一个优化目标中计算。实际操作就是把前面提到的两个优化目标放在一起，因为他们本质上也是针对那些被用户评分了的电影（即r(i,j)=1）。</p><p><img src="https://s2.ax1x.com/2019/05/04/EaYtR1.png" alt="EaYtR1.png"></p><p>值得注意的是，因为协同过滤算法是自动学习参数和特征，所以不需要像以前一样手动去添加偏置项。可以理解为如果算法需要，他会自己去计算出这一项。</p><h3 id="矢量化：低秩矩阵分解"><a href="#矢量化：低秩矩阵分解" class="headerlink" title="矢量化：低秩矩阵分解"></a>矢量化：低秩矩阵分解</h3><p>我们可以把用户对电影的评分的表格记录成矩阵的形式。</p><p><img src="https://s2.ax1x.com/2019/05/04/EagnzT.png" alt="EagnzT.png"></p><p>而我们的预测矩阵应该和矩阵Y中的值差不多</p><p><img src="https://s2.ax1x.com/2019/05/04/Ea25jO.png" alt="Ea25jO.png"></p><p>具体而言，可以把预测矩阵看成是由对每个特征向量转秩后放入矩阵x中，每位用户对某一电影类型的喜好放在矩阵$\Theta$中：</p><p><img src="https://s2.ax1x.com/2019/05/04/EdSRIA.md.png" alt="EdSRIA.md.png"></p><h3 id="实施细节：均值规范化"><a href="#实施细节：均值规范化" class="headerlink" title="实施细节：均值规范化"></a>实施细节：均值规范化</h3><p>如果多了一位用户Eve，她还没对任何电影做出过评分。那么应该如何预测她的评分呢？</p><p><img src="https://s2.ax1x.com/2019/05/04/EdPHlq.png" alt="EdPHlq.png"></p><p>如果我们仍然使用之前说过的方法来计算$\theta$和$x$，那么显然式子中的前两项为0，而只需要最后的正则项，又由于正则化项的性质可知（使得参数接近0），在没有数据时，根本不存在过拟合情况。最后一项中$\theta^{(5)}=0$。再根据$\left(\theta^{(5)}\right)^{T} x^{(i)}=0$可以得到上图中我们对Eve评分的预测值全为0，这显然没有意义。</p><p>为了解决这个问题，我们引入均值化。</p><p><img src="https://s2.ax1x.com/2019/05/04/EdiJAg.png" alt="EdiJAg.png"></p><p>上图中$\mu$为每一行的平均值，再用$Y-\mu$得到右边的矩阵。</p><p>这样在预测评分时，只需要把$\mu$加回去即可，既不会影响已经评分过了的用户，也使得还会评分的用户的预测值不会为0。</p><p><img src="https://s2.ax1x.com/2019/05/04/EdFaIe.png" alt="EdFaIe.png"></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;异常检测&quot;&gt;&lt;a href=&quot;#异常检测&quot; class=&quot;headerlink&quot; title=&quot;异常检测&quot;&gt;&lt;/a&gt;异常检测&lt;/h2&gt;&lt;p&gt;异常检测(&lt;strong&gt;Anomaly detection&lt;/strong&gt;)是一种特殊的无监督学习，但其和监督问题有些类似之处。&lt;/p&gt;
&lt;h3 id=&quot;问题动机&quot;&gt;&lt;a href=&quot;#问题动机&quot; class=&quot;headerlink&quot; title=&quot;问题动机&quot;&gt;&lt;/a&gt;问题动机&lt;/h3&gt;&lt;p&gt;从下图可以看出，对训练集建立模型后，输入$x_{test}$，通过概率来判断它是否异常。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://s2.ax1x.com/2019/05/02/EtQyRg.png&quot; alt=&quot;EtQyRg.png&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="Machine Learning" scheme="http://yoursite.com/tags/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>机器学习|吴恩达机器学习之PCA</title>
    <link href="http://yoursite.com/2019/05/02/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8BPCA/"/>
    <id>http://yoursite.com/2019/05/02/机器学习-吴恩达机器学习之PCA/</id>
    <published>2019-05-02T06:44:46.000Z</published>
    <updated>2019-05-02T06:44:46.165Z</updated>
    
    <content type="html"><![CDATA[<h2 id="K-means-Clustering"><a href="#K-means-Clustering" class="headerlink" title="K-means Clustering"></a>K-means Clustering</h2><p>导入模块</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> scipy.io <span class="keyword">import</span> loadmat</span><br><span class="line"><span class="keyword">from</span> skimage <span class="keyword">import</span> io</span><br></pre></td></tr></table></figure><a id="more"></a><h3 id="Implementing-K-means"><a href="#Implementing-K-means" class="headerlink" title="Implementing K-means"></a>Implementing K-means</h3><h4 id="Finding-closet-centroids"><a href="#Finding-closet-centroids" class="headerlink" title="Finding closet centroids"></a>Finding closet centroids</h4><ul><li>读入数据</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">data=loadmat(<span class="string">'ex7data2.mat'</span>)</span><br><span class="line">X=data[<span class="string">'X'</span>]</span><br></pre></td></tr></table></figure><ul><li>为每个样本找到最近的聚类中心</li></ul><p>$c^{(i)} :=j \quad$ that minimizes $\quad\left|x^{(i)}-\mu_{j}\right|^{2}$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#========================找聚类中心</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">find_closet_centroids</span><span class="params">(X,centroids)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    返回每个样本所在的cluster索引</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    idx=[]</span><br><span class="line">    max_dist=<span class="number">10000</span> <span class="comment"># 给一个距离限定</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(X)):</span><br><span class="line">        minus=X[i]-centroids <span class="comment"># minus是3x2的矩阵，每一行代表了第i个样本到一个centroids的x1,x2距离</span></span><br><span class="line">        dist=minus[:,<span class="number">0</span>]**<span class="number">2</span>+minus[:,<span class="number">1</span>]**<span class="number">2</span> <span class="comment">#求范式，即直线距离,dist是3x1的向量</span></span><br><span class="line">        <span class="keyword">if</span> dist.min()&lt;max_dist:</span><br><span class="line">            ci=np.argmin(dist) <span class="comment">#返回沿axis的最小值索引</span></span><br><span class="line">            idx.append(ci)</span><br><span class="line">    <span class="keyword">return</span> np.array(idx)</span><br></pre></td></tr></table></figure><p>可以自己初始化一组聚类中心来测试一下</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">init_centroids = np.array([[<span class="number">3</span>, <span class="number">3</span>], [<span class="number">6</span>, <span class="number">2</span>], [<span class="number">8</span>, <span class="number">5</span>]])</span><br><span class="line">idx = findClosestCentroids(X, init_centroids)</span><br><span class="line">print(idx[<span class="number">0</span>:<span class="number">3</span>])</span><br></pre></td></tr></table></figure><p>结果应该是</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[<span class="number">0</span> <span class="number">2</span> <span class="number">1</span>]</span><br></pre></td></tr></table></figure><h4 id="Computing-centroids-means"><a href="#Computing-centroids-means" class="headerlink" title="Computing  centroids means"></a>Computing  centroids means</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#=========================移动聚类中心</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_centroids</span><span class="params">(X,idx)</span>:</span></span><br><span class="line">    centroids=[]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(np.unique(idx))):</span><br><span class="line">        <span class="comment"># 布尔索引，idx==i运算返回bool value，根据值为true的下标来输出X中对应元素值</span></span><br><span class="line">        u_k=X[idx==i].mean(axis=<span class="number">0</span>)</span><br><span class="line">        </span><br><span class="line">        centroids.append(u_k)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> np.array(centroids)</span><br></pre></td></tr></table></figure><h4 id="K-means-on-example-dataset"><a href="#K-means-on-example-dataset" class="headerlink" title="K-means on example dataset"></a>K-means on example dataset</h4><ul><li>画图函数</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_data</span><span class="params">(X,centroids,idx=None)</span>:</span></span><br><span class="line"></span><br><span class="line">    colors = [<span class="string">'b'</span>,<span class="string">'g'</span>,<span class="string">'gold'</span>,<span class="string">'darkorange'</span>,<span class="string">'salmon'</span>,<span class="string">'olivedrab'</span>, </span><br><span class="line">              <span class="string">'maroon'</span>, <span class="string">'navy'</span>, <span class="string">'sienna'</span>, <span class="string">'tomato'</span>, <span class="string">'lightgray'</span>, <span class="string">'gainsboro'</span></span><br><span class="line">             <span class="string">'coral'</span>, <span class="string">'aliceblue'</span>, <span class="string">'dimgray'</span>, <span class="string">'mintcream'</span>, <span class="string">'mintcream'</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 这里centroids需要从ndarray转成list,但centroids[0]仍为ndarray.</span></span><br><span class="line">    <span class="keyword">assert</span> len(centroids[<span class="number">0</span>])&lt;=len(colors),<span class="string">'colors not enough'</span> </span><br><span class="line">    </span><br><span class="line">    subX=[]</span><br><span class="line">    <span class="keyword">if</span> idx <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(centroids[<span class="number">0</span>].shape[<span class="number">0</span>]): <span class="comment">#分成几类就循环几次</span></span><br><span class="line">            x_i=X[idx==i]</span><br><span class="line">            subX.append(x_i) <span class="comment">#把数据按cluster分成不同下标的元素，存储在subx这个list中</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        subX=[X]</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    fig=plt.figure(figsize=(<span class="number">8</span>,<span class="number">5</span>))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(subX)):</span><br><span class="line">        xx=subX[i]</span><br><span class="line">        plt.scatter(xx[:,<span class="number">0</span>],xx[:,<span class="number">1</span>],c=colors[i])    </span><br><span class="line">    plt.xlabel(<span class="string">'x'</span>)</span><br><span class="line">    plt.ylabel(<span class="string">'y'</span>)</span><br><span class="line">    plt.title(<span class="string">'Plot of X Points'</span>)</span><br><span class="line"></span><br><span class="line">    xx,yy=[],[]</span><br><span class="line">    <span class="keyword">for</span> centroid <span class="keyword">in</span> centroids: </span><br><span class="line">        xx.append(centroid[:,<span class="number">0</span>])</span><br><span class="line">        yy.append(centroid[:,<span class="number">1</span>])</span><br><span class="line">    </span><br><span class="line">    plt.plot(xx,yy,<span class="string">'rx--'</span>)</span><br><span class="line"></span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><ul><li>画出聚类中心移动过程</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#=============================聚类中心移动过程</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run_kmeans</span><span class="params">(X,centroids,max_iters)</span>:</span></span><br><span class="line"></span><br><span class="line">    centroids_all=[]</span><br><span class="line">    centroids_all.append(centroids)</span><br><span class="line">    centroid_i=centroids</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(max_iters):</span><br><span class="line">        idx=find_closet_centroids(X,centroid_i)</span><br><span class="line">        centroid_i=compute_centroids(X,idx) </span><br><span class="line">        centroids_all.append(centroid_i) <span class="comment">#每次移动后的聚类中心坐标都记录下来</span></span><br><span class="line">    <span class="keyword">return</span> idx,centroids_all</span><br></pre></td></tr></table></figure><p>结果：</p><p><img src="https://s2.ax1x.com/2019/05/02/EYOrCQ.png" alt="EYOrCQ.png"></p><h4 id="Random-initialization"><a href="#Random-initialization" class="headerlink" title="Random initialization"></a>Random initialization</h4><p>关于聚类中心的初始化，一个更好的方法是从样本集中随机选取。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#==============================随机初始化聚类中心</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">random_centroids</span><span class="params">(X,K)</span>:</span></span><br><span class="line">    m=X.shape[<span class="number">0</span>]</span><br><span class="line">    index=np.random.choice(m,K) <span class="comment">#在0~m中随机生成K个样本</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> X[index]</span><br></pre></td></tr></table></figure><p>最后再尝试一下聚类算法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">3</span>):</span><br><span class="line">        init_centroids=random_centroids(X,<span class="number">3</span>)</span><br><span class="line">        </span><br><span class="line">        idx,centroids_all=run_kmeans(X,init_centroids,<span class="number">20</span>)</span><br><span class="line">        plot_data(X,centroids_all,idx=idx) <span class="comment">#idx为每个样本所在cluster的索引</span></span><br></pre></td></tr></table></figure><p>得到最终结果：</p><p><img src="https://s2.ax1x.com/2019/05/02/EYXtG4.md.png" alt="EYXtG4.md.png"></p><h3 id="Image-compression-with-K-means"><a href="#Image-compression-with-K-means" class="headerlink" title="Image compression with K-means"></a>Image compression with K-means</h3><ul><li>导入数据</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">img=io.imread(<span class="string">'bird_small.png'</span>)</span><br><span class="line">plt.imshow(img)</span><br><span class="line">plt.show()</span><br><span class="line">print(img.shape)</span><br></pre></td></tr></table></figure><p>图像为</p><p><img src="https://s2.ax1x.com/2019/05/02/EYX5eP.png" alt="EYX5eP.png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(<span class="number">128</span>,<span class="number">128</span>,<span class="number">3</span>)</span><br></pre></td></tr></table></figure><p>可以看到图像以3维矩阵的方式存储，前面两个维度代表图像的像素点个数128x128，最后一个维度代表像素点由RGB三个通道表示。又因为一个通道占用8-bit，因此在原始的图像中一个像素点需要24-bit来储存。</p><p>在这幅图中包含了上千种种颜色，而在这个实验中，我们把颜色降为16种，也就是说，一像素点只需要4bit就足够了。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">img=img/<span class="number">255</span></span><br><span class="line">X=img.reshape(<span class="number">-1</span>,<span class="number">3</span>) <span class="comment">#转换成128x128行，3列为RGB三通道</span></span><br><span class="line">K=<span class="number">16</span> <span class="comment">#16个聚类中心，就是把所有颜色压缩为16种RGB颜色，那么每个像素值需要4bit存储即可</span></span><br><span class="line">init_centroids=random_centroids(X,K)</span><br><span class="line">idx,centroids_all=run_kmeans(X,init_centroids,<span class="number">10</span>)</span><br><span class="line">img_2=np.zeros(X.shape)</span><br><span class="line">centroids=centroids_all[<span class="number">-1</span>] <span class="comment"># 只需要记录聚类中心最后移动的位置即可</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(centroids)):</span><br><span class="line">    img_2[idx==i]=centroids[i]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">img_2=img_2.reshape((<span class="number">128</span>,<span class="number">128</span>,<span class="number">3</span>))</span><br><span class="line">fig,axes=plt.subplots(<span class="number">1</span>,<span class="number">2</span>,figsize=(<span class="number">12</span>,<span class="number">6</span>))</span><br><span class="line">axes[<span class="number">0</span>].imshow(img)</span><br><span class="line">axes[<span class="number">1</span>].imshow(img_2)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>比对一下压缩效果：</p><p><img src="https://s2.ax1x.com/2019/05/02/EYjBlj.md.png" alt="EYjBlj.md.png"></p><h2 id="Principle-Component-Analysis"><a href="#Principle-Component-Analysis" class="headerlink" title="Principle Component Analysis"></a>Principle Component Analysis</h2><ul><li>导入模块</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> scipy.io <span class="keyword">import</span> loadmat</span><br></pre></td></tr></table></figure><h3 id="Example-Dataset"><a href="#Example-Dataset" class="headerlink" title="Example Dataset"></a>Example Dataset</h3><ul><li>导入数据</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">data = loadmat(<span class="string">'ex7data1.mat'</span>)</span><br><span class="line">X = data[<span class="string">'X'</span>]</span><br><span class="line">print(X.shape)</span><br><span class="line">plt.scatter(X[:,<span class="number">0</span>], X[:,<span class="number">1</span>], facecolors=<span class="string">'none'</span>, edgecolors=<span class="string">'b'</span>)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(50,2)</span><br></pre></td></tr></table></figure><ul><li>显示图像：</li></ul><p><img src="https://s2.ax1x.com/2019/05/02/EYz5cQ.png" alt="EYz5cQ.png"></p><h3 id="Implementing-PCA"><a href="#Implementing-PCA" class="headerlink" title="Implementing PCA"></a>Implementing PCA</h3><blockquote><p>关于PCA的数学原理可以参考：<a href="https://nullblog.top/2019/05/01/%E6%B5%85%E8%B0%88SVD%E7%9A%84%E6%95%B0%E5%AD%A6%E5%8E%9F%E7%90%86%E5%8F%8A%E5%BA%94%E7%94%A8/" target="_blank" rel="noopener">浅谈SVD的数学原理及应用</a></p></blockquote><p>PCA主要分为两个计算步骤：</p><ul><li>计算数据的协方差矩阵：$\Sigma=\frac{1}{m} \sum_{i=1}^{n}\left(x^{(i)}\right)\left(x^{(i)}\right)^{T}$</li><li>用SVD函数进行奇异值分解</li></ul><p>在开始上面的步骤之前，需要先对数据进行特征缩放和归一化：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">feature_normalize</span><span class="params">(X)</span>:</span></span><br><span class="line">    means = X.mean(axis=<span class="number">0</span>)</span><br><span class="line">    stds = X.std(axis=<span class="number">0</span>, ddof=<span class="number">1</span>)</span><br><span class="line">    X_norm = (X - means) / stds</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> X_norm, means</span><br></pre></td></tr></table></figure><p>PCA：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#===================================PCA</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pca</span><span class="params">(X)</span>:</span></span><br><span class="line">    sigma = (<span class="number">1</span> / len(X)) * (X.T @ X)  <span class="comment">#求出协方差矩阵</span></span><br><span class="line">    U, S, V = np.linalg.svd(sigma)  <span class="comment">#奇异值分解</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> U, S, V</span><br></pre></td></tr></table></figure><p>可视化主成成分：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_reduce</span><span class="params">(means, U, S)</span>:</span></span><br><span class="line">    plt.plot([means[<span class="number">0</span>], means[<span class="number">0</span>] + <span class="number">1.5</span> * S[<span class="number">0</span>] * U[<span class="number">0</span>, <span class="number">0</span>]],</span><br><span class="line">             [means[<span class="number">1</span>], means[<span class="number">1</span>] + <span class="number">1.5</span> * S[<span class="number">0</span>] * U[<span class="number">0</span>, <span class="number">1</span>]],</span><br><span class="line">             c=<span class="string">'r'</span>,</span><br><span class="line">             linewidth=<span class="number">3</span>,</span><br><span class="line">             label=<span class="string">'First Principle Component'</span>)</span><br><span class="line">    plt.plot([means[<span class="number">0</span>], means[<span class="number">0</span>] + <span class="number">1.5</span> * S[<span class="number">1</span>] * U[<span class="number">1</span>, <span class="number">0</span>]],</span><br><span class="line">             [means[<span class="number">1</span>], means[<span class="number">1</span>] + <span class="number">1.5</span> * S[<span class="number">1</span>] * U[<span class="number">1</span>, <span class="number">1</span>]],</span><br><span class="line">             c=<span class="string">'g'</span>,</span><br><span class="line">             linewidth=<span class="number">3</span>,</span><br><span class="line">             label=<span class="string">'Second Principal Component'</span>)</span><br><span class="line">    plt.axis(<span class="string">'equal'</span>)</span><br><span class="line">    plt.legend()</span><br></pre></td></tr></table></figure><p>得到结果：</p><p><img src="https://s2.ax1x.com/2019/05/02/EtpMM4.md.png" alt="EtpMM4.md.png"></p><h3 id="Dimensionality-Reduction-with-PCA"><a href="#Dimensionality-Reduction-with-PCA" class="headerlink" title="Dimensionality Reduction with PCA"></a>Dimensionality Reduction with PCA</h3><h4 id="Projecting-the-data-onto-the-principal-components"><a href="#Projecting-the-data-onto-the-principal-components" class="headerlink" title="Projecting the data onto the principal components"></a>Projecting the data onto the principal components</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">返回值</span></span><br><span class="line"><span class="string">- Z：投影到主成成分后的样本点</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">project_data</span><span class="params">(X, U, K)</span>:</span></span><br><span class="line">    Z = X @ U[:, <span class="number">0</span>:K]</span><br><span class="line">    <span class="keyword">return</span> Z</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">K = <span class="number">1</span></span><br><span class="line">Z = project_data(X_norm, U, K)</span><br><span class="line">print(Z[<span class="number">0</span>])</span><br></pre></td></tr></table></figure><p>得到结果：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">array([<span class="number">1.48127391</span>])</span><br></pre></td></tr></table></figure><h4 id="Reconstructing-an-approximation-of-the-data"><a href="#Reconstructing-an-approximation-of-the-data" class="headerlink" title="Reconstructing an approximation of the data"></a>Reconstructing an approximation of the data</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#===================================重建数据</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">recover_data</span><span class="params">(Z, U, K)</span>:</span></span><br><span class="line">    X_rec = Z @ U[:, <span class="number">0</span>:K].T</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> X_rec</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># you will recover an approximation of the first example and you should see a value of</span></span><br><span class="line"><span class="comment"># about [-1.047 -1.047].</span></span><br><span class="line">X_rec = recover_data(Z, U, <span class="number">1</span>)</span><br><span class="line">X_rec[<span class="number">0</span>]</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">array([<span class="number">-1.04741883</span>, <span class="number">-1.04741883</span>])</span><br></pre></td></tr></table></figure><h4 id="Visualizing-the-projections"><a href="#Visualizing-the-projections" class="headerlink" title="Visualizing the projections"></a>Visualizing the projections</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#====================================PCA样本投影可视化</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">visualize_data</span><span class="params">(X_norm, X_rec)</span>:</span></span><br><span class="line">    fig, ax = plt.subplots(figsize=(<span class="number">8</span>, <span class="number">5</span>))</span><br><span class="line">    plt.axis(<span class="string">'equal'</span>)</span><br><span class="line">    plt.scatter(</span><br><span class="line">        X_norm[:, <span class="number">0</span>],</span><br><span class="line">        X_norm[:, <span class="number">1</span>],</span><br><span class="line">        s=<span class="number">30</span>,</span><br><span class="line">        facecolors=<span class="string">'none'</span>,</span><br><span class="line">        edgecolors=<span class="string">'blue'</span>,</span><br><span class="line">        label=<span class="string">'Original Data Points'</span>)</span><br><span class="line"></span><br><span class="line">    plt.scatter(</span><br><span class="line">        X_rec[:, <span class="number">0</span>],</span><br><span class="line">        X_rec[:, <span class="number">1</span>],</span><br><span class="line">        s=<span class="number">30</span>,</span><br><span class="line">        facecolors=<span class="string">'none'</span>,</span><br><span class="line">        edgecolors=<span class="string">'red'</span>,</span><br><span class="line">        label=<span class="string">'PCA Reduced Data Points'</span>)</span><br><span class="line"></span><br><span class="line">    plt.title(<span class="string">"Example Dataset: Reduced Dimension Points Shown"</span>,fontsize=<span class="number">14</span>)</span><br><span class="line">    plt.xlabel(<span class="string">'x1 [Feature Normalized]'</span>,fontsize=<span class="number">14</span>)</span><br><span class="line">    plt.ylabel(<span class="string">'x2 [Feature Normalized]'</span>,fontsize=<span class="number">14</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> x <span class="keyword">in</span> range(X_norm.shape[<span class="number">0</span>]):</span><br><span class="line">        plt.plot([X_norm[x,<span class="number">0</span>],X_rec[x,<span class="number">0</span>]],[X_norm[x,<span class="number">1</span>],X_rec[x,<span class="number">1</span>]],<span class="string">'k--'</span>)</span><br><span class="line">        plt.legend(loc=<span class="number">0</span>)</span><br><span class="line">        <span class="comment"># 输入第一项全是X坐标，第二项都是Y坐标</span></span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><p>结果如图：</p><p><img src="https://s2.ax1x.com/2019/05/02/Et9DcF.md.png" alt="Et9DcF.md.png"></p><h3 id="Face-Image-Dataset"><a href="#Face-Image-Dataset" class="headerlink" title="Face Image Dataset"></a>Face Image Dataset</h3><ul><li>导入数据</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">data_2=loadmat(<span class="string">'ex7faces.mat'</span>)</span><br><span class="line">X_2=data_2[<span class="string">'X'</span>]</span><br></pre></td></tr></table></figure><ul><li>数据可视化</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_face</span><span class="params">(X,row,col)</span>:</span></span><br><span class="line">    fig,ax=plt.subplots(row,col,figsize=(<span class="number">8</span>,<span class="number">8</span>))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(row):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(col):</span><br><span class="line">            ax[i][j].imshow(X[i*col+j].reshape(<span class="number">32</span>,<span class="number">32</span>).T,cmap=<span class="string">'Greys_r'</span>)</span><br><span class="line">            ax[i][j].set_xticks([])</span><br><span class="line">            ax[i][j].set_yticks([])</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><p>结果如图：</p><p><img src="https://s2.ax1x.com/2019/05/02/Et9Ije.png" alt="Et9Ije.png"></p><h4 id="PCA-on-Faces"><a href="#PCA-on-Faces" class="headerlink" title="PCA on Faces"></a>PCA on Faces</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X_2_norm,means_2=feature_normalize(X_2)</span><br><span class="line">U_2,S_2,V_2=pca(X_2_norm)</span><br></pre></td></tr></table></figure><p>显示主成成分</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plot_face(U[:,:<span class="number">36</span>].T, <span class="number">6</span>, <span class="number">6</span>)</span><br></pre></td></tr></table></figure><p>结果如下：</p><p><img src="https://s2.ax1x.com/2019/05/02/Et9O4P.png" alt="Et9O4P.png"></p><h4 id="Dimensionality-Reduction"><a href="#Dimensionality-Reduction" class="headerlink" title="Dimensionality Reduction"></a>Dimensionality Reduction</h4><p>把图片降维</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Z_2=project_data(X_2_norm,U_2,K=<span class="number">36</span>)</span><br><span class="line">X_2_rec=recover_data(Z_2,U_2,K=<span class="number">36</span>)</span><br><span class="line">plot_face(X_2_rec,<span class="number">10</span>,<span class="number">10</span>)</span><br></pre></td></tr></table></figure><p>结果：</p><p><img src="https://s2.ax1x.com/2019/05/02/Etn09H.png" alt="Etn09H.png"></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;K-means-Clustering&quot;&gt;&lt;a href=&quot;#K-means-Clustering&quot; class=&quot;headerlink&quot; title=&quot;K-means Clustering&quot;&gt;&lt;/a&gt;K-means Clustering&lt;/h2&gt;&lt;p&gt;导入模块&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; numpy &lt;span class=&quot;keyword&quot;&gt;as&lt;/span&gt; np&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; matplotlib.pyplot &lt;span class=&quot;keyword&quot;&gt;as&lt;/span&gt; plt&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;from&lt;/span&gt; scipy.io &lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; loadmat&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;from&lt;/span&gt; skimage &lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; io&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
    
      <category term="Machine Learning" scheme="http://yoursite.com/tags/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>浅谈SVD的数学原理及应用</title>
    <link href="http://yoursite.com/2019/05/01/%E6%B5%85%E8%B0%88SVD%E7%9A%84%E6%95%B0%E5%AD%A6%E5%8E%9F%E7%90%86%E5%8F%8A%E5%BA%94%E7%94%A8/"/>
    <id>http://yoursite.com/2019/05/01/浅谈SVD的数学原理及应用/</id>
    <published>2019-05-01T07:02:06.000Z</published>
    <updated>2019-05-01T11:13:56.918Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>在做PCA的实验时，遇到的SVD奇异值分解问题，这里记录自己直观的理解</p></blockquote><p>通常在<code>python</code>中我们都是直接调用<code>numpy</code>模块中的函数，那么返回的U,S,V几个值代表什么意思呢？</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">U, S, V = np.linalg.svd(X)</span><br></pre></td></tr></table></figure><p>遇事不明，手册先行：<a href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.svd.html" target="_blank" rel="noopener">官方手册</a></p><a id="more"></a><p>手册中简单说明了其含义：</p><ul><li>U：酉矩阵</li><li>S：奇异值向量（数学上是奇异值矩阵，但因为这个矩阵类似于对角阵，只有主对角线上的元素非0，程序对其进行了处理，直接提取了对角元素），且元素按降序排列。</li><li>V：酉矩阵</li></ul><p>首先给出SVD分解公式：$W=U \Sigma V^{T}$</p><p>如果你看的一脸懵逼，没关系，这只能说明你和我一样，线性代数没怎么学好。所以接下来简单解释一下其背后的数学原理，希望有帮助。</p><p>举一个具体分解的例子：</p><p>有这么一个矩阵：$W=\left[ \begin{array}{ll}{1} &amp; {1} \\ {0} &amp; {1} \\ {1} &amp; {0}\end{array}\right]$我们要如何把它SVD分解？</p><p><strong>step-1.1</strong> </p><p>计算第一个对角矩阵，$C=W^{T} W=\left[ \begin{array}{lll}{1} &amp; {0} &amp; {1} \\ {1} &amp; {1} &amp; {0}\end{array}\right] \left[ \begin{array}{ll}{1} &amp; {1} \\ {0} &amp; {1} \\ {1} &amp; {0}\end{array}\right]=\left[ \begin{array}{ll}{2} &amp; {1} \\ {1} &amp; {2}\end{array}\right]$，注意得到的是方阵。</p><p><strong>step-1.2</strong></p><p>接着对这个对角矩阵，求它的特征值$\lambda_{1}=3, \lambda_{2}=1$和特征向量$\vec{v}_{1}=\left[ \begin{array}{l}{\frac{1}{\sqrt{2}}} \\ {\frac{1}{\sqrt{2}}}\end{array}\right], \vec{v}_{2}=\left[ \begin{array}{l}{\frac{1}{\sqrt{2}}} \\ {\frac{-1}{\sqrt{2}}}\end{array}\right]$。</p><p><strong>step-2.1</strong></p><p>计算第二个对角矩阵，$B=W W^{T}=\left[ \begin{array}{ll}{1} &amp; {1} \\ {0} &amp; {1} \\ {1} &amp; {0}\end{array}\right] \left[ \begin{array}{lll}{1} &amp; {0} &amp; {1} \\ {1} &amp; {1} &amp; {0}\end{array}\right]=\left[ \begin{array}{lll}{2} &amp; {1} &amp; {1} \\ {1} &amp; {1} &amp; {0} \\ {1} &amp; {0} &amp; {1}\end{array}\right]$，和step-1.1比起来，能看到区别在于$W^{T}W$和$WW^{T}$。</p><p><strong>step-2.2</strong></p><p>同样，求它的特征值$\lambda_{1}=3, \lambda_{2}=1, \lambda_{3}=0$和特征向量$\vec{u}_{1}=\left[ \begin{array}{c}{\frac{2}{\sqrt{6} } } \\ {\frac{1}{\sqrt{6} } } \\ {\frac{1}{\sqrt{6} } }\end{array}\right], \vec{u}_{2}=\left[ \begin{array}{c}{0} \\ {\frac{-1}{\sqrt{2} } } \\ {\frac{1}{\sqrt{2} } }\end{array}\right], \vec{u}_{3}=\left[ \begin{array}{c}{\frac{-1}{\sqrt{3} } } \\ {\frac{1}{\sqrt{3} } } \\ {\frac{1}{\sqrt{3} } }\end{array}\right]$。</p><p>最终我们根据公式：$W=U \Sigma V^{T}$，就可以得到</p><p>$W=\left[ \begin{array}{ll}{1} &amp; {1} \\ {0} &amp; {1} \\ {1} &amp; {0}\end{array}\right]=\left[ \begin{array}{ccc}{\frac{2}{\sqrt{6}}} &amp; {0} &amp; {-\frac{1}{\sqrt{3}}} \\ {\frac{1}{\sqrt{6}}} &amp; {\frac{-1}{\sqrt{2}}} &amp; {\frac{1}{\sqrt{3}}} \\ {\frac{1}{\sqrt{6}}} &amp; {\frac{1}{\sqrt{2}}} &amp; {\frac{1}{\sqrt{3}}}\end{array}\right] \left[ \begin{array}{cc}{\sqrt{3}} &amp; {0} \\ {0} &amp; {1} \\ {0} &amp; {0}\end{array}\right] \left[ \begin{array}{cc}{\frac{1}{\sqrt{2}}} &amp; {\frac{1}{\sqrt{2}}} \\ {\frac{1}{\sqrt{2}}} &amp; {\frac{-1}{\sqrt{2}}}\end{array}\right]$</p><p>其中$U$就是$B$的特征向量拼成的酉矩阵，$V^{T}$就是$C$的特征向量拼成的酉矩阵。而$\Sigma$(即S)就是由<strong>特征值开根号</strong>得到的。</p><p>我们还可以把上面的矩阵写成另外的表达方式：</p><p>$W=\sqrt{3} \left[ \begin{array}{c}{\frac{2}{\sqrt{6}}} \\ {\frac{1}{\sqrt{6}}} \\ {\frac{1}{\sqrt{6}}}\end{array}\right] \left[ \begin{array}{cc}{\frac{1}{\sqrt{2}}} &amp; {\frac{1}{\sqrt{2}}}\end{array}\right]+\left[ \begin{array}{c}{0} \\ {-\frac{1}{\sqrt{2}}} \\ {\frac{1}{\sqrt{2}}}\end{array}\right] \left[ \begin{array}{cc}{\frac{1}{\sqrt{2}}} &amp; {\frac{-1}{\sqrt{2}}}\end{array}\right]$</p><p>也就是$W=\sqrt\lambda_{1}\vec{u}_{1}\left(\vec{v}_{1}\right)^{T}+\sqrt\lambda_{2}\vec{u}_{2}\left(\vec{v}_{2}\right)^{T}$。再说一次，<strong>奇异值的平方=特征值</strong>。</p><p>这样也就可以解释为什么SVD会被用于PCA，实际上我们只是举了很简单的例子，当矩阵的维数更高时，我们最后得到的式子的项数也就越多，但是有些项数的奇异值很小，舍去这些项虽然会丢失局部细节，但是能够在尽量保真的情况下节约存储空间。</p><p><strong>参考资料：</strong></p><p>[1] <a href="http://www.cnblogs.com/LeftNotEasy/archive/2011/01/19/svd-and-applications.html" target="_blank" rel="noopener">机器学习中的数学(5)-强大的矩阵奇异值分解(SVD)及其应用</a></p><p>[2] <a href="https://blog.csdn.net/u012162613/article/details/42214205" target="_blank" rel="noopener">【简化数据】奇异值分解(SVD)</a> </p><p>[3] <a href="https://www.bilibili.com/video/av15971352/?p=3" target="_blank" rel="noopener">矩阵分析之奇异值分解（SVD）</a></p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;在做PCA的实验时，遇到的SVD奇异值分解问题，这里记录自己直观的理解&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;通常在&lt;code&gt;python&lt;/code&gt;中我们都是直接调用&lt;code&gt;numpy&lt;/code&gt;模块中的函数，那么返回的U,S,V几个值代表什么意思呢？&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;U, S, V = np.linalg.svd(X)&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;遇事不明，手册先行：&lt;a href=&quot;https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.svd.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;官方手册&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="Machine Learning" scheme="http://yoursite.com/tags/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>机器学习笔记-第八周</title>
    <link href="http://yoursite.com/2019/04/29/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E7%AC%AC%E5%85%AB%E5%91%A8/"/>
    <id>http://yoursite.com/2019/04/29/机器学习笔记-第八周/</id>
    <published>2019-04-29T03:55:29.000Z</published>
    <updated>2019-04-29T04:02:11.567Z</updated>
    
    <content type="html"><![CDATA[<h2 id="无监督学习"><a href="#无监督学习" class="headerlink" title="无监督学习"></a>无监督学习</h2><h3 id="无监督学习-1"><a href="#无监督学习-1" class="headerlink" title="无监督学习"></a>无监督学习</h3><p>之间学习的都是监督学习，也就是样本都有标签。</p><p><img src="https://s2.ax1x.com/2019/04/28/EMlktI.png" alt="EMlktI.png"></p><a id="more"></a><p>而无监督学习的样本是没有标签的，也就是只有输入x，没有输出标记。</p><p><img src="https://s2.ax1x.com/2019/04/28/EMllAs.md.png" alt="EMllAs.md.png"></p><p>无监督学习就是找到隐含在这类无标签数据中的结构，这类算法称为聚类算法（<strong>clustering algorithm</strong>）。</p><h3 id="K-Means算法"><a href="#K-Means算法" class="headerlink" title="K-Means算法"></a>K-Means算法</h3><p>k均值算法，先随机生成两个数据，即聚类中心（<strong>cluster centroids</strong>）。</p><p><img src="https://s2.ax1x.com/2019/04/28/EM1lrD.md.png" alt="EM1lrD.md.png"></p><p>然后遍历所有数据，和蓝色聚类中心靠近的，就渲染为蓝色；和红色聚类中心靠近的，就渲染为红色。然后移动聚类中心到同颜色点的均值处。</p><p><img src="https://s2.ax1x.com/2019/04/28/EM1Bqg.png" alt="EM1Bqg.png"></p><p>重复上述步骤，直到聚类完成，聚类中心不再变化，样本的分类也不再变化。</p><p><img src="https://s2.ax1x.com/2019/04/28/EM14LF.png" alt="EM14LF.png"></p><p>通过图像我们可以直观的了解到，使用K均值算法聚类，输入需要样本特征$x^{(i)}$和聚类中心数K。</p><p>具体步骤如下：</p><p><img src="https://s2.ax1x.com/2019/04/28/EM8ObD.png" alt="EM8ObD.png"></p><p>循环1是根据聚类中心分类，循环2是根据平均值移动聚类中心。其中$c^{(i)}$表示的是$x^{(i)}$到不同聚类中心，距离最小的那个的聚类中心的索引值。</p><p>有时候数据的分类不那么明确，比如T-shirt的尺寸。</p><p><img src="https://s2.ax1x.com/2019/04/28/EMGLzq.png" alt="EMGLzq.png"></p><p>同样也可以用K均值算法进行聚类。将T-shirt的尺寸根据身高和体重，分类为S、M、L。</p><p><img src="https://s2.ax1x.com/2019/04/28/EMGvLT.png" alt="EMGvLT.png"></p><h3 id="优化目标"><a href="#优化目标" class="headerlink" title="优化目标"></a>优化目标</h3><p>K均值算法同样有优化目标，或者说代价函数（这里也叫畸变函数）。</p><p><img src="https://s2.ax1x.com/2019/04/28/EMNc0f.png" alt="EMNc0f.png"></p><p>代价函数就是样本到聚类中心距离的平方和，而优化目标就是找到代价函数值最小时的$c^{(i)}$和聚类中心$\mu_{k}$，从下图也可以看出两个循环就是为了得到它们。</p><p><img src="https://s2.ax1x.com/2019/04/28/EMUFAO.md.png" alt="EMUFAO.md.png"></p><h3 id="随机初始化"><a href="#随机初始化" class="headerlink" title="随机初始化"></a>随机初始化</h3><p>在实际运用中，一般来说，都是随机选取K个样本点作为聚类中心而不是像上面那样选择非样本点。当然，K的值要小于m。</p><p>通常我们都期望随机选取的聚类中心具有比较好的性质</p><p><img src="https://s2.ax1x.com/2019/04/28/EMwxIK.png" alt="EMwxIK.png"></p><p>但人生不如意之事十之八九，有时候选择的聚类中心不理想</p><p><img src="https://s2.ax1x.com/2019/04/28/EM08Zq.png" alt="EM08Zq.png"></p><p>就会在训练的过程中，导致局部最优解，如下图中右下角那样的结果。</p><p><img src="https://s2.ax1x.com/2019/04/28/EMwD58.png" alt="EMwD58.png"></p><p>解决这个问题的方法简单粗暴，就是多次随机初始化聚类中心，得到多个拟合结果，然后挑选出代价最小的分类方法    。</p><p><img src="https://s2.ax1x.com/2019/04/28/EMBGXd.png" alt="EMBGXd.png"></p><h3 id="选取聚类数量"><a href="#选取聚类数量" class="headerlink" title="选取聚类数量"></a>选取聚类数量</h3><p>关于如何确定K值，是一个很难回答的问题，甚至可以说没有确切的答案。但通常我们会采用以下的方法来帮助确定K的大小。</p><ul><li>肘部法则（<strong>Elbow method</strong>）</li></ul><p><img src="https://s2.ax1x.com/2019/04/28/EM2vX8.png" alt="EM2vX8.png"></p><p>但有些时候，得到的图像看起来像是连续的，不容易判断“肘部“位置。</p><p><img src="https://s2.ax1x.com/2019/04/28/EMRn74.png" alt="EMRn74.png"></p><p>所以这个方法没办法适用于所有情况，所以通常推荐的方法是：根据你聚类的目的来确定K的数目（感觉是废话）。</p><h2 id="降维-Dimensionality-Reduction"><a href="#降维-Dimensionality-Reduction" class="headerlink" title="降维(Dimensionality Reduction)"></a>降维(Dimensionality Reduction)</h2><p>除了聚类之外，还有另一种无监督学习算法，叫做降维。</p><h3 id="目标I-数据压缩"><a href="#目标I-数据压缩" class="headerlink" title="目标I:数据压缩"></a>目标I:数据压缩</h3><p>降维的作用是消除特征冗余，如下图所示，特征$x_{1}$代表cm而特征$x_{2}$代表inches，含义相同，在图像上呈现出线性关系，完全可以只用一个特征来表示。</p><p><img src="https://s2.ax1x.com/2019/04/28/EMfnY9.png" alt="EMfnY9.png"></p><p>更一般的，我们可以把两个冗余的特征用一个新的特征来表示，记为$z_{1}$，如下图。从另一个角度来看，也可以认为是把所有数据投影到了绿色的那条线上。</p><p><img src="https://s2.ax1x.com/2019/04/28/EMfs0S.png" alt="EMfs0S.png"></p><p>当然，很多时候冗余的特征不只两个，但原理是一样的，比如以3维为例：</p><p><img src="https://s2.ax1x.com/2019/04/28/EMfo0U.png" alt="EMfo0U.png"></p><h3 id="目标II：可视化"><a href="#目标II：可视化" class="headerlink" title="目标II：可视化"></a>目标II：可视化</h3><p>特征很多的情况下没办法直接可视化。</p><p><img src="https://s2.ax1x.com/2019/04/28/EQYCZT.png" alt="EQYCZT.png"></p><p>这时候需要用到降维。</p><p><img src="https://s2.ax1x.com/2019/04/28/EQYBFg.png" alt="EQYBFg.png"></p><p>降维之后的特征可能不具有物理意义，但方便画图。</p><p><img src="https://s2.ax1x.com/2019/04/28/EQY7lR.png" alt="EQY7lR.png"></p><h3 id="主成分分析问题规划I"><a href="#主成分分析问题规划I" class="headerlink" title="主成分分析问题规划I"></a>主成分分析问题规划I</h3><p>主成分分析问题规划（<strong>Principle Component Analysis</strong>），是目前最流行的降维算法。举个例子简单说明原理：</p><p><img src="https://s2.ax1x.com/2019/04/28/EQavDI.png" alt="EQavDI.png"></p><p>图为从2维降到1维，PCA要求样本到投影点的距离的（投影误差）平方和最小。</p><p>更一般的情况，从n-D投影到k-D，也就是找到k个向量$u^{(1)},u^{(2)},…,u^{(3)}$作为被投影的线性子空间。再用3D投影到2D为例，记住PCA关键是投影误差要最小</p><p><img src="https://s2.ax1x.com/2019/04/28/EQWAYj.png" alt="EQWAYj.png"></p><ul><li>PCA和线性回归的区别</li></ul><p>二者毫无联系，虽然有时看上去图像是一样的，但线性回归时预测输出变量，它的代价函数表示的是拟合模型的输出误差，如图左；PCA的代价函数则是投影误差，如图右。</p><p><img src="https://s2.ax1x.com/2019/04/28/EQW73n.png" alt="EQW73n.png"></p><h3 id="主成分分析问题规划II"><a href="#主成分分析问题规划II" class="headerlink" title="主成分分析问题规划II"></a>主成分分析问题规划II</h3><ul><li>特征缩放和均值归一化</li></ul><p><img src="https://s2.ax1x.com/2019/04/29/ElGKMt.png" alt="ElGKMt.png"></p><ul><li>求出$u^{(i)}$</li></ul><p>把数据从n维降到k维。</p><p>计算协方差矩阵：</p><p><img src="https://s2.ax1x.com/2019/04/29/ElGIoD.png" alt="ElGIoD.png"></p><p>其中$x^{(i)}$是nx1维，所以$\Sigma$是nxn维。写成向量形式：$\Sigma=1 / m\left(X^{T} X\right)$</p><p>再用svd对协方差矩阵进行奇异值分解：</p><p><img src="https://s2.ax1x.com/2019/04/29/ElGHWd.png" alt="ElGHWd.png"></p><p>最后我们需要的是U矩阵（nxn），取它的k个列向量构成矩阵z，再乘上$x^{(i)}$就可以得到$z^{(i)}$。</p><p><img src="https://s2.ax1x.com/2019/04/29/ElGvef.png" alt="ElGvef.png"></p><h3 id="主成分数量选择"><a href="#主成分数量选择" class="headerlink" title="主成分数量选择"></a>主成分数量选择</h3><p>之前提到过如何选择压缩后的维度k是很困难的，但在PCA中有一种很实用的方法，这一节展开说说。</p><p>通常来说，选择的k要让平均投影误差/数据总方差小于0.01，这种情况也称为“99%的方差被保留”，这句话代表了投影降维后的效果好。</p><p><img src="https://s2.ax1x.com/2019/04/29/ElwiUP.png" alt="ElwiUP.png"></p><p>从上面的公式可以看出，如果需要选择出合适的K，我们可以尝试不同的K值，直到找到能让公式小于0.01为止。</p><p><img src="https://s2.ax1x.com/2019/04/29/El07Y6.png" alt="El07Y6.png"></p><p>但这种方法显然效率不高，因此最好使用另一种方法。</p><p>之前提到的奇异值分解：$[U,S,V]=svd(Sigma)$，其中的矩阵S是一个对角矩阵，如下图所示，取主对角线中的k个值，满足$1-\frac{\sum_{i=1}^{k} S_{i i} }{\sum_{i=1}^{k} S_{ii} } \leqslant 0.01$即可。</p><p><img src="https://s2.ax1x.com/2019/04/29/El0XOH.png" alt="El0XOH.png"></p><h3 id="压缩重现"><a href="#压缩重现" class="headerlink" title="压缩重现"></a>压缩重现</h3><p>如果能从n维降至k维，同理也就能从k维升至n维。只需要把$z^{(i)}$代入$X_{appox} $=$ U_{radue} \cdot z^{(i)}$。就能求出x近似值。</p><p><img src="https://s2.ax1x.com/2019/04/29/ElDQUI.png" alt="ElDQUI.png"></p><h3 id="应用PCA的建议"><a href="#应用PCA的建议" class="headerlink" title="应用PCA的建议"></a>应用PCA的建议</h3><p>PCA也可以应用在监督学习算法中，只需要将$x^{(i)}$提取出来即可。另外必须要注意的是，把$x^{(i)}$映射到$z^{(i)}$（即求出U）的过程中只能在训练集中运行。</p><p><img src="https://s2.ax1x.com/2019/04/29/Elsvgf.png" alt="Elsvgf.png"></p><ul><li>PCA的应用</li></ul><p>（1）压缩数据，加快算法运行效率</p><p>（2）可视化：没得办法，我们只能可视化K&lt;3的数据</p><p>（3）不要用PCA来防止过拟合，因为PCA不使用标签y，这可能会导致信息的丢失。用正则化就行了，不要皮。</p><p>（4）在使用PCA之前，应该先考虑清楚是否真的有必要使用它，直接用原始数据也许就能得出想要的答案。</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;无监督学习&quot;&gt;&lt;a href=&quot;#无监督学习&quot; class=&quot;headerlink&quot; title=&quot;无监督学习&quot;&gt;&lt;/a&gt;无监督学习&lt;/h2&gt;&lt;h3 id=&quot;无监督学习-1&quot;&gt;&lt;a href=&quot;#无监督学习-1&quot; class=&quot;headerlink&quot; title=&quot;无监督学习&quot;&gt;&lt;/a&gt;无监督学习&lt;/h3&gt;&lt;p&gt;之间学习的都是监督学习，也就是样本都有标签。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://s2.ax1x.com/2019/04/28/EMlktI.png&quot; alt=&quot;EMlktI.png&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="Machine Learning" scheme="http://yoursite.com/tags/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>机器学习-吴恩达机器学习之支持向量机</title>
    <link href="http://yoursite.com/2019/04/29/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/"/>
    <id>http://yoursite.com/2019/04/29/机器学习-吴恩达机器学习之支持向量机/</id>
    <published>2019-04-28T23:46:42.000Z</published>
    <updated>2019-04-29T08:10:07.261Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Support-Vector-Machine"><a href="#Support-Vector-Machine" class="headerlink" title="Support Vector Machine"></a>Support Vector Machine</h2><p>导入库函数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> scipy.io <span class="keyword">import</span> loadmat</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> svm</span><br></pre></td></tr></table></figure><a id="more"></a><h3 id="Example-Dataset-1"><a href="#Example-Dataset-1" class="headerlink" title="Example Dataset 1"></a>Example Dataset 1</h3><p>画出含有两个特征的样本集的线性边界函数。</p><h4 id="数据处理"><a href="#数据处理" class="headerlink" title="数据处理"></a>数据处理</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">data=loadmat(<span class="string">'ex6data1.mat'</span>)</span><br><span class="line">X=data[<span class="string">'X'</span>]</span><br><span class="line">y=data[<span class="string">'y'</span>].flatten()</span><br></pre></td></tr></table></figure><p>不知道标签的话可以查看</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(data.keys()) <span class="comment"># 用于查看标签名称</span></span><br></pre></td></tr></table></figure><ul><li>将数据可视化以便观察，注意要把正负样本分开可视化。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#==================================观察数据</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_data</span><span class="params">(X,y)</span>:</span></span><br><span class="line">    positive=X[y==<span class="number">1</span>]</span><br><span class="line">    negative=X[y==<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    fig,ax=plt.subplots(figsize=(<span class="number">8</span>,<span class="number">5</span>))    </span><br><span class="line">    plt.scatter(positive[:,<span class="number">0</span>],positive[:,<span class="number">1</span>],marker=<span class="string">'+'</span>,label=<span class="string">'positive'</span>)</span><br><span class="line">    plt.scatter(negative[:,<span class="number">0</span>],negative[:,<span class="number">1</span>],color=<span class="string">'red'</span>,label=<span class="string">'negative'</span>)</span><br></pre></td></tr></table></figure><h4 id="SVM拟合"><a href="#SVM拟合" class="headerlink" title="SVM拟合"></a>SVM拟合</h4><p>可以通过改变c的大小来观察决策边界的变换。C的大小影响着模型对异常样本的反应。</p><p>因为这里是线性分类，所以使用线性核函数，参数<code>kernel=&#39;linear&#39;</code>。使用<code>sklearn</code>中的<code>svm.SVC()</code>来拟合，其结果返回一个分类器对象。最后还需要用<code>clf.fit(X,y)</code>来拟合出最终模型。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">c=<span class="number">1</span></span><br><span class="line">clf=svm.SVC(c,kernel=<span class="string">'linear'</span>) <span class="comment"># 参数 c,kernel 返回一个分类器对象</span></span><br><span class="line">clf.fit(X,y) <span class="comment"># 用训练数据拟合分类器模型</span></span><br></pre></td></tr></table></figure><h4 id="可视化决策边界"><a href="#可视化决策边界" class="headerlink" title="可视化决策边界"></a>可视化决策边界</h4><p><code>np.meshgrid()</code>生成网格点，再对每个网格点进行预测，最后画出等高线图，即决策边界。</p><p>关于<code>np.meshgrid()</code>可以参考 <a href="https://nullblog.top/2019/03/16/Numpy%E4%B8%AD%E7%9A%84Meshgrid/" target="_blank" rel="noopener">Numpy中的Meshgrid</a>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#=============================================可视化决策边界</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">visualize_boundary</span><span class="params">(clf,X)</span>:</span></span><br><span class="line">    x_min,x_max=X[:,<span class="number">0</span>].min()*<span class="number">1.2</span>,X[:,<span class="number">0</span>].max()*<span class="number">1.1</span></span><br><span class="line">    y_min,y_max=X[:,<span class="number">1</span>].min()*<span class="number">1.2</span>,X[:,<span class="number">1</span>].max()*<span class="number">1.1</span></span><br><span class="line">    xx,yy=np.meshgrid(np.arange(x_min,x_max,<span class="number">0.02</span>),np.arange(y_min,y_max,<span class="number">0.02</span>)) <span class="comment"># 画网格点</span></span><br><span class="line">    Z=clf.predict(np.c_[xx.ravel(),yy.ravel()]) <span class="comment"># 用训练好的分类器对网格点进行预测</span></span><br><span class="line"></span><br><span class="line">    Z=Z.reshape(xx.shape) <span class="comment"># 转换成对应的网格点</span></span><br><span class="line">    plt.contour(xx,yy,Z,level=[<span class="number">0</span>],colors=<span class="string">'black'</span>) <span class="comment"># 等高线图，画出0/1分界线</span></span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><p>像前面提到的，不同的C值对决策边界会产生不同影响</p><p>c=1时：</p><p><img src="https://s2.ax1x.com/2019/04/27/EKmvo6.md.png" alt="EKmvo6.md.png"></p><p>c=1000时：</p><p><img src="https://s2.ax1x.com/2019/04/27/EKno7t.md.png" alt="EKno7t.md.png"></p><p>注意左上角的正样本，c较大时，决策边界会过于追求将数据正确分类，而失去<strong>大间距</strong>的特点。</p><h3 id="SVM-with-Gaussion-Kernels"><a href="#SVM-with-Gaussion-Kernels" class="headerlink" title="SVM with Gaussion Kernels"></a>SVM with Gaussion Kernels</h3><h4 id="Gaussion-Kernel"><a href="#Gaussion-Kernel" class="headerlink" title="Gaussion Kernel"></a>Gaussion Kernel</h4><p>用SVM做分线性分类，我们需要用到高斯核函数。</p><ul><li>公式：</li></ul><p><img src="https://s2.ax1x.com/2019/04/27/EKKnMQ.md.png" alt="EKKnMQ.md.png"></p><ul><li>代码：</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#=============================================高斯核函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gaussKernel</span><span class="params">(x1,x2,sigma)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> np.exp(-((x1-x2)**<span class="number">2</span>).sum()/(<span class="number">2</span>*sigma**<span class="number">2</span>))</span><br></pre></td></tr></table></figure><h4 id="Example-Dataset-2"><a href="#Example-Dataset-2" class="headerlink" title="Example Dataset 2"></a>Example Dataset 2</h4><ul><li>数据处理及可视化</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">data2=loadmat(<span class="string">'ex6data2.mat'</span>)</span><br><span class="line">X2=data2[<span class="string">'X'</span>]</span><br><span class="line">y2=data2[<span class="string">'y'</span>].flatten()</span><br></pre></td></tr></table></figure><p><img src="https://s2.ax1x.com/2019/04/27/EKKoJf.md.png" alt="EKKoJf.md.png"></p><ul><li>用高斯核函数拟合模型</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sigma=<span class="number">0.1</span></span><br><span class="line">gamma=np.power(sigma,<span class="number">-2</span>)/<span class="number">2</span></span><br><span class="line">clf=svm.SVC(c,kernel=<span class="string">'rbf'</span>,gamma=gamma) <span class="comment"># 注意这里的参数gamma是整个分母，且要写成乘法形式</span></span><br><span class="line">clf.fit(X2,y2)</span><br></pre></td></tr></table></figure><p><code>kernel=&#39;rbf&#39;</code>代表使用高斯核函数，其中<code>gamma</code>值就是公式中的整个分母项，即$\frac{1}{2\sigma^{2} }$。</p><ul><li>决策边界</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">plot_data(X2,y2)</span><br><span class="line">visualize_boundary(clf,X2)</span><br></pre></td></tr></table></figure><p><img src="https://s2.ax1x.com/2019/04/27/EKJ52q.md.png" alt="EKJ52q.md.png"></p><h4 id="Example-Dataset-3"><a href="#Example-Dataset-3" class="headerlink" title="Example Dataset 3"></a>Example Dataset 3</h4><ul><li>数据处理即可视化</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">data3=loadmat(<span class="string">'ex6data3.mat'</span>)</span><br><span class="line">X3=data3[<span class="string">'X'</span>]</span><br><span class="line">y3=data3[<span class="string">'y'</span>].flatten()</span><br></pre></td></tr></table></figure><p><img src="https://s2.ax1x.com/2019/04/27/EKGYkj.md.png" alt="EKGYkj.md.png"></p><p>可以发现，有个别样本存在比较大的差异。</p><ul><li>使用带高斯核函数的SVM进行训练</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">clf=svm.SVC(c,kernel=<span class="string">'rbf'</span>,gamma=gamma)</span><br><span class="line">clf.fit(X3,y3)</span><br></pre></td></tr></table></figure><ul><li>决策边界</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">plot_data(X3,y3)</span><br><span class="line">visualize_boundary(clf,X3)</span><br></pre></td></tr></table></figure><p><img src="https://s2.ax1x.com/2019/04/27/EKGHNd.md.png" alt="EKGHNd.md.png"></p><h2 id="Spam-Classification"><a href="#Spam-Classification" class="headerlink" title="Spam Classification"></a>Spam Classification</h2><p>建立一个垃圾邮件分类器，下面这个例子将会告诉你如何通过一封邮件来建立特征向量。</p><p>导入库函数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> scipy.io <span class="keyword">import</span> loadmat</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> svm</span><br><span class="line"><span class="keyword">import</span> re <span class="comment"># 电子邮件处理的正则表达式</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 一个英文分词算法(Poter stemmer)</span></span><br><span class="line"><span class="keyword">import</span> nltk, nltk.stem.porter</span><br></pre></td></tr></table></figure><h3 id="Preprocesssing-Email"><a href="#Preprocesssing-Email" class="headerlink" title="Preprocesssing Email"></a>Preprocesssing Email</h3><ul><li>读取数据</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> open(<span class="string">'emailSample1.txt'</span>,<span class="string">'r'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    email=f.read()</span><br></pre></td></tr></table></figure><p>打印邮件内容为</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&gt; Anyone knows how much it costs to host a web portal ?</span><br><span class="line">&gt;</span><br><span class="line">Well, it depends on how many visitors you&apos;re expecting.</span><br><span class="line">This can be anywhere from less than 10 bucks a month to a couple of $100. </span><br><span class="line">You should checkout http://www.rackspace.com/ or perhaps Amazon EC2 </span><br><span class="line">if youre running something big..</span><br><span class="line"></span><br><span class="line">To unsubscribe yourself from this mailing list, send an email to:</span><br><span class="line">groupname-unsubscribe@egroups.com</span><br></pre></td></tr></table></figure><p>一般邮件都具有一些相似的内容，比如数字、URL、其它邮件地址。因此我们会采取一些”标准化“的方法来处理邮件，这些方法会提高垃圾邮件分类的性能。</p><p>例如：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">1. Lower-casing: 把整封邮件转化为小写。</span><br><span class="line">2. Stripping HTML: 移除所有HTML标签，只保留内容。</span><br><span class="line">3. Normalizing URLs: 将所有的URL替换为字符串 “httpaddr”.</span><br><span class="line">4. Normalizing Email Addresses: 所有的地址替换为 “emailaddr”</span><br><span class="line">5. Normalizing Dollars: 所有dollar符号($)替换为“dollar”.</span><br><span class="line">6. Normalizing Numbers: 所有数字替换为“number”</span><br><span class="line">7. Word Stemming(词干提取): 将所有单词还原为词源。例如，“discount”, “discounts”, “discounted” and “discounting”都替换为“discount”。</span><br><span class="line">8. Removal of non-words: 移除所有非文字类型，所有的空格(tabs, newlines, spaces)调整为一个空格.</span><br></pre></td></tr></table></figure><ul><li>邮件内容处理：</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">process_email</span><span class="params">(email)</span>:</span></span><br><span class="line">    <span class="string">"""做除了Word Stemming和Removal of non-words的所有处理"""</span></span><br><span class="line">    email = email.lower()</span><br><span class="line">    email = re.sub(<span class="string">'&lt;[^&lt;&gt;]&gt;'</span>, <span class="string">' '</span>, email)  </span><br><span class="line">    <span class="comment"># 匹配&lt;开头，然后所有不是&lt; ,&gt; 的内容，知道&gt;结尾，相当于匹配&lt;...&gt;</span></span><br><span class="line">    email = re.sub(<span class="string">'(http|https)://[^\s]*'</span>, <span class="string">'httpaddr'</span>, email )  </span><br><span class="line">    <span class="comment"># 匹配//后面不是空白字符的内容，遇到空白字符则停止</span></span><br><span class="line">    email = re.sub(<span class="string">'[^\s]+@[^\s]+'</span>, <span class="string">'emailaddr'</span>, email)</span><br><span class="line">    email = re.sub(<span class="string">'[\$]+'</span>, <span class="string">'dollar'</span>, email)</span><br><span class="line">    email = re.sub(<span class="string">'[\d]+'</span>, <span class="string">'number'</span>, email) </span><br><span class="line">    <span class="keyword">return</span> email</span><br></pre></td></tr></table></figure><p>再接下来提取词干，去除非字符内容，并返回一个单词列表。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">email2TokenList</span><span class="params">(email)</span>:</span></span><br><span class="line">    <span class="string">"""预处理数据，返回一个干净的单词列表"""</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># I'll use the NLTK stemmer because it more accurately duplicates the</span></span><br><span class="line">    <span class="comment"># performance of the OCTAVE implementation in the assignment</span></span><br><span class="line">    stemmer = nltk.stem.porter.PorterStemmer()</span><br><span class="line">    </span><br><span class="line">    email = process_email(email)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将邮件分割为单个单词，re.split() 可以设置多种分隔符</span></span><br><span class="line">    tokens = re.split(<span class="string">'[ \@\$\/\#\.\-\:\&amp;\*\+\=\[\]\?\!\(\)\&#123;\&#125;\,\'\"\&gt;\_\&lt;\;\%]'</span>, email)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 遍历每个分割出来的内容</span></span><br><span class="line">    tokenlist = []</span><br><span class="line">    <span class="keyword">for</span> token <span class="keyword">in</span> tokens:</span><br><span class="line">        <span class="comment"># 删除任何非字母数字的字符</span></span><br><span class="line">        token = re.sub(<span class="string">'[^a-zA-Z0-9]'</span>, <span class="string">''</span>, token);</span><br><span class="line">        <span class="comment"># Use the Porter stemmer to 提取词根</span></span><br><span class="line">        stemmed = stemmer.stem(token)</span><br><span class="line">        <span class="comment"># 去除空字符串‘’，里面不含任何字符</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> len(token): <span class="keyword">continue</span></span><br><span class="line">        tokenlist.append(stemmed)</span><br><span class="line">            </span><br><span class="line">    <span class="keyword">return</span> tokenlist</span><br></pre></td></tr></table></figure><h4 id="Vocabulary-List"><a href="#Vocabulary-List" class="headerlink" title="Vocabulary List"></a>Vocabulary List</h4><p>我们得到了邮件的单词列表，接下来需要结合记录实际中经常使用到的单词的词汇表<code>vocab.txt</code>。函数返回邮件单词在词汇表中的索引值。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">email2VocabIndices</span><span class="params">(email, vocab)</span>:</span></span><br><span class="line">    <span class="string">"""提取存在单词的索引"""</span></span><br><span class="line">    token = email2TokenList(email)</span><br><span class="line">    index = [i <span class="keyword">for</span> i <span class="keyword">in</span> range(len(vocab)) <span class="keyword">if</span> vocab[i] <span class="keyword">in</span> token]</span><br><span class="line">    <span class="keyword">return</span> index</span><br></pre></td></tr></table></figure><p>得到的索引值如下：</p><p><img src="https://s2.ax1x.com/2019/04/27/EK0An1.png" alt="EK0An1.png"></p><h4 id="Extracting-Feature-from-Emails"><a href="#Extracting-Feature-from-Emails" class="headerlink" title="Extracting Feature from Emails"></a>Extracting Feature from Emails</h4><p>如果邮件中的单词出现在词汇表的第i个位置，则把特征向量的第i个索引值置为1,；如果没出现，置为0。也就是说，建立一个和词汇表同维度的向量feature，再把上面得到的索引位置的值改写为1，其余为0。可以得到：</p><p><img src="https://s2.ax1x.com/2019/04/27/EKcP5q.png" alt="EKcP5q.png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">email_feature_vector</span><span class="params">(email)</span>:</span></span><br><span class="line">    <span class="string">'''将email的单词转换为特征向量0/1'''</span></span><br><span class="line">    df = pd.read_table(<span class="string">'vocab.txt'</span>, names=[<span class="string">'words'</span>])</span><br><span class="line">    vocab = df.values <span class="comment"># Datafram转换为ndarray</span></span><br><span class="line">    vector = np.zeros(len(vocab))</span><br><span class="line">    vecab_indices = email2VocabIndices(email, vocab)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> vecab_indices:</span><br><span class="line">        vector[i] = <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> vector</span><br></pre></td></tr></table></figure><p>该向量长度为1899，其中有45个索引值为1。</p><h3 id="Training-SVM-for-Spam-Classification"><a href="#Training-SVM-for-Spam-Classification" class="headerlink" title="Training SVM for Spam Classification"></a>Training SVM for Spam Classification</h3><p>用已经预处理过的训练集和测试集来拟合模型，并计算准确度。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">clf = svm.SVC(C=<span class="number">0.1</span>, kernel=<span class="string">'linear'</span>)</span><br><span class="line">clf.fit(X, y)</span><br><span class="line">print(clf.score(X,y),clf.score(Xtest,ytest))</span><br></pre></td></tr></table></figure><p>打印结果：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">0.99825 0.989</span><br></pre></td></tr></table></figure><h3 id="Top-predictors-for-Spam"><a href="#Top-predictors-for-Spam" class="headerlink" title="Top predictors for Spam"></a>Top predictors for Spam</h3><p>返回权重最大的15个单词，这些单词出现频率高的邮件就是垃圾邮件。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_vocab_list</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">'''以字典形式获得词汇表'''</span></span><br><span class="line">    vocab_dict = &#123;&#125;</span><br><span class="line">    <span class="keyword">with</span> open(<span class="string">'vocab.txt'</span>) <span class="keyword">as</span> f:  <span class="comment">#打开txt格式的词汇表</span></span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> f:</span><br><span class="line">            (val, key) = line.split()  <span class="comment">#读取每一行的键和值</span></span><br><span class="line">            vocab_dict[int(val)] = key  <span class="comment">#存放到字典中</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> vocab_dict</span><br></pre></td></tr></table></figure><p>获取词汇表</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">vocab_list = get_vocab_list()  <span class="comment">#得到词汇表 存在字典中</span></span><br><span class="line">indices = np.argsort(clf.coef_).flatten()[::<span class="number">-1</span>]  <span class="comment">#对权重序号进行从大到小排序 并返回</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">15</span>):  <span class="comment">#打印权重最大的前15个词 及其对应的权重</span></span><br><span class="line">    print(<span class="string">'&#123;&#125; (&#123;:0.6f&#125;)'</span>.format(vocab_list[indices[i]],</span><br><span class="line">                                clf.coef_.flatten()[indices[i]]))</span><br></pre></td></tr></table></figure><p>最终打印结果:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">otherwis (0.500614)</span><br><span class="line">clearli (0.465916)</span><br><span class="line">remot (0.422869)</span><br><span class="line">gt (0.383622)</span><br><span class="line">visa (0.367710)</span><br><span class="line">base (0.345064)</span><br><span class="line">doesn (0.323632)</span><br><span class="line">wife (0.269724)</span><br><span class="line">previous (0.267298)</span><br><span class="line">player (0.261169)</span><br><span class="line">mortgag (0.257298)</span><br><span class="line">natur (0.253941)</span><br><span class="line">ll (0.253467)</span><br><span class="line">futur (0.248297)</span><br><span class="line">hot (0.246404)</span><br></pre></td></tr></table></figure><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p>[1] <a href="https://blog.csdn.net/sdu_hao/article/details/84189266" target="_blank" rel="noopener">机器学习 | 吴恩达机器学习第七周编程作业(Python版)</a></p><p>[2] <a href="https://blog.csdn.net/Cowry5/article/details/80465922" target="_blank" rel="noopener">吴恩达机器学习作业Python实现(六)：SVM支持向量机</a></p><blockquote><p>这次的代码几乎完全<strong>照抄</strong>两位大神的，实在是正则化的内容不懂，而因为用了sklearn库使得拟合变得又很简单。记录一下自己的naive，还得努力。</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Support-Vector-Machine&quot;&gt;&lt;a href=&quot;#Support-Vector-Machine&quot; class=&quot;headerlink&quot; title=&quot;Support Vector Machine&quot;&gt;&lt;/a&gt;Support Vector Machine&lt;/h2&gt;&lt;p&gt;导入库函数&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; numpy &lt;span class=&quot;keyword&quot;&gt;as&lt;/span&gt; np&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; pandas &lt;span class=&quot;keyword&quot;&gt;as&lt;/span&gt; pd&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; matplotlib.pyplot &lt;span class=&quot;keyword&quot;&gt;as&lt;/span&gt; plt&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;from&lt;/span&gt; scipy.io &lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; loadmat&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;from&lt;/span&gt; sklearn &lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; svm&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
    
      <category term="Machine Learning" scheme="http://yoursite.com/tags/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>numpy中向量的表示方法</title>
    <link href="http://yoursite.com/2019/04/27/numpy%E4%B8%AD%E5%90%91%E9%87%8F%E7%9A%84%E8%A1%A8%E7%A4%BA%E6%96%B9%E6%B3%95/"/>
    <id>http://yoursite.com/2019/04/27/numpy中向量的表示方法/</id>
    <published>2019-04-27T11:17:01.000Z</published>
    <updated>2019-04-27T11:27:46.982Z</updated>
    
    <content type="html"><![CDATA[<h3 id="行向量"><a href="#行向量" class="headerlink" title="行向量"></a>行向量</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">np.array([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>]])</span><br></pre></td></tr></table></figure><p>结果:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>]]</span><br></pre></td></tr></table></figure><p><code>.shape</code>为（1,3）。是一个一行三列的<strong>行向量</strong>。</p><a id="more"></a><h3 id="数组"><a href="#数组" class="headerlink" title="数组"></a>数组</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">np.array([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>])</span><br></pre></td></tr></table></figure><p>结果：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[1,2,3]</span><br></pre></td></tr></table></figure><p><code>.shape</code>为（3，）。是一个ndarray<strong>数组</strong>，严格来说并不是向量，注意和行向量区分。</p><h3 id="列向量"><a href="#列向量" class="headerlink" title="列向量"></a>列向量</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">np.array([[<span class="number">1</span>],[<span class="number">2</span>],[<span class="number">3</span>]])</span><br></pre></td></tr></table></figure><p>结果：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[[<span class="number">1</span>],</span><br><span class="line"> [<span class="number">2</span>],</span><br><span class="line"> [<span class="number">3</span>]]</span><br></pre></td></tr></table></figure><p><code>.shape</code>为（3,1）。是一个三行一列的<strong>列向量</strong>。</p>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;行向量&quot;&gt;&lt;a href=&quot;#行向量&quot; class=&quot;headerlink&quot; title=&quot;行向量&quot;&gt;&lt;/a&gt;行向量&lt;/h3&gt;&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;np.array([[&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;,&lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;,&lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;]])&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;结果:&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;[[&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;,&lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;,&lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;]]&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;&lt;code&gt;.shape&lt;/code&gt;为（1,3）。是一个一行三列的&lt;strong&gt;行向量&lt;/strong&gt;。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Python" scheme="http://yoursite.com/tags/Python/"/>
    
      <category term="Numpy" scheme="http://yoursite.com/tags/Numpy/"/>
    
  </entry>
  
  <entry>
    <title>机器学习笔记|第七周SVM</title>
    <link href="http://yoursite.com/2019/04/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E7%AC%AC%E4%B8%83%E5%91%A8/"/>
    <id>http://yoursite.com/2019/04/26/机器学习笔记-第七周/</id>
    <published>2019-04-26T03:16:22.000Z</published>
    <updated>2019-05-26T02:33:38.858Z</updated>
    
    <content type="html"><![CDATA[<h2 id="支持向量机"><a href="#支持向量机" class="headerlink" title="支持向量机"></a>支持向量机</h2><h3 id="优化目标"><a href="#优化目标" class="headerlink" title="优化目标"></a>优化目标</h3><p>在监督学习中，重要的不是你选择的算法，而是应用这些算法时，选择的特征、正则化参数等等诸如此类。其中有一种非常强大的分类工具，称为支持向量机。（<strong>Support Vector Machine，SVM</strong>）</p><p>下面展示如何通过修改逻辑回归，来得到SVM。</p><p>首先回顾一下逻辑回归假设函数，注意<code>y=1</code>和<code>y=0</code>的情况。</p><p><img src="https://s2.ax1x.com/2019/04/24/EVoj9P.md.png" alt="EVoj9P.md.png"></p><a id="more"></a><p>进一步写出代价函数</p><p><img src="https://s2.ax1x.com/2019/04/24/EVTC7j.md.png" alt="EVTC7j.md.png"></p><p>当<code>y=1</code>时，此时代价函数剩左边一项$-y \log \frac{1}{1+e^{-\theta^{T} x} }$，图像为</p><p><img src="https://s2.ax1x.com/2019/04/24/EVTFNn.png" alt="EVTFNn.png"></p><p>其中的曲线为逻辑回归代价函数，两段直线为SVM的代价函数，记为$cost_{1}(z)$</p><p>同理，当<code>y=0</code>时，代价函数剩右边一项$(1-y) \log \left(1-\frac{1}{1+e^{-\theta^{T} x} }\right)$，图像为</p><p><img src="https://s2.ax1x.com/2019/04/24/EVTfEj.png" alt="EVTfEj.png"></p><p>其中的SVM代价函数，记为$cost_{0}(z)$。</p><p>完整的逻辑回归代价函数写成：</p><p>$\min _{\theta} \frac{1}{m}\left[\sum_{i=1}^{m} y^{(i)}\left(-\log h_{\theta}\left(x^{(i)}\right)\right)+\left(1-y^{(i)}\right)\left(\left(-\log \left(1-h_{\theta}\left(x^{(i)}\right)\right)\right)\right]+\frac{\lambda}{2 m} \sum_{j=1}^{n} \theta_{j}^{2}\right.$</p><p>可以看出由代价项和正则化项两部分组成，基本形式为$A+\lambda B$，$\lambda$可以看做是控制两部分的<em>权重</em>。</p><p>要写出SVM的代价函数，首先我们把逻辑回归代价函数的$\frac{1}{m}$去掉，不影响$\theta$值。然后我们把正则化项的系数$\lambda$去掉，在代价项前面添上一个系数C，效果等同于$\frac{1}{\lambda}$。</p><p>因此，最终SVM的代价函数为：</p><p>$\min _{\theta} C \sum_{i=1}^{m}\left[y^{(i)} \cos t_{1}\left(\theta^{T} x^{(i)}\right)+\left(1-y^{(i)}\right) \operatorname{cost}_{0}\left(\theta^{T} x^{(i)}\right)\right]+\frac{1}{2} \sum_{i=1}^{n} \theta_{j}^{2}$</p><p>假设函数为：</p><p><img src="https://s2.ax1x.com/2019/04/24/EVHUkd.png" alt="EVHUkd.png"></p><p>注意，和逻辑回归假设函数计算出的是概率不同，SVM假设函数直接计算结果(1/0)。</p><h3 id="直观理解大间距"><a href="#直观理解大间距" class="headerlink" title="直观理解大间距"></a>直观理解大间距</h3><p>SVM又被称为大间距分类器，以下直观讲解为什么。</p><p><img src="https://s2.ax1x.com/2019/04/25/EZ0zzq.md.png" alt="EZ0zzq.md.png"></p><p>从图中可以看出：</p><p><code>y=1</code>时，需要$\theta^{T}x\ge1$，区别于逻辑回归中$\ge0$，此时$cost_{1}(z)=0$，即SVM在正确分类的基础上还构建了一个安全距离。</p><p><code>y=0</code>时，需要$\theta^{T}x\le-1$，区别于逻辑回归中$\le0$，此时$cost_{0}(z)=0$，即SVM在正确分类的基础上还构建了一个安全距离。</p><p>接下来，如果我们把代价函数中的常数C设置成一个非常大的值，例如C=100000，此时为了使代价最小，就必须让最优化函数<img src="https://s2.ax1x.com/2019/04/25/EZBHpR.md.png" alt="EZBHpR.md.png"></p><p>中蓝色框部分等于0，又根据<code>y=1</code>或<code>y=0</code>时$\theta^{T}x$的取值，可以把优化问题和约束条件改为：</p><p><img src="https://s2.ax1x.com/2019/04/25/EZBv7D.png" alt="EZBv7D.png"></p><p>当你把它最小化时，可以得到决策边界。</p><p><img src="https://s2.ax1x.com/2019/04/25/EZDphd.png" alt="EZDphd.png"></p><p>其中黑线为SVM的决策边界，比起粉红线和绿线，显然黑线具有更好的鲁棒性（robustness，又叫<strong>健壮性</strong>，在计算机中代表运行过程中处理错误，或是算法、输入异常时仍然正确运行的能力），距离正负样本的最小距离最大。也因此我们把SVM称为最大间距分类。</p><p>如果把C设置的非常大时，可能会出现下面的情况</p><p><img src="https://s2.ax1x.com/2019/04/25/EZDK9s.png" alt="EZDK9s.png"></p><p>由于左下角出现了一个异常点，决策边界会从黑线变为粉红线。相当于$\lambda$过小导致过拟合。如果把C值设置的小一点，则会忽略异常点的影响。因此对C的讨论，其实也是过拟合和欠拟合的问题。</p><p><img src="https://s2.ax1x.com/2019/04/25/EZDcUe.png" alt="EZDcUe.png"></p><h3 id="大间隔分类器数学原理"><a href="#大间隔分类器数学原理" class="headerlink" title="大间隔分类器数学原理"></a>大间隔分类器数学原理</h3><p>本节解释了为什么SVM能够大间隔分类</p><ul><li>向量内积</li></ul><p><img src="https://s2.ax1x.com/2019/04/25/EZfmL9.md.png" alt="EZfmL9.md.png"></p><p>简单来说，就是内积等于$u^{T}v$，也等于$p\lVert u \rVert$，其中p为向量v对向量u的投影，$\lVert u \rVert$为范数（norm，意思是具有“长度”概念的函数，这里可以简单理解为向量的长度）。内积具有正负，当向量夹角大于90度，内积为负；小于90度，内积为正。</p><p>那么回到优化问题上</p><p><img src="https://s2.ax1x.com/2019/04/25/EZ5mLQ.md.png" alt="EZ5mLQ.md.png"></p><p>我们假设n=2，$\theta_{0}=0$，然后通过之前提到的内容，可以改写优化问题：$\min _{\theta} \frac{1}{2} \sum_{j=1}^{n} \theta_{j}^{2}=\frac{1}{2}|\theta|^{2}$</p><p>也可以改写约束条件，即把$\theta^{T} x^{(i)}$看成是向量相乘，最终能改写为下面的式子：</p><p><img src="https://s2.ax1x.com/2019/04/25/EZ5Tl8.png" alt="EZ5Tl8.png"></p><p>那么为什么SVM不会选择下图中的决策边界呢？</p><p><img src="https://s2.ax1x.com/2019/04/25/EZIJht.png" alt="EZIJht.png"></p><p>其原因是，由下图可以看出，位于一、四象限的样本$x^{(1)}$，因为和决策边界接近，当它投影到$\theta$上时得到的$p^{(1)}$非常小，因此如果要满足$p^{(i)} \cdot|\theta| \geq 1$，那么$|\theta|$就必须非常大，这跟我们的优化问题$\min _{\theta} \frac{1}{2} \sum_{j=1}^{n} \theta_{j}^{2}=\frac{1}{2}|\theta|^{2}$矛盾。</p><p>位于二、三象限样本同理。</p><p><img src="https://s2.ax1x.com/2019/04/25/EZIB7j.png" alt="EZIB7j.png"></p><p>正确的样本边界如下图</p><p><img src="https://s2.ax1x.com/2019/04/25/EeKFDx.png" alt="EeKFDx.png"></p><p>额外说明一下，这里的$\theta_{0}=0$意味着决策边界一定会经过原点，如果令其不等于0，结论也是同样成立的。</p><h3 id="核函数1"><a href="#核函数1" class="headerlink" title="核函数1"></a>核函数1</h3><p>本质上说，核函数跟SVM没有必然联系，但是在用于求非线性分类器时具有很好的效果。</p><p>考虑下图中的数据集：</p><p><img src="https://s2.ax1x.com/2019/04/25/EeYNJP.md.png" alt="EeYNJP.md.png"></p><p>这种非线性的情况，显然用原始输入的两个特征是无法表示的，因此我们需要增加多项式特征。</p><p><img src="https://s2.ax1x.com/2019/04/25/EecHzQ.png" alt="EecHzQ.png"></p><p>这里我们可以将特征重新编号为$f_{i}$：</p><p><img src="https://s2.ax1x.com/2019/04/25/EecLss.md.png" alt="EecLss.md.png"></p><p>但问题在于，如何去选取新的特征呢？之前在逻辑回归时，我们是列举出了多个不同组合方式，通过交叉验证集进行验证，挑选出最合适的。而在SVM中，我们可以通过核函数对原始输入特征进行映射进而得到新的特征。</p><p>举例说明：</p><p>假设此时原始输入x有两个特征$x_{1}$和$x_{2}$（不考虑$x_{0}$），需要得到新的特征$f_{1}$、$f_{2}$、$f_{3}$。</p><p>首先我们选取了3个点$l_{1}$、$l_{2}$、$l_{3}$，称为标记（<strong>landmark</strong>）。</p><p><img src="https://s2.ax1x.com/2019/04/25/Eegrmn.md.png" alt="Eegrmn.md.png"></p><p>从图中可以看出，$f_{i}$为x和$l^{(i)}$相似度，而这个求相似度的函数就被称为核函数（<strong>kernel</strong>），这里用到的是高斯核函数（<strong>Gaussion Kernels</strong>），${ {f}_{1} }=similarity(x,{ {l}^{ (1) } } )=e(-\frac{ { {\left| x-{ {l}^{(1)} } \right|}^{2} } }{2{ {\sigma }^{2} } } )$。</p><p>那么右边的式子究竟是什么含义？</p><p><img src="https://s2.ax1x.com/2019/04/25/Ee2KA0.md.png" alt="Ee2KA0.md.png"></p><p>图中可以很明显的看出，$f_{i}$的取值在0~1之间。</p><p>绘制出核函数的图像如下</p><p><img src="https://s2.ax1x.com/2019/04/25/Ee2f4f.md.png" alt="Ee2f4f.md.png"></p><p>图中水平面坐标代表$x_{1}$和$x_{2}$，垂直的坐标代表$f$，可以看出，只有当x和$l$重合时，$f$才具有最大值。</p><p>同时也可以看出$\sigma$对$f$改变速率的影响。$\sigma$较小时，中间凸起的部分较窄，当x远离$l$时，$f$下降的较快；当$\sigma$较大时则刚好相反。</p><p>得到新的特征值$f$之后，就可以写出假设函数$h(\theta)=\theta_{0}+\theta_{1} f_{1}+\theta_{2} f_{2}+\theta_{3} f_{3}$，进而在图像上画出决策边界如下：</p><p><img src="https://s2.ax1x.com/2019/04/25/EeWpJf.png" alt="EeWpJf.png"></p><p>可以看出，当样本位于粉红色点的位置时，x靠近$l^{(1)}$，y=1；位于绿色点时，靠近$l^{(2)}$，y=1；位于蓝色点时，靠近$l^{(3)}$，y=0。</p><h3 id="核函数2"><a href="#核函数2" class="headerlink" title="核函数2"></a>核函数2</h3><p>在上一节中，我们提到一个重要问题，那就是如何选取标记？</p><p>通常根据训练集的样本数量来选择对应地标，即如果训练集中有m个样本，就选取m个标记。并且令：$l^{(1)}=x^{(1)}, l^{(2)}=x^{(2)}, \ldots, l^{(m)}=x^{(m)}$，这样做的好处在于我们得到的新标记是基于原有特征与其他样本特征的距离。</p><p><img src="https://s2.ax1x.com/2019/04/26/EmE7Q0.md.png" alt="EmE7Q0.md.png"></p><p>由上图我们可以得出结论，对于一个样本$x^{(i)}$，我们可以将它的特征映射为$f^{(i)}$，从维度上看，我们也将原始输入特征从n+1维，映射为m+1维（+1为偏置项，m为标记个数也是样本个数）</p><p><img src="https://s2.ax1x.com/2019/04/26/EmExY9.png" alt="EmExY9.png"></p><p>下面将核函数运用到SVM中，首先我们将原始输入特征映射为新特征$f$，然后根据代价函数求出最优化的$\theta$</p><p>$min C\sum\limits_{i=1}^{m}{ [ { {y}^{ (i)} }cos { {t}_{1} } }( { {\theta }^{T} }{ {f}^{(i)} })+(1-{ {y}^{(i)} })cos { {t}_{0} }( { {\theta }^{T} }{ {f}^{(i)} })]+\frac{1}{2}\sum\limits_{j=1}^{m}{\theta _{j}^{2} }$</p><p>注意这里的最后的正则化项，$j$是从1-m而不是1-n，因为我们新特征$f \in \mathbb{R}^{m+1}$。而在实际运用中，我们还得对这一项的计算做一些修改，通常我们对正则化项的计算是用矩阵相乘$\sum_{j=1}^{n=m} \theta_{j}^{2}=\theta^{T} \theta$，但在SVM中，我们用$θ^TMθ$代替$θ^Tθ$，其中M根据我们选择的核函数来确定，这样可以简化运算。</p><p>最后再代入新的假设函数中。</p><p>使用SVM时还需要选择几个参数：</p><p><img src="https://s2.ax1x.com/2019/04/26/Emn4xA.md.png" alt="Emn4xA.md.png"></p><h3 id="使用SVM"><a href="#使用SVM" class="headerlink" title="使用SVM"></a>使用SVM</h3><p>使用成熟的软件包来计算参数</p><p><img src="https://s2.ax1x.com/2019/04/26/EmMZHx.md.png" alt="EmMZHx.md.png"></p><p>不过还是有些问题需要你自己解决</p><p><img src="https://s2.ax1x.com/2019/04/26/EmMQ8e.png" alt="EmMQ8e.png"></p><p>关于内核函数的选择</p><p>1.可以使用线性内核函数，即不用内核函数，这种情况适用于n较大，m较小。因为如果样本个数少，特征多，去拟合复杂的非线性函数，容易导致过拟合。</p><p><img src="https://s2.ax1x.com/2019/04/26/EmMOPO.md.png" alt="EmMOPO.md.png"></p><p>2.也可以使用核函数（高斯核函数），适用于n较小，m较大。</p><p><img src="https://s2.ax1x.com/2019/04/26/Em1gat.md.png" alt="Em1gat.md.png"></p><p>以一个高斯核函数为例，输入两个特征向量，输出一个实数<img src="https://s2.ax1x.com/2019/04/26/Em3gOJ.png" alt="Em3gOJ.png"></p><p>注意在使用高斯核函数时，一定要对原始输入特征进行缩放：</p><p><img src="https://s2.ax1x.com/2019/04/26/Em35Y6.md.png" alt="Em35Y6.md.png"></p><p>否则会如图中那样，不同范围的特征在生成新特征时会产生不同的影响。</p><p>除此之外还可以选择其他的核函数，但是都必须满足莫塞尔定理，不过除了线性核函数和高斯核函数，其他的一般也不太常用。</p><p><img src="https://s2.ax1x.com/2019/04/26/Em8ehV.md.png" alt="Em8ehV.md.png"></p><h4 id="多类分类问题"><a href="#多类分类问题" class="headerlink" title="多类分类问题"></a>多类分类问题</h4><p><img src="https://s2.ax1x.com/2019/04/26/EmJnL4.png" alt="EmJnL4.png"></p><p>解决多分类问题的方法</p><p>1.可以使用封装好的模块</p><p>2.和之前逻辑回归中多分类问题类似，也可以使用把多分类问题转换成多个二分类问题，最终对新样本进行预测时，只要看它属于哪个正类的假设函数值最大，就属于哪个类别。</p><h4 id="关于逻辑回归、核函数SVM、不含核函数SVM的选择"><a href="#关于逻辑回归、核函数SVM、不含核函数SVM的选择" class="headerlink" title="关于逻辑回归、核函数SVM、不含核函数SVM的选择"></a>关于逻辑回归、核函数SVM、不含核函数SVM的选择</h4><p><img src="https://s2.ax1x.com/2019/04/26/EmJH6U.md.png" alt="EmJH6U.md.png"></p><p>SVM的效果可能不如神经网络，但训练速度快，且是一个凸优化问题（即一定能得到全局最优解）。</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;支持向量机&quot;&gt;&lt;a href=&quot;#支持向量机&quot; class=&quot;headerlink&quot; title=&quot;支持向量机&quot;&gt;&lt;/a&gt;支持向量机&lt;/h2&gt;&lt;h3 id=&quot;优化目标&quot;&gt;&lt;a href=&quot;#优化目标&quot; class=&quot;headerlink&quot; title=&quot;优化目标&quot;&gt;&lt;/a&gt;优化目标&lt;/h3&gt;&lt;p&gt;在监督学习中，重要的不是你选择的算法，而是应用这些算法时，选择的特征、正则化参数等等诸如此类。其中有一种非常强大的分类工具，称为支持向量机。（&lt;strong&gt;Support Vector Machine，SVM&lt;/strong&gt;）&lt;/p&gt;
&lt;p&gt;下面展示如何通过修改逻辑回归，来得到SVM。&lt;/p&gt;
&lt;p&gt;首先回顾一下逻辑回归假设函数，注意&lt;code&gt;y=1&lt;/code&gt;和&lt;code&gt;y=0&lt;/code&gt;的情况。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://s2.ax1x.com/2019/04/24/EVoj9P.md.png&quot; alt=&quot;EVoj9P.md.png&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="Machine Learning" scheme="http://yoursite.com/tags/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>机器学习笔记|第六周</title>
    <link href="http://yoursite.com/2019/04/24/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E7%AC%AC%E5%85%AD%E5%91%A8/"/>
    <id>http://yoursite.com/2019/04/24/机器学习笔记-第六周/</id>
    <published>2019-04-24T07:37:37.000Z</published>
    <updated>2019-05-02T10:21:39.332Z</updated>
    
    <content type="html"><![CDATA[<h2 id="运用机器学习建议"><a href="#运用机器学习建议" class="headerlink" title="运用机器学习建议"></a>运用机器学习建议</h2><h3 id="决定下一步做什么？"><a href="#决定下一步做什么？" class="headerlink" title="决定下一步做什么？"></a>决定下一步做什么？</h3><p>当你的模型运用于新的样本时，如果产生巨大的误差该怎么办？</p><p>一般来说，有以下几种处理方式：</p><ul><li>获得更多的训练样本——通常是有效的，但代价较大，下面的方法也可能有效，可考虑先采用下面的几种方法。</li><li>尝试减少特征的数量</li><li>尝试获得更多的特征</li><li>尝试增加多项式特征</li><li>尝试减少正则化程度$\lambda$</li><li>尝试增加正则化程度$\lambda$</li></ul><p>当然我们不可能随机去一个个方法尝试，所以需要一点手段来预测。</p><a id="more"></a><h3 id="评估假设函数"><a href="#评估假设函数" class="headerlink" title="评估假设函数"></a>评估假设函数</h3><p>首先，如果你计算出的误差（代价函数值）非常大，那么选取的假设函数就可能存在问题；即便误差小，也有可能引起过拟合。</p><p>那么如何来判断过拟合问题？</p><p>可以将数据集按比例划分成，训练集（Training set）：测试集（Test set）=70：30。</p><p><img src="https://s2.ax1x.com/2019/04/13/AqXOCq.md.png" alt="AqXOCq.md.png"></p><p>然后将训练集得到的模型运用于测试集，用来计算误差：</p><p>1.对于线性回归模型，我们直接计算<img src="https://s2.ax1x.com/2019/04/13/ALnxt1.png" alt="ALnxt1.png"></p><p>2.对于逻辑回归模型，除了计算代价函数<img src="https://s2.ax1x.com/2019/04/13/AL18fA.png" alt="AL18fA.png"></p><p>更常见的方法是计算误分类的比例（0/1错误分类度量）<img src="https://s2.ax1x.com/2019/04/13/AqjGxf.png" alt="AqjGxf.png"></p><h3 id="模型选择和交叉验证集"><a href="#模型选择和交叉验证集" class="headerlink" title="模型选择和交叉验证集"></a>模型选择和交叉验证集</h3><p>当模型确定的时候可以使用上一节的方法来验证，但是如何确定一个模型呢？首先需要知道，正则化惩罚项系数$\lambda$的选择；增加多项式特征时，多项式的次数等这类问题，称为模型选择问题。</p><p>假设我们的输入特征只有一个时，拟合效果是非常不理想的。因此，我们通常会增加特征项，那么问题又来了，多项式特征的次数应该怎么选取，即到底选取什么样的模型（假设）？</p><p>我们可以罗列出多种情况</p><p><img src="https://s2.ax1x.com/2019/04/13/AqvWX8.md.png" alt="AqvWX8.md.png"></p><p>这时再将数据集划分成训练集和测试集，对这些假设分别在训练集上训练，通过最小化训练集的代价，求出最优参数$\Theta_{1}$~$\Theta_{10}$。将其代入测试集，计算每个模型的误差，选择误差最小的那组作为假设，并把这组的误差值作为泛化误差。</p><p>然而其中存在一个问题：通过测试集来选取模型，又用测试集来求泛化误差，显然是不是坠吼滴。</p><p>因此，我们重新划分数据集的比例，训练集：<strong>交叉验证集(Cross Validation set)</strong>：测试集=60:20:20</p><p><img src="https://s2.ax1x.com/2019/04/13/Aqzdde.md.png" alt="Aqzdde.md.png"></p><p>然后计算选择出模型：</p><p>1.用训练集训练出$\Theta_{1}$~$\Theta_{10}$;</p><p>2.用交叉验证集计算出最小误差，选择误差最小的模型;</p><p>3.用第2步中选择的模型计算测试集得出泛化误差。</p><p><strong><em>Train/validation/test error</em></strong></p><p><strong>Training error:</strong></p><p>​            <img src="https://s2.ax1x.com/2019/04/24/EVNxVs.png" alt="EVNxVs.png"></p><p><strong>Cross Validation error:</strong></p><p>​            <img src="https://s2.ax1x.com/2019/04/24/EVUS5q.png" alt="EVUS5q.png"></p><p><strong>Test error:</strong></p><p>​            <img src="https://s2.ax1x.com/2019/04/24/EVUCGV.png" alt="EVUCGV.png"></p><blockquote><p>上面说到的，是关于如何改变特征来减小误差，而接下来的内容则和正则化$\lambda$有关</p></blockquote><h3 id="诊断偏差和方差"><a href="#诊断偏差和方差" class="headerlink" title="诊断偏差和方差"></a>诊断偏差和方差</h3><p>当运行结果不理想时，多半有两种情况：<strong>过拟合</strong>或者<strong>欠拟合</strong>。而这两种情况，哪种和高偏差有关？哪种和高方差有关？还是都有关系？</p><p><img src="https://s2.ax1x.com/2019/04/13/ALkcV0.md.png" alt="ALkcV0.md.png"></p><p>从图中可以看出，欠拟合时高偏差，过拟合时高方差。那么在没法画图的情况下（基本都是这种情况）如何来确定是高偏差还是高方差呢？</p><p>方法如下：</p><p><img src="https://s2.ax1x.com/2019/04/13/ALAvkV.png" alt="ALAvkV.png"></p><p>将训练集误差和验证集误差绘制在图中，其中横坐标为多项式次数。</p><p><strong>Training error:</strong>                               <img src="https://s2.ax1x.com/2019/04/24/EVUP2T.png" alt="EVUP2T.png"></p><p><strong>Cross Validation error:</strong>                <img src="https://s2.ax1x.com/2019/04/24/EVUkMF.png" alt="EVUkMF.png"></p><p>可以明显看出</p><ul><li>对于训练集，d越大，误差越小</li><li>对于验证集，随着d增长，误差会先减小后增大，其中的最低点就是开始过拟合的情况。</li></ul><p>那么当我们在没有图像的情况下，得出验证集误差较大时，只需要根据训练集误差的大小就能得出是高偏差还是高方差了。</p><p><img src="https://s2.ax1x.com/2019/04/13/ALE10I.md.png" alt="ALE10I.md.png"></p><p>具体来说，就是：</p><ul><li>训练集误差和交叉验证集误差近似时：偏差/欠拟合</li><li>交叉验证集误差远大于训练集误差时：方差/过拟合</li></ul><h3 id="正则化和偏差、方差"><a href="#正则化和偏差、方差" class="headerlink" title="正则化和偏差、方差"></a>正则化和偏差、方差</h3><p>正则化常常用来解决过拟合问题，但是对于正则化项参数$\lambda$，如下图所示不同的选取可能会导致不同的偏差问题，那么应该如何选择合适的值呢？</p><p><img src="https://s2.ax1x.com/2019/04/13/AL8954.md.png" alt="AL8954.md.png"></p><p>我们先选择一系列想要测试的$\lambda$，通常是0~12之间呈现2倍的关系，例如：$0,0.01,0.02,0.04,0.08,0.15,0.32,0.64,1.28,2.56,5.12,10$（共12个）。</p><p>再像之前那样，把数据集划分为训练集、验证集和测试集。但是因为之前我们都是不带正则化，所以$J(\theta)$和$J_{train}(\theta)$是一样的，但是这里就要如下图中的那样分开。</p><p><img src="https://s2.ax1x.com/2019/04/13/AL8MPH.md.png" alt="AL8MPH.md.png"></p><p>先在训练集上用最小化代价函数，求得最优的一组参数。然后再用该参数在训练/验证/测试集上根据公式计算相应误差。</p><p>（这里存疑？关于$J(\theta)$和$J_{train}(\theta)$什么时候使用）</p><p><img src="https://s2.ax1x.com/2019/04/13/ALJn9H.md.png" alt="ALJn9H.md.png"></p><p>具体步骤如下：</p><p>1.先对每个$\lambda$值，求对应最小化代价函数的参数</p><p><img src="https://s2.ax1x.com/2019/04/13/ALJ0uq.png" alt="ALJ0uq.png"></p><p>2.将上述求得的参数，代入交叉验证集计算代价函数$J_{cv}(\theta)$，选择对应最小代价函数的$\theta$作为最优模型参数。</p><p>3.代入测试集$J_{test}(\theta)$计算泛化误差。</p><p>还可以把验证集误差和训练集误差绘制成图，横坐标为$\lambda$。</p><p><img src="https://s2.ax1x.com/2019/04/13/ALw9I0.md.png" alt="ALw9I0.md.png"></p><p>可以看出：</p><ul><li>当$\lambda$较小时，$J_{train}(\theta)$值较小，$J_{cv}(\theta)$值较大，过拟合</li><li>当$\lambda$较大时，$J_{train}(\theta)$值较大，$J_{cv}(\theta)$值较大，欠拟合</li></ul><h3 id="学习曲线"><a href="#学习曲线" class="headerlink" title="学习曲线"></a>学习曲线</h3><p>学习曲线是一个非常好的工具，用于判断模型偏差、方差问题。它是一个以<strong>样本个数</strong>为横坐标，以<strong>误差</strong>为纵坐标绘制的图像。</p><h4 id="1-如何利用学习曲线识别高偏差？"><a href="#1-如何利用学习曲线识别高偏差？" class="headerlink" title="1.如何利用学习曲线识别高偏差？"></a>1.如何利用学习曲线识别高偏差？</h4><p><img src="https://s2.ax1x.com/2019/04/13/AL2J9s.md.png" alt="AL2J9s.md.png"></p><p>从图中可以看出，对于欠拟合而言，增加样本个数没有意义，因为模型拟合能力差，没有能力去关注”细节“。</p><p><img src="https://s2.ax1x.com/2019/04/13/AL2wHU.png" alt="AL2wHU.png"></p><p>从这张图中也可以看出，当样本到达一定数量时，训练集合测试集的误差会非常接近且不再变化，但高于我们期望的误差。</p><h4 id="如何利用学习曲线识别高方差？"><a href="#如何利用学习曲线识别高方差？" class="headerlink" title="如何利用学习曲线识别高方差？"></a>如何利用学习曲线识别高方差？</h4><p><img src="https://s2.ax1x.com/2019/04/13/ALfbvR.md.png" alt="ALfbvR.md.png"></p><p>可以看出在过拟合情况下，增加数据，可能可以提高算法效果。</p><p><img src="https://s2.ax1x.com/2019/04/13/ALh3q0.png" alt="ALh3q0.png"></p><p>增加样本个数会小幅度提高训练集误差，但是始终维持在一个相对较低的水平。而验证集个数增加为增进模型对数据的了解，因此会验证集误差会减小</p><blockquote><p>上面的test error和cross validation error用哪个效果都一样。</p></blockquote><h4 id="决定下一步做什么？-1"><a href="#决定下一步做什么？-1" class="headerlink" title="决定下一步做什么？"></a>决定下一步做什么？</h4><p>如何通过这些诊断法来帮助我们选择改进模型的方法呢？回到最初的问题上</p><ol><li>获得更多的训练样本——解决高方差</li><li>尝试减少特征的数量——解决高方差</li><li>尝试获得更多的特征——解决高偏差</li><li>尝试增加多项式特征——解决高偏差</li><li>尝试减少正则化程度λ——解决高偏差</li><li>尝试增加正则化程度λ——解决高方差</li></ol><p><strong>神经网络</strong></p><p><img src="https://s2.ax1x.com/2019/04/14/AOn2b4.md.png" alt="AOn2b4.md.png"></p><p>使用神经元较少的神经网络（左图）跟参数较少时情况类似，容易导致高偏差和欠拟合；同理，神经元较多（右图）这容易导致高方差，可以使用正则化手段来解决。</p><p>一般使用神经元较多的情况比较好处理。当然你也可以同样把数据集划分为训练集、交叉验证集和测试集。然后从一层隐藏层开始逐一尝试。找到验证集误差最小的作为模型。</p><h2 id="机器学习系统设计"><a href="#机器学习系统设计" class="headerlink" title="机器学习系统设计"></a>机器学习系统设计</h2><h3 id="确定执行的优先级"><a href="#确定执行的优先级" class="headerlink" title="确定执行的优先级"></a>确定执行的优先级</h3><p>以建立一个垃圾邮件分类器为例</p><h5 id="step1-用向量表示邮件"><a href="#step1-用向量表示邮件" class="headerlink" title="step1.用向量表示邮件"></a>step1.用向量表示邮件</h5><p>输入变量x为邮件的特征；y表示邮件的标签，1代表垃圾邮件，0代表不是。</p><p>我们可以人工选择100个单词作为词典，然后比对邮件中单词是否出现，出现则记为1，未出现记为0（注意不是记录出现个数），因此能够得到下图中的100维向量，这个向量作为输入。</p><p><img src="https://s2.ax1x.com/2019/04/24/EV3bgH.md.png" alt="EV3bgH.md.png"></p><blockquote><p>实际上，我们不会人工选择单词构成字典，而是在训练集中自动选择出现频率最高的n(10000-50000)个单词构成字典,然后用一个n维的特征向量来表示邮件。</p></blockquote><h5 id="step2-想办法降低分类的错误率"><a href="#step2-想办法降低分类的错误率" class="headerlink" title="step2.想办法降低分类的错误率"></a>step2.想办法降低分类的错误率</h5><p>1.收集更多的训练数据，但这种要视情况而定。</p><p>2.为每个邮件设计更复杂的特征，比如把邮件正文标题也考虑进去</p><p>3.为邮件的正文设计更复杂的特征，比如单词的单复数，discount和discounts是否应该看成一个单词；首字母大小、后缀，deal和Dealer是否应该看作一个单词；是否应该考虑标点符号，可能垃圾邮件中叹号会比较多。</p><p>4.构建更复杂的算法来检测邮件中的错误拼写，比如垃圾邮件发送者经常把一些容易被检测到的单词写错，如m0rtgage，med1cine，w4tches等，从而避免被检测到。</p><p>当然上述方法如何选择，同样也是个问题。</p><h3 id="误差分析"><a href="#误差分析" class="headerlink" title="误差分析"></a>误差分析</h3><p>当你准备好构建一个机器学习系统时，最好的方式是先用简单的方法快速实现。具体来说：</p><p>1.从一个简单的能快速实现的算法开始，实现该算法并用交叉验证集数据测试。</p><p>2.绘制学习曲线，判断是高偏差或是高方差，决定是增加更多的数据还是添加更多的特征。</p><p>3.<strong>误差分析：</strong>人工观察交叉验证集，看看被分错的样本是否具有某些规律。</p><ul><li>举个误差分析的例子：</li></ul><p>假设验证集有500个样本，$m_{cv}=500$，分类器分错了100个样本，此时可以人工检查这100个错误：首先分析这100个样本的错分类型和数量。如下所示：</p><p><img src="https://s2.ax1x.com/2019/04/24/EVYC8O.png" alt="EVYC8O.png"></p><p>卖药的邮件12封，卖假货的邮件4封，钓鱼的邮件53封，其他的31封；那么我们就可以把关注点放在钓鱼邮件上。</p><p>然会继续对钓鱼邮件分析，是否可以发现一些新的特征。能够提高分类器的的性能，比如这些钓鱼邮件存在以下几个问题：</p><p><img src="https://s2.ax1x.com/2019/04/24/EVYArd.png" alt="EVYArd.png"></p><p>我们又可以把关注点放在“不常用的标点”这一项上，毕竟有32封邮件都存在该问题，进而设计一些包含标点的更复杂的特征，以此提高性能。</p><ul><li>数值评估：</li></ul><p>在做垃圾邮件分类时，可能会遇到这种问题：</p><p>应不应该把discount,discounts,discounted,discounting看作是同一个单词,即是否看成一个特征。</p><p>在自然语言处理中会使用词干提取，在这种情况下，上述单词会被看作是同一个。但是词干提取有利有弊，这种方法通常指关注前几个字母，比如它可能会把universe/university看作一个单词，这显然不合理。</p><p>那么我们到底要不要使用词干提取？这时我们可以使用数值评估的方法：</p><p>首先使用词干提取，训练一个分类器，在验证集上计算它的错误率；然后不使用词干提取，训练一个分类器，在验证集上计算它的错误率。二者进行比较，哪个错误率低，就使用哪种方法。</p><p>类似的这种问题，还包括是否应该区分单词大小写，如mom和Mom是否应该看作一个单词，都可以使用上述数值评估的方法，选择一个合理的做法。</p><h3 id="不对称分类的误差评估"><a href="#不对称分类的误差评估" class="headerlink" title="不对称分类的误差评估"></a>不对称分类的误差评估</h3><p>我们通常用错误率/正确率来评估一个算法，但有时这种方法是不合适的，比如偏斜类问题。</p><p><img src="https://s2.ax1x.com/2019/04/24/EEHJrF.md.png" alt="EEHJrF.md.png"></p><p>以这个判断肿瘤恶性/良性的分类器为例，如果分类器错误率为1%，而实际只有0.5%的病人肿瘤为恶性。这种情况下，尽管错误率看起来非常低，也可能造成严重后果。此时哪怕直接把所有病人都认为良性，也只有0.5%的错误率，高于分类器。</p><p>这就是<strong>偏斜类</strong>问题，情况表现为训练集中同一种类样本非常多，而其他类样本样本比较少。</p><blockquote><p>那么如何解决偏斜类问题？换句话说，如何知道算法把所有病人都认为良性而没有做出真正的分类？</p></blockquote><p>可以使用查准率(<strong>Precision</strong>)和召回率(<strong>Recall</strong>)。</p><blockquote><p>召回率又可以叫做<strong>查全率</strong>，事实上叫查全率显然更符合其含义</p></blockquote><p>我的理解，查准率指预测正确的样本个数占样本总数的多少；查全率指预测正确的样本个数占实际正确的样本个数。</p><p>还是以上面肿瘤的例子</p><p><img src="https://s2.ax1x.com/2019/04/24/EELpE4.png" alt="EELpE4.png"></p><p>准确率等于True positive除以一行；</p><p>召回率等于True positive除以一列；</p><p>由此可以看出，当使用<code>y=0</code>（把所有病人判断为良性）的方法预测时，召回率为0，由此可以排除。</p><h3 id="查准率和召回率的均衡"><a href="#查准率和召回率的均衡" class="headerlink" title="查准率和召回率的均衡"></a>查准率和召回率的均衡</h3><p>在肿瘤例子中，我们一般情况下设置分类的阈值为0.5。</p><p><img src="https://s2.ax1x.com/2019/04/24/EEzys1.png" alt="EEzys1.png"></p><ul><li>我们可能需要在非常准确的情况下，才去预测肿瘤为恶性，否则让良性肿瘤的病人去接受治疗，那肯定要被骂，因此这时候我们需要高查准率，因此可以调高阈值为0.7或0.9，可以更准确的预测出恶性肿瘤。但此时明显，召回率就被降低了</li><li>但召回率的降低可能导致更多恶性肿瘤的情况被归类为良性肿瘤，这种情况也是我们不愿见到的，因此我们也可能想调高召回率。</li></ul><p>由此可以画出查准率和召回率的图像：</p><p><img src="https://s2.ax1x.com/2019/04/24/EVPm4K.png" alt="EVPm4K.png"></p><p>综上，我们需要向办法设置合适的阈值，以达到查准率和召回率的平衡。</p><p>通常使用$F_{1}score$来权衡，计算公式为${ {F}_{1} }Score:2\frac{PR}{P+R}$，$F_{1}score$值越高，模型的性能越好。</p><h3 id="机器学习数据"><a href="#机器学习数据" class="headerlink" title="机器学习数据"></a>机器学习数据</h3><p>之前曾经说过，不要盲目的收集大量训练数据。</p><p>但是有些时候，收集大量数据会得到一个性能良好的学习算法。</p><p>即当你的算法有很多参数，数据特征充足（当一个人类专家拿到输入x时，能做出良好的判断，证明特征充足）时，更多的训练数据会带来更好的性能。</p><p><strong>参考资料</strong></p><p>[1] <a href="https://blog.csdn.net/sdu_hao/article/details/84026798#7.%E5%86%B3%E5%AE%9A%E6%8E%A5%E4%B8%8B%E6%9D%A5%E5%81%9A%E4%BB%80%E4%B9%88" target="_blank" rel="noopener">机器学习 | 吴恩达机器学习第六周学习笔记</a></p><p>[2] <a href="https://zhuanlan.zhihu.com/p/28409180" target="_blank" rel="noopener">机器学习笔记（3）—— 优化，偏差和方差，偏斜数据 - 关右的文章 - 知乎</a></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;运用机器学习建议&quot;&gt;&lt;a href=&quot;#运用机器学习建议&quot; class=&quot;headerlink&quot; title=&quot;运用机器学习建议&quot;&gt;&lt;/a&gt;运用机器学习建议&lt;/h2&gt;&lt;h3 id=&quot;决定下一步做什么？&quot;&gt;&lt;a href=&quot;#决定下一步做什么？&quot; class=&quot;headerlink&quot; title=&quot;决定下一步做什么？&quot;&gt;&lt;/a&gt;决定下一步做什么？&lt;/h3&gt;&lt;p&gt;当你的模型运用于新的样本时，如果产生巨大的误差该怎么办？&lt;/p&gt;
&lt;p&gt;一般来说，有以下几种处理方式：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;获得更多的训练样本——通常是有效的，但代价较大，下面的方法也可能有效，可考虑先采用下面的几种方法。&lt;/li&gt;
&lt;li&gt;尝试减少特征的数量&lt;/li&gt;
&lt;li&gt;尝试获得更多的特征&lt;/li&gt;
&lt;li&gt;尝试增加多项式特征&lt;/li&gt;
&lt;li&gt;尝试减少正则化程度$\lambda$&lt;/li&gt;
&lt;li&gt;尝试增加正则化程度$\lambda$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;当然我们不可能随机去一个个方法尝试，所以需要一点手段来预测。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Machine Learning" scheme="http://yoursite.com/tags/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>matplotlib坐标原点不重合</title>
    <link href="http://yoursite.com/2019/04/15/matplotlib%E5%9D%90%E6%A0%87%E5%8E%9F%E7%82%B9%E4%B8%8D%E9%87%8D%E5%90%88/"/>
    <id>http://yoursite.com/2019/04/15/matplotlib坐标原点不重合/</id>
    <published>2019-04-15T02:22:16.000Z</published>
    <updated>2019-04-24T07:37:43.663Z</updated>
    
    <content type="html"><![CDATA[<p>用matplotlib画图时会遇到原点不重合在左下角的情况</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">fig,ax=plt.subplots(figsize=(<span class="number">8</span>,<span class="number">5</span>))</span><br><span class="line">    ax.plot(range(<span class="number">1</span>,<span class="number">13</span>),error_train,label=<span class="string">"Train"</span>)</span><br><span class="line">    ax.plot(range(<span class="number">1</span>,<span class="number">13</span>),error_cv,label=<span class="string">"Cross Validation"</span>,color=<span class="string">"green"</span>)</span><br><span class="line">    ax.legend()</span><br><span class="line">    plt.xlabel(<span class="string">"Number of training examples"</span>)</span><br><span class="line">    plt.ylabel(<span class="string">'error'</span>)</span><br><span class="line">    plt.title(<span class="string">'Learning curve of linear regression'</span>)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><a id="more"></a><p><img src="https://s2.ax1x.com/2019/04/15/AXb0a9.md.png" alt="AXb0a9.md.png"></p><p>只需要添加两行代码即可</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">plt.ylim(bottom=<span class="number">0</span>)</span><br><span class="line">plt.xlim(left=<span class="number">0</span>)</span><br></pre></td></tr></table></figure><p><img src="https://s2.ax1x.com/2019/04/15/AXqoTJ.md.png" alt="AXqoTJ.md.png"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;用matplotlib画图时会遇到原点不重合在左下角的情况&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;fig,ax=plt.subplots(figsize=(&lt;span class=&quot;number&quot;&gt;8&lt;/span&gt;,&lt;span class=&quot;number&quot;&gt;5&lt;/span&gt;))&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    ax.plot(range(&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;,&lt;span class=&quot;number&quot;&gt;13&lt;/span&gt;),error_train,label=&lt;span class=&quot;string&quot;&gt;&quot;Train&quot;&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    ax.plot(range(&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;,&lt;span class=&quot;number&quot;&gt;13&lt;/span&gt;),error_cv,label=&lt;span class=&quot;string&quot;&gt;&quot;Cross Validation&quot;&lt;/span&gt;,color=&lt;span class=&quot;string&quot;&gt;&quot;green&quot;&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    ax.legend()&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    plt.xlabel(&lt;span class=&quot;string&quot;&gt;&quot;Number of training examples&quot;&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    plt.ylabel(&lt;span class=&quot;string&quot;&gt;&#39;error&#39;&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    plt.title(&lt;span class=&quot;string&quot;&gt;&#39;Learning curve of linear regression&#39;&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    plt.show()&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
    
      <category term="matplotlib" scheme="http://yoursite.com/tags/matplotlib/"/>
    
  </entry>
  
</feed>
