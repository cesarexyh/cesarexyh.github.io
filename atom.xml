<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>空博客</title>
  
  <subtitle>总不能浪费个副标题吧</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2019-04-08T02:06:46.741Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>JQK/许阳航</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>subplots画图</title>
    <link href="http://yoursite.com/2019/04/07/subplots%E7%94%BB%E5%9B%BE/"/>
    <id>http://yoursite.com/2019/04/07/subplots画图/</id>
    <published>2019-04-07T10:32:59.000Z</published>
    <updated>2019-04-08T02:06:46.741Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>matplotlib是python领域中很常见的绘图模块</p></blockquote><p>几乎只在需要用到时查手册，这里记录一些比较常用的函数，有机会再来系统学习一下</p><a id="more"></a><p>以一段画图代码为例</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_data</span><span class="params">(X)</span>:</span></span><br><span class="line">    <span class="string">'''随机画100个数字'''</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># np.random.choice(arrange,size),返回ndarray</span></span><br><span class="line">    index=np.random.choice(range(<span class="number">5000</span>),<span class="number">100</span>) </span><br><span class="line">    images=X[index] <span class="comment"># 随机选择100个样本</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># ax_array为Axes对象</span></span><br><span class="line">    fig,ax_array=plt.subplots(<span class="number">10</span>,<span class="number">10</span>,sharex=<span class="keyword">True</span>,sharey=<span class="keyword">True</span>,figsize=(<span class="number">8</span>,<span class="number">8</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> r <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">        <span class="keyword">for</span> c <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">            <span class="comment"># matshow() 第一个参数为要显示的矩阵</span></span><br><span class="line">            <span class="comment">#Display an array as a matrix in a new figure window</span></span><br><span class="line">            ax_array[r,c].matshow(images[r*<span class="number">10</span>+c].reshape(<span class="number">20</span>,<span class="number">20</span>),cmap=<span class="string">'gray_r'</span>)</span><br><span class="line">    plt.yticks([])</span><br><span class="line">    plt.xticks([])</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><p>1.<code>matplotlib.pyplot.subplots()</code>：创建一个figure和一组subplots</p><p>参数：</p><ul><li>nrows，ncols：<code>axes</code>的数量，这里是10*10</li><li>sharex，sharey：共享所有<code>axes</code>X轴和y轴的属性，设置<code>True</code>开启</li></ul><p>返回值：</p><ul><li>figure</li><li>ax：一个或多个<code>axes</code>对象</li></ul><p><strong>*</strong>  <code>axes</code>和<code>subplot</code>的区别：简单来说，如果把<code>figure</code>看做是电脑桌面，那么<code>axes</code>就是可自由移动的图标，<code>subplot</code>则是不可自由移动的图标。</p><p>2.<code>matplotlib.pyplot.matshow()</code>：在窗口用矩阵显示一个数组</p><p>参数：</p><ul><li>array-like(M,N)：要显示的(M,N)<strong>矩阵</strong></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;matplotlib是python领域中很常见的绘图模块&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;几乎只在需要用到时查手册，这里记录一些比较常用的函数，有机会再来系统学习一下&lt;/p&gt;
    
    </summary>
    
    
      <category term="python" scheme="http://yoursite.com/tags/python/"/>
    
      <category term="matplotlib" scheme="http://yoursite.com/tags/matplotlib/"/>
    
  </entry>
  
  <entry>
    <title>机器学习|吴恩达机器学习之神经网络</title>
    <link href="http://yoursite.com/2019/04/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    <id>http://yoursite.com/2019/04/06/机器学习-吴恩达机器学习之神经网络/</id>
    <published>2019-04-06T07:43:14.000Z</published>
    <updated>2019-04-06T08:27:28.290Z</updated>
    
    <content type="html"><![CDATA[<h2 id="多类分类问题"><a href="#多类分类问题" class="headerlink" title="多类分类问题"></a>多类分类问题</h2><p>就是把多个类别细分为多个0-1类别来分析</p><p><img src="https://s2.ax1x.com/2019/04/06/AW3v7T.png" alt="AW3v7T.png"></p><a id="more"></a><h3 id="代码："><a href="#代码：" class="headerlink" title="代码："></a>代码：</h3><h4 id="导入与处理数据"><a href="#导入与处理数据" class="headerlink" title="导入与处理数据"></a>导入与处理数据</h4><p>1.导入模块</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> scipy.io <span class="keyword">import</span> loadmat</span><br><span class="line"><span class="keyword">from</span> scipy.optimize <span class="keyword">import</span> minimize</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> Sigmoid</span><br></pre></td></tr></table></figure><p>2.导入数据</p><p>因为图像的灰度值是用<code>.mat</code>存储的，所以用<code>loadmat</code>来导入到<code>python</code>中</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 5000个训练样本，每个样本20*20的灰度值，展开为400维向量。输出为0~9的数字</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_data</span><span class="params">(path)</span>:</span></span><br><span class="line">    data=loadmat(path)</span><br><span class="line">    X=data[<span class="string">'X'</span>]</span><br><span class="line">    y=data[<span class="string">'y'</span>]</span><br><span class="line">    <span class="keyword">return</span> X,y</span><br></pre></td></tr></table></figure><p>3.可以随机显示一张图片来观察</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_an_image</span><span class="params">(X,y)</span>:</span></span><br><span class="line">    pick_one=np.random.randint(<span class="number">0</span>,<span class="number">5000</span>)</span><br><span class="line">    image=X[pick_one,:]</span><br><span class="line">    fig,ax=plt.subplots(figsize=(<span class="number">1</span>,<span class="number">1</span>))</span><br><span class="line">    ax.matshow(image.reshape(<span class="number">20</span>,<span class="number">20</span>),cmap=<span class="string">'gray_r'</span>)</span><br><span class="line">    plt.xticks([])</span><br><span class="line">    plt.yticks([])</span><br><span class="line">    plt.show()</span><br><span class="line">    print(<span class="string">'this should be &#123;&#125;'</span>.format(y[pick_one]))</span><br></pre></td></tr></table></figure><p>结果如图：</p><p><img src="https://s2.ax1x.com/2019/04/06/AWG3RJ.png" alt="AWG3RJ.png"></p><h4 id="代价函数"><a href="#代价函数" class="headerlink" title="代价函数"></a>代价函数</h4><p>正则化项逻辑回归，和之前的没区别</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cost_reg</span><span class="params">(theta,X,y,lmd)</span>:</span></span><br><span class="line">    theta_reg=theta[<span class="number">1</span>:]</span><br><span class="line">    first=y*np.log(Sigmoid.sigmoid(X@theta))+(<span class="number">1</span>-y)*np.log(<span class="number">1</span>-Sigmoid.sigmoid(X@theta))</span><br><span class="line">    reg=(lmd/(<span class="number">2</span>*len(X)))*(theta_reg@theta_reg)<span class="comment"># 惩罚项不从第一项开始</span></span><br><span class="line">    <span class="keyword">return</span> -np.mean(first)+reg</span><br></pre></td></tr></table></figure><h4 id="梯度函数"><a href="#梯度函数" class="headerlink" title="梯度函数"></a>梯度函数</h4><p>和之前的没区别，只不过这里用了<code>np.concatenate()</code>来拼接。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradient_reg</span><span class="params">(theta,X,y,lmd)</span>:</span></span><br><span class="line">    theta_reg=theta[<span class="number">1</span>:]</span><br><span class="line">    first=(<span class="number">1</span>/len(X))*(X.T@(Sigmoid.sigmoid(X@theta)-y))</span><br><span class="line">    reg=np.concatenate([np.array([<span class="number">0</span>]),(lmd/len(X))*theta_reg])</span><br><span class="line">    <span class="keyword">return</span> first+reg</span><br></pre></td></tr></table></figure><h4 id="one-vs-all"><a href="#one-vs-all" class="headerlink" title="one vs all"></a>one vs all</h4><p>每次循环都求出0~9中任一数字的$\theta$值（将该数字和其他数字二分类），最后拼接成一个矩阵。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">one_vs_all</span><span class="params">(X,y,lmd,K)</span>:</span></span><br><span class="line">    all_theta=np.zeros((K,X.shape[<span class="number">1</span>]))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,K+<span class="number">1</span>):</span><br><span class="line">        theta=np.zeros(X.shape[<span class="number">1</span>])</span><br><span class="line">        y_i=np.array([<span class="number">1</span> <span class="keyword">if</span> label==i <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">for</span> label <span class="keyword">in</span> y])</span><br><span class="line">    </span><br><span class="line">        ret=minimize(fun=cost_reg,x0=theta,args=(X,y_i,lmd),method=<span class="string">'TNC'</span>,</span><br><span class="line">                        jac=gradient_reg,options=&#123;<span class="string">'disp'</span>:<span class="keyword">True</span>&#125;)</span><br><span class="line">        </span><br><span class="line">        all_theta[i<span class="number">-1</span>,:]=ret.x</span><br><span class="line">   </span><br><span class="line">    <span class="keyword">return</span> all_theta</span><br></pre></td></tr></table></figure><h4 id="计算精确度"><a href="#计算精确度" class="headerlink" title="计算精确度"></a>计算精确度</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">计算精确度</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">返回值</span></span><br><span class="line"><span class="string">h_argmax是一个存放预测数字的一维ndarray</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict_all</span><span class="params">(X, all_theta)</span>:</span></span><br><span class="line">    h = Sigmoid.sigmoid(X @ all_theta.T)</span><br><span class="line">    <span class="comment"># 返回指定方向上的最大值的索引 axis=0:按列索引，axis=1：按行索引</span></span><br><span class="line">    h_argmax = np.argmax(h, axis=<span class="number">1</span>)</span><br><span class="line">    h_argmax = h_argmax + <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> h_argmax</span><br></pre></td></tr></table></figure><h4 id="main函数"><a href="#main函数" class="headerlink" title="main函数"></a>main函数</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line">    X, y = load_data(<span class="string">'ex3data1.mat'</span>)</span><br><span class="line">    plot_an_image(X,y)</span><br><span class="line">    X = np.insert(X, <span class="number">0</span>, <span class="number">1</span>, axis=<span class="number">1</span>)</span><br><span class="line">    y = y.flatten()</span><br><span class="line">    all_theta = one_vs_all(X, y, <span class="number">1</span>, <span class="number">10</span>)</span><br><span class="line">    y_pred = predict_all(X, all_theta)</span><br><span class="line">    accuracy = np.mean(y_pred == y)  <span class="comment"># 精确度94.46%</span></span><br></pre></td></tr></table></figure><h2 id="神经网络"><a href="#神经网络" class="headerlink" title="神经网络"></a>神经网络</h2><h3 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h3><p>模仿人类大脑的神经元：</p><p><img src="https://s2.ax1x.com/2019/04/06/AW01J0.md.png" alt="AW01J0.md.png"></p><p>进一步设计出神经网络：</p><p><img src="https://s2.ax1x.com/2019/04/06/AW0JQU.md.png" alt="AW0JQU.md.png"></p><h3 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h3><p>这个实验是神经网络的正向传播过程，不涉及如何训练。</p><h4 id="导入模块"><a href="#导入模块" class="headerlink" title="导入模块"></a>导入模块</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scipy.io <span class="keyword">import</span> loadmat</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> Sigmoid</span><br><span class="line"><span class="keyword">import</span> multiClassClassification <span class="keyword">as</span> mc</span><br></pre></td></tr></table></figure><h4 id="导入数据"><a href="#导入数据" class="headerlink" title="导入数据"></a>导入数据</h4><p>这里的权重已经给出，导入即可</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_weigth</span><span class="params">(path)</span>:</span></span><br><span class="line">    data=loadmat(path)</span><br><span class="line">    <span class="comment"># Theta1是输入层和隐藏层之间的参数；Theta2是隐藏层和输出层之间的参数</span></span><br><span class="line">    <span class="keyword">return</span> data[<span class="string">'Theta1'</span>],data[<span class="string">'Theta2'</span>]</span><br></pre></td></tr></table></figure><h4 id="main函数-1"><a href="#main函数-1" class="headerlink" title="main函数"></a>main函数</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line">    theta1, theta2 = load_weigth(<span class="string">'ex3weights.mat'</span>)</span><br><span class="line">    X, y = mc.load_data(<span class="string">'ex3data1.mat'</span>)</span><br><span class="line">    y = y.flatten()</span><br><span class="line">    X = np.insert(X, <span class="number">0</span>, values=np.ones(X.shape[<span class="number">0</span>]), axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 输入层到隐藏层</span></span><br><span class="line">    z2 = X @ theta1.T</span><br><span class="line">    z2 = np.insert(z2, <span class="number">0</span>, <span class="number">1</span>, axis=<span class="number">1</span>)</span><br><span class="line">    <span class="comment"># 隐藏层到输出层</span></span><br><span class="line">    a2 = Sigmoid.sigmoid(z2)</span><br><span class="line">    </span><br><span class="line">    z3 = a2 @ theta2.T</span><br><span class="line">    a3 = Sigmoid.sigmoid(z3)</span><br><span class="line"></span><br><span class="line">    y_pred = np.argmax(a3, axis=<span class="number">1</span>) + <span class="number">1</span></span><br><span class="line">    accurcy = np.mean(y_pred == y)  <span class="comment"># 精确度97.52%</span></span><br></pre></td></tr></table></figure><h3 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h3><p><a href="https://nullblog.top/2019/03/23/%E6%AD%A3%E5%90%91%E4%BC%A0%E6%92%AD%E7%9A%84%E5%90%91%E9%87%8F%E5%8C%96%E5%AE%9E%E7%8E%B0/#more" target="_blank" rel="noopener">正向传播的向量化实现</a></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;多类分类问题&quot;&gt;&lt;a href=&quot;#多类分类问题&quot; class=&quot;headerlink&quot; title=&quot;多类分类问题&quot;&gt;&lt;/a&gt;多类分类问题&lt;/h2&gt;&lt;p&gt;就是把多个类别细分为多个0-1类别来分析&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://s2.ax1x.com/2019/04/06/AW3v7T.png&quot; alt=&quot;AW3v7T.png&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="Machine Learning" scheme="http://yoursite.com/tags/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>机器学习|吴恩达机器学习之线性回归</title>
    <link href="http://yoursite.com/2019/04/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/"/>
    <id>http://yoursite.com/2019/04/06/机器学习-吴恩达机器学习之线性回归/</id>
    <published>2019-04-06T07:31:33.000Z</published>
    <updated>2019-04-06T07:31:33.892Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>对《机器学习》这门课程的回顾，系列文章的目的是希望能够把<strong>原理</strong>和<strong>代码</strong>实现统一起来，增进理解，所以对一些我认为简单的知识，可能会一笔带过。</p></blockquote><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>这里对第一周的内容做一些简单的回顾：</p><p><img src="https://s2.ax1x.com/2019/03/29/A0dT2V.png" alt=""></p><a id="more"></a><h2 id="线性回归-单变量-one-variable"><a href="#线性回归-单变量-one-variable" class="headerlink" title="线性回归-单变量(one variable)"></a>线性回归-单变量(one variable)</h2><h3 id="基本内容"><a href="#基本内容" class="headerlink" title="基本内容"></a>基本内容</h3><h5 id="假设函数"><a href="#假设函数" class="headerlink" title="假设函数"></a>假设函数</h5><p>（1）公式：$h_{\theta}(x)=\theta_{0}+\theta_{1}x​$</p><p>（2）原理：输入一个单变量x，输出预测值</p><h5 id="代价函数"><a href="#代价函数" class="headerlink" title="代价函数"></a>代价函数</h5><p>（1）公式：$J(\theta_{0},\theta_{1})=\frac{1}{2m}\sum_{i=1}^{m}(h_{\theta}(x_{i})-y_{i})^{2}$，又称“平方误差函数”，这里的1/2是方便后面求导时约掉平方。</p><p>（2）原理：最小二乘法判断误差。</p><h5 id="梯度下降法"><a href="#梯度下降法" class="headerlink" title="梯度下降法"></a>梯度下降法</h5><p>（1）公式：</p><p><img src="https://s2.ax1x.com/2019/03/29/A0BCHe.png" alt=""></p><p>重复直到收敛。</p><h3 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h3><p><strong>在实际代码中，数据都是用矩阵的方式来表示。</strong></p><p>1.导入几个常用库</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt  <span class="comment">#画图函数</span></span><br><span class="line"><span class="keyword">from</span> mpl_toolkits.mplot3d <span class="keyword">import</span> Axes3D</span><br><span class="line"><span class="keyword">from</span> matplotlib.colors <span class="keyword">import</span> LogNorm</span><br></pre></td></tr></table></figure><h4 id="数据处理"><a href="#数据处理" class="headerlink" title="数据处理"></a>数据处理</h4><p>1.导入<code>csv</code>文件，用<code>DataFrame</code>结构存储。</p><p>（1）关于<code>csv</code>文件：简单来说就是纯文本，以行为一条记录，每条记录被分隔符分隔。<a href="https://baike.baidu.com/item/CSV/10739" target="_blank" rel="noopener">CSV文件</a></p><p>（2）pandas中的<code>DataFrame</code>：<a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html" target="_blank" rel="noopener">官方手册</a></p><p>（3）关于<code>.describe()</code>返回值中的分位数：<a href="https://nullblog.top/2019/03/12/%E6%B5%85%E8%B0%88%E5%88%86%E4%BD%8D%E6%95%B0/" target="_blank" rel="noopener">浅谈分为数</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'''导入数据'''</span></span><br><span class="line">path = <span class="string">'Data\ex1data1.txt'</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">参数：</span></span><br><span class="line"><span class="string">path:路径</span></span><br><span class="line"><span class="string">header:列名，等于None，是因为接下去列名会显示传递</span></span><br><span class="line"><span class="string">names:需要传递的列名</span></span><br><span class="line"><span class="string">返回值：</span></span><br><span class="line"><span class="string">DataFrame形式数据结构</span></span><br><span class="line"><span class="string">'''</span> </span><br><span class="line">data = pd.read_csv(</span><br><span class="line">    path, header=<span class="keyword">None</span>,</span><br><span class="line">    names=[<span class="string">'Population'</span>,</span><br><span class="line">           <span class="string">'Profit'</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 导入数据以后可视化观察一下</span></span><br><span class="line">data.head() <span class="comment"># 查看记录，默认返回前5条</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看数据集的描述信息 </span></span><br><span class="line"><span class="comment"># count:样本个数  mean：均值  std:标准差  min:最小值  25%:四分之一位数  50%:中位数  75%:四分之# 三位数  max:最大值</span></span><br><span class="line">data.describe()</span><br></pre></td></tr></table></figure><p>2.对得到的数据进行加工，方便计算</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">从样本集中分离出X和y</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"></span><br><span class="line">data.insert(<span class="number">0</span>, <span class="string">'one'</span>, <span class="number">1</span>)<span class="comment"># 插入第1列：X0=1</span></span><br><span class="line"></span><br><span class="line">cols = data.shape[<span class="number">1</span>]  <span class="comment"># .shape返回一个元组，[0]为行数，[1]为列数</span></span><br><span class="line"><span class="comment"># 提取X，y的值</span></span><br><span class="line">X = data.iloc[:, <span class="number">0</span>:cols - <span class="number">1</span>]</span><br><span class="line">y = data.iloc[:, cols - <span class="number">1</span>:cols]</span><br></pre></td></tr></table></figure><p>初始化数据矩阵，这里转化成了<code>np.matrix</code>，但建议使用<code>np.ndarray</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">数据处理</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="comment"># 将dataframe结构转化成np的matrix</span></span><br><span class="line"><span class="comment"># 当Theta取0时计算平均误差</span></span><br><span class="line">X = np.matrix(X.values)</span><br><span class="line">y = np.matrix(y.values)</span><br><span class="line">theta = np.matrix([<span class="number">0</span>, <span class="number">0</span>])</span><br><span class="line"><span class="comment"># 初始化学习速率和迭代次数</span></span><br><span class="line">alpha = <span class="number">0.01</span></span><br><span class="line">epoch = <span class="number">1000</span></span><br></pre></td></tr></table></figure><h4 id="计算函数"><a href="#计算函数" class="headerlink" title="计算函数"></a>计算函数</h4><p>1.代价函数</p><p>公式：$J(\theta_{0},\theta_{1})=\frac{1}{2m}\sum_{i=1}^{m}(h_{\theta}(x_{i})-y_{i})^{2}$，其中主要矩阵化：假设函数$h_{\theta}(x_{i})=\theta^{T}X$和$y_{(i)}$</p><p>（1）向量化：计算过程中注意矩阵乘法或是向量乘法的<strong>合法性</strong>。</p><p>（2）数据可以用<code>np.matrix</code>存储，但更建议使用<code>np.ndarray</code></p><p>（3）<code>np.matrix</code>和<code>np.ndarray</code>在乘法上有所区别:<a href="https://nullblog.top/2019/03/19/Numpy%E4%B8%AD%E7%9A%84%E7%9F%A9%E9%98%B5%E4%B9%98%E6%B3%95/" target="_blank" rel="noopener">Numpy中的矩阵乘法</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">函数名：</span></span><br><span class="line"><span class="string">代价函数</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">参数：</span></span><br><span class="line"><span class="string">X：矩阵 </span></span><br><span class="line"><span class="string">-可用.shape查看维度(97,2)</span></span><br><span class="line"><span class="string">y：向量，用numpy.matrix的结构存储</span></span><br><span class="line"><span class="string">    -可用.shape查看维度(97,1)</span></span><br><span class="line"><span class="string">theta:向量，np.matrix的结构存储</span></span><br><span class="line"><span class="string">-维度(1,2)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">返回值：</span></span><br><span class="line"><span class="string">代价函数值</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">costFunciton</span><span class="params">(X, y, theta)</span>:</span></span><br><span class="line">    inner = np.power(((X * theta.T) - y), <span class="number">2</span>)</span><br><span class="line">    <span class="keyword">return</span> np.sum(inner) / (<span class="number">2</span> * len(X))</span><br></pre></td></tr></table></figure><p>2.批量梯度下降：</p><p>（1）公式：</p><p><img src="https://s2.ax1x.com/2019/03/29/A0BCHe.png" alt=""></p><p>（2）作用：通过迭代的方式来寻找代价函数最小时的参数（$\theta_{j}$）</p><p>（3）学习速率：如果过大，会导致无法到达代价值最小点（函数发散或震荡）；如果过小，则会使得迭代时间过长。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">函数名：</span></span><br><span class="line"><span class="string">梯度下降法</span></span><br><span class="line"><span class="string">参数：</span></span><br><span class="line"><span class="string">X:矩阵</span></span><br><span class="line"><span class="string">y:向量</span></span><br><span class="line"><span class="string">theta:向量</span></span><br><span class="line"><span class="string">alpha:学习速率</span></span><br><span class="line"><span class="string">epoch：迭代次数</span></span><br><span class="line"><span class="string">返回值：</span></span><br><span class="line"><span class="string">theta:最后得到的两个参数theta_0,theta_1</span></span><br><span class="line"><span class="string">cost:最后得到的误差</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradientDescent</span><span class="params">(X, y, theta, alpha, epoch)</span>:</span></span><br><span class="line"></span><br><span class="line">    temp = np.matrix(np.zeros(theta.shape))</span><br><span class="line"></span><br><span class="line">    cost = np.zeros(epoch)  <span class="comment"># epoch为迭代次数</span></span><br><span class="line">    m = X.shape[<span class="number">0</span>]  <span class="comment"># 样本数量</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(epoch):</span><br><span class="line">        temp = theta - (alpha / m) * (X.dot(theta.T) - y).T.dot(X)</span><br><span class="line">        theta = temp</span><br><span class="line">        <span class="comment"># 记录一下每次更新后的误差</span></span><br><span class="line">        cost[i] = costFunciton(X, y, theta)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> theta, cost</span><br></pre></td></tr></table></figure><p>2*.正规方程法(Normal Equation)</p><p>（1）同样也可以用于寻找代价函数最小时候的参数取值，与梯度下降法(Gradient Descent)比较</p><p><img src="https://s2.ax1x.com/2019/04/02/Aydhpd.md.jpg" alt="Aydhpd.md.jpg"></p><p>（2）公式：$\theta=(X^{T}X)^{-1}X^{T}y$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'''特征方程'''</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">norEquation</span><span class="params">(X,y)</span>:</span></span><br><span class="line">    theta=np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> theta</span><br></pre></td></tr></table></figure><h4 id="画图函数"><a href="#画图函数" class="headerlink" title="画图函数"></a>画图函数</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">x = np.linspace(</span><br><span class="line">    data.Population.min(), data.Population.max(),</span><br><span class="line">    <span class="number">100</span>)  <span class="comment"># 横坐标:linspace(start,end, num)从start开始到end结束，平均分成num份，返回一个数组</span></span><br><span class="line">f = final_theta[<span class="number">0</span>, <span class="number">0</span>] + (final_theta[<span class="number">0</span>, <span class="number">1</span>] * x)  <span class="comment"># 假设函数</span></span><br><span class="line"></span><br><span class="line"><span class="string">'''函数和散点图'''</span></span><br><span class="line">plt.figure(figsize=(<span class="number">8</span>, <span class="number">5</span>))</span><br><span class="line">plt.plot(x, f, <span class="string">'r'</span>, label=<span class="string">'Prediction'</span>)</span><br><span class="line">plt.scatter(data[<span class="string">'Population'</span>], data.Profit, label=<span class="string">'Training Data'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'Population'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Profit'</span>)</span><br><span class="line">plt.title(<span class="string">'Predicted Profit vs. Population Size'</span>)</span><br></pre></td></tr></table></figure><p>结果图：</p><p><img src="https://s2.ax1x.com/2019/04/02/AydTnP.md.png" alt="AydTnP.md.png"></p><p>可以看出拟合效果还可以。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">绘制代价函数与迭代次数的图像</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line">plt.figure(figsize=(<span class="number">8</span>, <span class="number">5</span>))</span><br><span class="line">plt.plot(np.arange(epoch), cost, <span class="string">'r'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'iteration'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'cost'</span>)</span><br></pre></td></tr></table></figure><p>结果图：</p><p><img src="https://s2.ax1x.com/2019/04/02/AydqAS.md.png" alt="AydqAS.md.png"></p><p>随着迭代次数的增加，代价函数值单调递减。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">绘制代价函数3D图像</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line">fig = plt.figure(figsize=(<span class="number">8</span>, <span class="number">5</span>))</span><br><span class="line">ax = Axes3D(fig)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制网格</span></span><br><span class="line"><span class="comment"># X,Y value</span></span><br><span class="line">theta0 = np.linspace(<span class="number">-10</span>, <span class="number">10</span>, <span class="number">100</span>)  <span class="comment"># 网格theta0范围</span></span><br><span class="line">theta1 = np.linspace(<span class="number">-1</span>, <span class="number">4</span>, <span class="number">100</span>)  <span class="comment"># 网格theta1范围</span></span><br><span class="line">x1, y1 = np.meshgrid(theta0, theta1)  <span class="comment"># 画网格</span></span><br><span class="line"><span class="comment"># height value</span></span><br><span class="line">z = np.zeros(x1.shape)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, theta0.size):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">0</span>, theta1.size):</span><br><span class="line">        t = np.matrix([theta0[i], theta1[j]])</span><br><span class="line">        z[i][j] = costFunciton(X, y, t)</span><br><span class="line"><span class="comment"># 由循环可以看出，这里是先取x=-10时，y的所有取值，然后计算代价函数传入z的第一行</span></span><br><span class="line"><span class="comment"># 因此在绘图过程中，需要把行和列转置过来</span></span><br><span class="line">z = z.T</span><br><span class="line">ax.set_xlabel(<span class="string">r'$\theta_0$'</span>)</span><br><span class="line">ax.set_ylabel(<span class="string">r'$\theta_1$'</span>)</span><br><span class="line"><span class="comment"># 绘制函数图像</span></span><br><span class="line">ax.plot_surface(x1, y1, z, rstride=<span class="number">1</span>, cstride=<span class="number">1</span>)</span><br></pre></td></tr></table></figure><p>结果图：</p><p><img src="https://s2.ax1x.com/2019/04/02/AydLtg.md.png" alt="AydLtg.md.png"></p><p>关于3D图中的<a href="https://nullblog.top/2019/03/16/Numpy%E4%B8%AD%E7%9A%84Meshgrid/" target="_blank" rel="noopener">Numpy中的Meshgrid</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">绘制等高线图</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line">plt.figure(figsize=(<span class="number">8</span>, <span class="number">5</span>))</span><br><span class="line">lvls = np.logspace(<span class="number">-2</span>, <span class="number">3</span>, <span class="number">20</span>)</span><br><span class="line">plt.contour(x1, y1, z, levels=lvls, norm=LogNorm())  <span class="comment"># 画出等高线</span></span><br><span class="line">plt.plot(final_theta[<span class="number">0</span>, <span class="number">0</span>], final_theta[<span class="number">0</span>, <span class="number">1</span>], <span class="string">'r'</span>, marker=<span class="string">'x'</span>)  <span class="comment"># 标出代价函数最小值点</span></span><br></pre></td></tr></table></figure><p>结果图：</p><p><img src="https://s2.ax1x.com/2019/04/02/AydOhQ.md.png" alt="AydOhQ.md.png"></p><h2 id="线性回归-多变量-multiple-variables"><a href="#线性回归-多变量-multiple-variables" class="headerlink" title="线性回归-多变量(multiple variables)"></a>线性回归-多变量(multiple variables)</h2><h3 id="特征缩放-Feature-scaling"><a href="#特征缩放-Feature-scaling" class="headerlink" title="特征缩放(Feature scaling)"></a>特征缩放(Feature scaling)</h3><p>也叫均值归一化</p><p>（1）公式：$x=\frac{x-\mu}{s_{1}}$，其中$\mu$为$x$的均值，$s_{1}$为$x$的最大值减去最小值，或者使用标准差。</p><p>（2）作用：在多个特征值情况下，如果某个特征值$x_{i}$的取值范围和另一个特征值$x_{j}$的取值范围相差太大，会减慢梯度下降的速度，因此需要用特征缩放，将不同特征值的取值限定在差不多的范围内。</p><p>e.g. $x_{1}$为房子的面积，取值范围0~2000；$x_{2}$为卧室的数量，取值0-5；那么对二者使用特征缩放，可得：</p><p><img src="https://s2.ax1x.com/2019/03/31/ADOqIS.md.png" alt="ADOqIS.md.png"></p><p>（3）代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">data = (data - data.mean()) / data.std()  </span><br><span class="line"><span class="comment"># 除数可以用标准差也可以用max-min，因为pandas方便，所以使用标准差</span></span><br></pre></td></tr></table></figure><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p>[1] <a href="https://blog.csdn.net/sdu_hao/article/details/82973212" target="_blank" rel="noopener">机器学习 | 吴恩达机器学习第一周学习笔记</a></p><p>[2] <a href="https://blog.csdn.net/sdu_hao/article/details/83932480" target="_blank" rel="noopener">机器学习 | 吴恩达机器学习第二周编程作业(Python版）</a></p><p>[3] <a href="https://blog.csdn.net/Cowry5/article/details/80174130" target="_blank" rel="noopener">吴恩达机器学习作业Python实现(一)：线性回归</a></p><h2 id="代码-1"><a href="#代码-1" class="headerlink" title="代码"></a>代码</h2><p><a href="https://github.com/scp-1024/Coursera-ML-Ng/tree/master/Exercise%201-Linear%20Regression" target="_blank" rel="noopener">scp-1024/Coursera-ML-Ng</a></p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;对《机器学习》这门课程的回顾，系列文章的目的是希望能够把&lt;strong&gt;原理&lt;/strong&gt;和&lt;strong&gt;代码&lt;/strong&gt;实现统一起来，增进理解，所以对一些我认为简单的知识，可能会一笔带过。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;这里对第一周的内容做一些简单的回顾：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://s2.ax1x.com/2019/03/29/A0dT2V.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="Machine Learning" scheme="http://yoursite.com/tags/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>机器学习|吴恩达机器学习之逻辑回归</title>
    <link href="http://yoursite.com/2019/04/03/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/"/>
    <id>http://yoursite.com/2019/04/03/机器学习-吴恩达机器学习之逻辑回归/</id>
    <published>2019-04-03T04:27:42.000Z</published>
    <updated>2019-04-06T08:46:45.062Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>一个名为回归确用于解决分类的算法，吴恩达coursera第三周</p></blockquote><a id="more"></a><h2 id="逻辑回归-Logic-Regression"><a href="#逻辑回归-Logic-Regression" class="headerlink" title="逻辑回归(Logic Regression)"></a>逻辑回归(Logic Regression)</h2><h3 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h3><h4 id="数据处理"><a href="#数据处理" class="headerlink" title="数据处理"></a>数据处理</h4><p>（1）常规读取数据的方法，值得注意的是<code>X = data.iloc[:, :-1].values</code>中<code>.values</code>作用是将<code>DataFrame</code>转化为<code>ndarray</code>。根据手册，更推荐使用<code>.to_numpy()</code>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">path = <span class="string">'ex2data1.txt'</span></span><br><span class="line">data = pd.read_csv(path, names=(<span class="string">'exam1'</span>, <span class="string">'exam2'</span>, <span class="string">'admitted'</span>))</span><br><span class="line">data_copy = pd.read_csv(path, names=(<span class="string">'exam1'</span>, <span class="string">'exam2'</span>, <span class="string">'admitted'</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 进一步准备数据，对结构初始化</span></span><br><span class="line">data.insert(<span class="number">0</span>, <span class="string">'One'</span>, <span class="number">1</span>)</span><br><span class="line">X = data.iloc[:, :<span class="number">-1</span>].values </span><br><span class="line">y = data.iloc[:, <span class="number">-1</span>].values</span><br><span class="line">theta = np.zeros(</span><br><span class="line">    X.shape[<span class="number">1</span>])  <span class="comment"># 注意这里theta创建的是一维的数组，对于ndarray一定要注意一维时它的shape（和matrix有很大区别）</span></span><br></pre></td></tr></table></figure><h4 id="数据可视化"><a href="#数据可视化" class="headerlink" title="数据可视化"></a>数据可视化</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">数据可视化</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_data</span><span class="params">(data)</span>:</span></span><br><span class="line">    cols = data.shape[<span class="number">1</span>]</span><br><span class="line">    feature = data.iloc[:, <span class="number">0</span>:cols - <span class="number">1</span>]</span><br><span class="line">    label = data.iloc[:, cols - <span class="number">1</span>]</span><br><span class="line">    <span class="comment"># iloc 根据列的位置索引来切片</span></span><br><span class="line">    postive = feature[label == <span class="number">1</span>]</span><br><span class="line">    negtive = feature[label == <span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># plt.figure(figsize=(8, 5))</span></span><br><span class="line">    plt.scatter(postive.iloc[:, <span class="number">0</span>], postive.iloc[:, <span class="number">1</span>])</span><br><span class="line">    plt.scatter(negtive.iloc[:, <span class="number">0</span>], negtive.iloc[:, <span class="number">1</span>], c=<span class="string">'r'</span>, marker=<span class="string">'x'</span>)</span><br><span class="line">    plt.legend([<span class="string">'Admitted'</span>, <span class="string">'Not admitted'</span>], loc=<span class="number">1</span>)</span><br><span class="line">    plt.xlabel(<span class="string">'Exam1 score'</span>)</span><br><span class="line">    plt.ylabel(<span class="string">'Exam2 score'</span>)</span><br></pre></td></tr></table></figure><h4 id="假设函数"><a href="#假设函数" class="headerlink" title="假设函数"></a>假设函数</h4><h5 id="假设函数："><a href="#假设函数：" class="headerlink" title="假设函数："></a>假设函数：</h5><p>（1）公式：$h_{\theta}(x)=\frac{1}{1+e^{-\theta^{T}x}}$，又名Sigmoid function，函数图像如下图所示，从图中可以看出$h_{\theta}$的取值范围0~1</p><p><img src="https://s2.ax1x.com/2019/04/03/Ac02QJ.png" alt=""></p><p>（2）$h_{\theta}$的含义是：<strong>特征x的情况下，y=1的概率</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span><span class="params">(z)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span> / (<span class="number">1</span> + np.exp(-z))</span><br></pre></td></tr></table></figure><h4 id="代价函数："><a href="#代价函数：" class="headerlink" title="代价函数："></a>代价函数：</h4><p>如果这时再使用线性回归的$J(\theta_{0},\theta_{1})=\frac{1}{2m}\sum_{i=1}^{m}(h_{\theta}(x_{i})-y_{i})^{2}$将会导致函数图像不光滑。这样使用最小值法时容易导致无法找到全局最优解。因此需要使用新的代价函数。</p><p><img src="https://s2.ax1x.com/2019/04/03/AcBilQ.png" alt="AcBilQ.png"></p><p>（1）公式：<img src="https://s2.ax1x.com/2019/04/03/Ac0jeI.png" alt="Ac0jeI.png"></p><p>具体解释如下：</p><p><img src="https://s2.ax1x.com/2019/04/03/AcBVwq.md.png" alt="AcBVwq.md.png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">cost function可以用矩阵实现也可以用ndarray实现，更建议使用后者</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cost</span><span class="params">(theta, X, y)</span>:</span></span><br><span class="line">    first = (-y) * np.log(Sigmoid.sigmoid(X @ theta))  <span class="comment"># 这里*号是对应位置相乘而不是矩阵运算</span></span><br><span class="line">    second = (<span class="number">1</span> - y) * np.log(<span class="number">1</span> - Sigmoid.sigmoid(X @ theta))</span><br><span class="line">    <span class="keyword">return</span> np.mean(first - second)</span><br></pre></td></tr></table></figure><h4 id="高级优化法："><a href="#高级优化法：" class="headerlink" title="高级优化法："></a>高级优化法：</h4><p>除了梯度下降法之外，还有其他几种优化方法，比起gradient descent，这些方法更适合处理大型数据且不需要你设置学习速率。</p><p><img src="https://s2.ax1x.com/2019/04/03/AcBl6J.md.png" alt="AcBl6J.md.png"></p><p>这些方法的具体原理非常复杂，但是，python的模块往往是非常强大的，因此往往只需要你计算一下到导数项即可</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradient</span><span class="params">(theta, X, y)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> (<span class="number">1</span> / len(X)) * (X.T @ (Sigmoid.sigmoid(X @ theta) - y))</span><br></pre></td></tr></table></figure><p>再使用<code>import scipy.optimize as opt</code>中的<code>.fmin_tnc</code>来迭代,得到最终的$\theta$值。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 这里不使用梯度下降法，换成其他优化算法来迭代</span></span><br><span class="line">result = opt.fmin_tnc(func=cost, x0=theta, fprime=gradient, args=(X, y))</span><br><span class="line">final_theta = result[<span class="number">0</span>]</span><br></pre></td></tr></table></figure><h4 id="检测准确率"><a href="#检测准确率" class="headerlink" title="检测准确率"></a>检测准确率</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">预测标签</span></span><br><span class="line"><span class="string">参数：</span></span><br><span class="line"><span class="string">-参数：theta</span></span><br><span class="line"><span class="string">-样本：X</span></span><br><span class="line"><span class="string">返回值：</span></span><br><span class="line"><span class="string">-预测标签</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(theta, X)</span>:</span></span><br><span class="line">    probability = Sigmoid.sigmoid(X @ theta)</span><br><span class="line">    <span class="keyword">return</span> [<span class="number">1</span> <span class="keyword">if</span> x &gt;= <span class="number">0.5</span> <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">for</span> x <span class="keyword">in</span> probability]</span><br></pre></td></tr></table></figure><p>将得到的预测标签同数据原有的标签进行比对，得到准确率</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">predictions = predict(final_theta, X)</span><br><span class="line">correct = [<span class="number">1</span> <span class="keyword">if</span> a == b <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">for</span> (a, b) <span class="keyword">in</span> zip(predictions, y)]</span><br><span class="line">accurcy = np.sum(correct) / len(X)  <span class="comment"># 准确率89%</span></span><br></pre></td></tr></table></figure><p>也可以自定义一个样本预测一发</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 输入一个数据进行预测</span></span><br><span class="line">test = np.array([<span class="number">1</span>, <span class="number">45</span>, <span class="number">85</span>]).reshape(<span class="number">1</span>, <span class="number">3</span>)</span><br><span class="line">predict_result = predict(final_theta, test)  <span class="comment"># 预测值y=1，概率为0.776</span></span><br></pre></td></tr></table></figure><h4 id="决策边界"><a href="#决策边界" class="headerlink" title="决策边界"></a>决策边界</h4><p>$h_{\theta}=0.5$时的直线，由假设函数图像可以看出，当$h_{\theta}=0.5$时，$z=\theta^{T}x=0$，这里图中是以$x2$作为纵坐标。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 决策边界</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_decision_boundary</span><span class="params">(theta, X)</span>:</span></span><br><span class="line">    plot_x = np.linspace(<span class="number">20</span>, <span class="number">110</span>)</span><br><span class="line">    plot_y = -(theta[<span class="number">0</span>] + plot_x * theta[<span class="number">1</span>]) / theta[<span class="number">2</span>]</span><br><span class="line">    plt.plot(plot_x, plot_y, c=<span class="string">'y'</span>)</span><br></pre></td></tr></table></figure><h4 id="画图函数"><a href="#画图函数" class="headerlink" title="画图函数"></a>画图函数</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">plt.figure(figsize=(<span class="number">8</span>, <span class="number">5</span>))</span><br><span class="line">plot_data(data_copy)</span><br><span class="line">plot_decision_boundary(final_theta, X)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>结果图为：</p><p><img src="https://s2.ax1x.com/2019/04/03/AcBvjJ.md.png" alt="AcBvjJ.md.png"></p><h2 id="带正则化项的逻辑回归函数"><a href="#带正则化项的逻辑回归函数" class="headerlink" title="带正则化项的逻辑回归函数"></a>带正则化项的逻辑回归函数</h2><h4 id="读入数据："><a href="#读入数据：" class="headerlink" title="读入数据："></a>读入数据：</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">path = <span class="string">'ex2data2.txt'</span></span><br><span class="line">data = pd.read_csv(</span><br><span class="line">    path, names=(<span class="string">'Microchip Test1'</span>, <span class="string">'Microchip Test2'</span>, <span class="string">'Accept'</span>))</span><br><span class="line">x1 = data.iloc[:, <span class="number">0</span>].values</span><br><span class="line">x2 = data.iloc[:, <span class="number">1</span>].values</span><br></pre></td></tr></table></figure><h4 id="可视化数据"><a href="#可视化数据" class="headerlink" title="可视化数据"></a>可视化数据</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_data</span><span class="params">(data)</span>:</span></span><br><span class="line">    feature = data.iloc[:, <span class="number">0</span>:<span class="number">2</span>]</span><br><span class="line">    label = data.iloc[:, <span class="number">2</span>]</span><br><span class="line">    positive = feature[label == <span class="number">1</span>]</span><br><span class="line">    negative = feature[label == <span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    plt.scatter(positive.iloc[:, <span class="number">0</span>].values, positive.iloc[:, <span class="number">1</span>].values)</span><br><span class="line">    plt.scatter(</span><br><span class="line">        negative.iloc[:, <span class="number">0</span>].values,</span><br><span class="line">        negative.iloc[:, <span class="number">1</span>].values,</span><br><span class="line">        c=<span class="string">'r'</span>,</span><br><span class="line">        marker=<span class="string">'x'</span>)</span><br></pre></td></tr></table></figure><h4 id="特征映射"><a href="#特征映射" class="headerlink" title="特征映射"></a>特征映射</h4><p>由可视化数据可知，如果单独用两个特征，是无法表示出决策边界的（欠拟合underfit）。因此需要映射多个特征。</p><p><img src="https://s2.ax1x.com/2019/04/03/AcDVjH.png" alt="AcDVjH.png"></p><p>这里将$x1$和$x2$映射为6个特征值</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">map_feature</span><span class="params">(x1, x2)</span>:</span></span><br><span class="line">    degree = <span class="number">6</span></span><br><span class="line"></span><br><span class="line">    x1 = x1.reshape((x1.size, <span class="number">1</span>))  <span class="comment"># ndarray.size：数组中元素的个数</span></span><br><span class="line">    x2 = x2.reshape((x2.size, <span class="number">1</span>))</span><br><span class="line">    result = np.ones(x1.shape[<span class="number">0</span>])  <span class="comment"># 初始化一个值为1的数组(列向量)</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, degree + <span class="number">1</span>):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">0</span>, i + <span class="number">1</span>):</span><br><span class="line">            result = np.c_[result, (x1**(i - j) * (x2**j))]  <span class="comment"># np.c_：列拼接</span></span><br><span class="line">    <span class="keyword">return</span> result  <span class="comment"># 返回值即为特征值X</span></span><br></pre></td></tr></table></figure><p>但这种映射可能导致过拟合（overfit），泛化能力差，因此还需要正则化（regularization）。</p><h4 id="正则化"><a href="#正则化" class="headerlink" title="*正则化"></a>*正则化</h4><p>（1）原理：</p><p>因为过拟合是由于特征项过多引起的，减少特征的数量固然可以，还有一种方法就是正则化：<strong>减小$\theta_{j}​$的值。</strong></p><p><img src="https://s2.ax1x.com/2019/04/03/AcDRV1.md.png" alt="AcDRV1.md.png"></p><p>由图中可以看出，线性回归算法中添加两个正则化项——也叫惩罚项，$1000<em>\theta_{3}$和$1000</em>\theta_{4}$。上图可以看出，在利用优化算法求解参数时，要想让代价函数值变小，$\theta_{3}$和$\theta_{4}$必须非常小，也就导致了$\theta_{3}x^{3}$和$\theta_{4}x^{4}$非常小，那么图中右边的假设函数就近似与左边的函数了。</p><p>实际操作中，我们很多时候并不知道究竟应该惩罚哪一项，所以实际上除了$\theta_{0}​$（全是1），所有项都会惩罚。</p><p>回到逻辑回归算法上也是一样的</p><p>（2）公式：</p><p><img src="https://s2.ax1x.com/2019/04/03/AcD4PK.md.png" alt="AcD4PK.md.png"></p><p>对于线性回归算法也类似：</p><p><img src="https://s2.ax1x.com/2019/04/03/AcD7KH.png" alt="AcD7KH.png"></p><h4 id="代价函数：-1"><a href="#代价函数：-1" class="headerlink" title="代价函数："></a>代价函数：</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cost_reg</span><span class="params">(theta,X, y, lmd)</span>:</span></span><br><span class="line">    <span class="comment"># 不惩罚第一项</span></span><br><span class="line">    _theta = theta[<span class="number">1</span>:]</span><br><span class="line">    reg = (lmd / (<span class="number">2</span> * len(X))) * (_theta @ _theta)</span><br><span class="line"></span><br><span class="line">    first = (y) * np.log(Sigmoid.sigmoid(X @ theta))</span><br><span class="line">    second = (<span class="number">1</span> - y) * np.log(<span class="number">1</span> - Sigmoid.sigmoid(X @ theta))</span><br><span class="line">    final = -np.mean(first + second)</span><br><span class="line">    <span class="keyword">return</span> final + reg</span><br></pre></td></tr></table></figure><h4 id="梯度函数"><a href="#梯度函数" class="headerlink" title="梯度函数"></a>梯度函数</h4><p>（1）公式：<img src="https://s2.ax1x.com/2019/04/06/AWJzAf.md.png" alt="AWJzAf.md.png"></p><p>（2）向量化：$\theta_{j}:=\theta-\alpha[\frac{1}{m} X^{T} (g(X^{T}\theta)-y)+\frac{\lambda}{m}\theta_{j}]$</p><p>代码只需要计算蓝色括号中的内容，然后用<strong>优化算法</strong>迭代：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradient_reg</span><span class="params">(theta,X, y, lmd)</span>:</span></span><br><span class="line">    <span class="comment"># 因为不惩罚第一项，所以要分开计算</span></span><br><span class="line">    grad = (<span class="number">1</span> / len(X)) * (X.T @ (Sigmoid.sigmoid(X @ theta) - y))</span><br><span class="line">    grad[<span class="number">1</span>:] += (lmd / len(X)) * theta[<span class="number">1</span>:]</span><br><span class="line">    <span class="keyword">return</span> grad</span><br></pre></td></tr></table></figure><h4 id="优化算法："><a href="#优化算法：" class="headerlink" title="优化算法："></a>优化算法：</h4><p>用于迭代计算$\theta_{j}$值。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">result = opt.fmin_tnc(</span><br><span class="line">func=cost_reg,</span><br><span class="line">x0=theta,</span><br><span class="line">fprime=gradient_reg,</span><br><span class="line">args=(X, y, <span class="number">1</span>),</span><br><span class="line">)</span><br></pre></td></tr></table></figure><h4 id="画出决策边界"><a href="#画出决策边界" class="headerlink" title="画出决策边界"></a>画出决策边界</h4><p>不是代入假设函数来画！！在逻辑回归中假设函数时Sigmoid function，用于计算概率的！！</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_decision_boundary</span><span class="params">(theta)</span>:</span></span><br><span class="line">    x=np.linspace(<span class="number">-1</span>,<span class="number">1.5</span>,<span class="number">50</span>)</span><br><span class="line">    plot_x,plot_y=np.meshgrid(x,x) <span class="comment"># 先画网格</span></span><br><span class="line"></span><br><span class="line">    z=map_feature(plot_x,plot_y)  </span><br><span class="line">    z=z@theta <span class="comment"># 画出边界</span></span><br><span class="line">    z=z.reshape(plot_x.shape)</span><br><span class="line">    plt.contour(plot_x,plot_y,z,<span class="number">0</span>,colors=<span class="string">'yellow'</span>)</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;一个名为回归确用于解决分类的算法，吴恩达coursera第三周&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
      <category term="Machine Learning" scheme="http://yoursite.com/tags/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>信息熵</title>
    <link href="http://yoursite.com/2019/03/29/%E4%BF%A1%E6%81%AF%E7%86%B5/"/>
    <id>http://yoursite.com/2019/03/29/信息熵/</id>
    <published>2019-03-29T02:53:40.000Z</published>
    <updated>2019-03-29T02:53:40.260Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>我们在生活中常常听到“这句话信息量好大。”、“这都是废话，没什么信息量。“，而我们却很少思考，这里的信息量究竟是什么意思？是否可以给出精确定义，甚至，量化它呢？</p></blockquote><p><img src="https://s2.ax1x.com/2019/03/26/ANfKbT.png" alt=""></p><a id="more"></a><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>1984年香农提出了”信息熵“的概念，解决了这个问题。</p><h2 id="信息中的熵"><a href="#信息中的熵" class="headerlink" title="信息中的熵"></a>信息中的熵</h2><blockquote><p>因为熵不只在信息，还在物理等其他领域有定义，虽然他们本质一样，但表达上和所研究的问题上略有差异，这里特作说明。</p></blockquote><p>可以这么理解：当一件事情（宏观态）具有多种情况时，对于观察者而言，其具体情况（微观态）的<strong>不确定度</strong>，称为熵。</p><p>显然这个定义看上去肯定是懵逼的，没事，用一个例子来解释。</p><p>假设我们在做一道单项选择题，这道选择题的答案具有4个选项。那么对你这个观察者而言，这个选择题正确答案的可能选择情况（四种选择情况：选A、选B、选C、选D）就是宏观态，而其中的每一个选择情况就被称为微观态。我们目前并不能确定谁是正确答案，所以其微观态就具有不确定性。那么此时此刻，”选择正确选项”这件事情对你而言，熵为2bit。</p><p>（1）、而正在你陷入迷茫之际，你的好基友小明来到了你的身边，悄悄地对你说：”有50%可能性选C。“</p><p>这条消息中含有的<strong>信息量</strong>，帮助你调整了每个微观态的概率：从等概率（A：25%，B：25%，C：25%，D：25%）调整为（A：16.6%，B：16.6%，C：50%，D：16.6%），而这条消息中含有的信息量为0.21bit，所以才“使用”了这条消息之后，“选出正确选项”这件事情的熵为$2-0.21=1.79bit$</p><p>（2）、这时候，你突然想起来可以直接看参考答案啊，于是你打开了书的最后一页发现——这题的正确答案选C。同样，这条消息含有的信息量同样也帮你调整了概率，这次更直接的把C的概率改为了100%。那么这条消息的信息量为2bit，因为它帮助你完全消除了“选出正确选项”这件事情的不确定性。</p><p>从上面的例子中可以发现，信息量的作用，就是用来消除熵。</p><p>到这里，你应该对熵的概念有了大体的理解，但还有一个问题，就是信息量是如何计算的？</p><h2 id="信息量是如何计算的"><a href="#信息量是如何计算的" class="headerlink" title="信息量是如何计算的"></a>信息量是如何计算的</h2><p>我们都知道，在定义重量的单位时，我们先是选择了一个<strong>参照物</strong>，比如一块砖头，我们把这块砖头的重量定义为1千克，剩下的所有物体的重量都可以利用这块砖头来表示，比如把我和一堆砖头放在天平上，当达到水平状态时，只要数出砖头个数——65个，那么就可以知道我的重量是65kg。</p><p>基于同样的道理，我们先找一个参照物事件——扔硬币！它只有2种等概率情况，即50%正面50%反面（微观态）。它的熵（不确定性）我们记为1bit，那么再回顾一下你的选择题，在刚开始时，它的等概率情况有4种，所以它的熵就是2bit。</p><p>同样推断，如果一件事情的等概率情况有8种，那么它的熵就是4bit…那是不可能的。</p><p>你先仔细想想，什么事情具有8种等概率情况？对的，扔3枚硬币，所以熵是3bit而不是4bit。原因是，扔硬币结果的个数和硬币之间是<strong>指数关系</strong>而不是线性关系。</p><p>所以我们可以总结：<strong>在各个微观态之间是等概率的情况下，把宏观态的熵记为n，微观态的个数记为m，就可以得到 $n=log_{2}m​$</strong></p><p>但问题又来了，如果各个微观态之间不是等概率的情况呢？</p><p>显然无法直接用上面的公式，这里回到“选择题事件”的（1）中，当我们得知了小明给出的消息之后，熵的bit数为：$n=p_{A}<em>log_{2}(1/p_{A})+p_{B}</em>log_{2}(1/p_{B})+p_{C}<em>log_{2}(1/p_{C})+p_{D}</em>log_{2}(1/p_{D})$ </p><p>其中$p$为每个选项的概率。这里对数中概率取倒数的原因：概率的倒数等于其发生在等概率情况的个数。</p><p>计算后熵 $n=1.79bit$ ，则小明的消息中的信息量就是0.21bit 。</p><p><strong>以上的内容均来自 <a href="https://space.bilibili.com/344849038/video" target="_blank" rel="noopener">B站—YJango</a>中10~11期的内容。</strong></p><h2 id="计算熵的公式"><a href="#计算熵的公式" class="headerlink" title="计算熵的公式"></a>计算熵的公式</h2><p>那么可以做出一个总结，如果一个随机变量A有k中可能取值，其中第i种发生的概率为P(i)，那么信息熵的公式为：<img src="https://s2.ax1x.com/2019/03/26/ANwmT0.png" alt=""></p><p>由此也可见，一个取值的概率越低，他的熵就越高，你确定他所需要的信息量就越大。也可以证明，等概率情况下，信息熵的值最大$^{[1]}$。</p><p>而在等概率（每个情况发生的概率都为P(a)）情况下，我们需要了解一个随机变量所需要的信息量，为$L=log_{2}(1/P(a))$，这种说法由R.V.L.哈莱特与1928年提出$^{[2]}$，早于香农，实际上这里求的也就是熵，只不过那时候没这么叫。（见 信息量和信息熵的区别）</p><p>当时看到这里我有一个巨大的疑问，如果按这个公式计算，那么“选择题”中的（1），小明的消息提供的信息量应该为1bit啊怎么会是0.21bit。</p><p>后来我是这么理解的，如果<strong>单独</strong>把”50%的可能性选C”看成随机变量的话，你作为观察者所面对的情况实际上只有 <strong>2种</strong>：选C或者不选C。那么的确熵为1bit。</p><p>而放回到原来的事件中，情况为<strong>4种</strong>，这条信息只不过使得每个情况的概率都改变了，或者说，仅仅是4种情况之间的权重发生了改变，需要通过期望的方式来计算熵（1.79bit），再相减，得出结果（0.21bit）代表了这条信息在<strong>当前所讨论的宏观态下消除的不确定性。</strong></p><p>更进一步说，其实这里隐藏了一个条件，<strong>我们默认把”50%的可能性选C“这件条信息当成是确定的（熵为0而不是单独情况时的1bit）</strong>，并且在宏观态中根据信息调整了微观态的权重，而这时这条信息对这个宏观态产生了影响，使其熵减了0.21bit。</p><h2 id="信息量和信息熵"><a href="#信息量和信息熵" class="headerlink" title="信息量和信息熵"></a>信息量和信息熵</h2><blockquote><p>信息熵就是上面说的熵，用“信息熵”的叫法是为了和信息量对称</p></blockquote><p>信息熵：描述的是一个事件（宏观态）的不确定度，也就是说，信息熵是宏观态的一个<strong>属性</strong>，并不因为观察者而产生变化，比如你知道了一道选择题的答案，那么这道题的信息熵仅仅对你而言是0bit（也就是说你对这道题已经是确定了的），这时另外有人看到了这道选择题，那么对他而言，这道题的信息熵仍然为2bit。</p><p>信息量：你解开一个不确定事件的过程，在这个过程中你不断的获得<strong>信息量</strong>，来消除信息熵。</p><p>二者的本质上存在差异，但其实是角度不同导致的。</p><blockquote><p>比较好的表述是:</p><p>信息熵是</p><p><strong>对于事件A，我们对A不了解的程度</strong>。</p><p>换句话说，</p><p><strong>就是我们还需要多少信息量才能完全了解事件A</strong></p><p>所需要的信息量就是信息熵。</p><p><strong>而信息熵很大的意思是事件A本身所携带的信息量很大。</strong></p></blockquote><p>引用自<a href="https://www.zhihu.com/question/274997106/answer/383102744" target="_blank" rel="noopener">信息熵越大，信息量到底是越大还是越小？ - 捣衣的回答 - 知乎</a></p><h2 id="熵在物理中的运用"><a href="#熵在物理中的运用" class="headerlink" title="熵在物理中的运用"></a>熵在物理中的运用</h2><p>简单来说，热力学第二定律推导出熵增原理：孤立系统的熵永不减少。进而判断宇宙处在一个不断熵增的状态，因此总有一天会达到“热寂寞”</p><p>有兴趣可参考几个视频</p><p><a href="https://www.bilibili.com/video/av31931321?from=search&amp;seid=9987571277393916947" target="_blank" rel="noopener">热力学第二定律是什么？“麦克斯韦妖”是什么鬼？李永乐老师告诉你</a></p><p><a href="https://www.bilibili.com/video/av25408872/?spm_id_from=333.788.videocard.0" target="_blank" rel="noopener">熵到底是什么？一副牌中抽三张为同花的概率是多大？</a></p><p><a href="https://www.bilibili.com/video/av42589628" target="_blank" rel="noopener">【学习观12】阻碍人类永生的原因是？</a></p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p>[1].<a href="https://www.zhihu.com/question/22178202/answer/125040625" target="_blank" rel="noopener">信息熵是什么？ - 柯伟辰的回答 - 知乎</a></p><p>[2].<a href="https://baike.baidu.com/item/%E4%BF%A1%E6%81%AF%E9%87%8F" target="_blank" rel="noopener">信息量—百度百科</a></p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;我们在生活中常常听到“这句话信息量好大。”、“这都是废话，没什么信息量。“，而我们却很少思考，这里的信息量究竟是什么意思？是否可以给出精确定义，甚至，量化它呢？&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img src=&quot;https://s2.ax1x.com/2019/03/26/ANfKbT.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="Reading" scheme="http://yoursite.com/tags/Reading/"/>
    
  </entry>
  
  <entry>
    <title>YJango学习观笔记及感悟</title>
    <link href="http://yoursite.com/2019/03/25/YJango%E5%AD%A6%E4%B9%A0%E8%A7%82%E7%AC%94%E8%AE%B0%E5%8F%8A%E6%84%9F%E6%82%9F/"/>
    <id>http://yoursite.com/2019/03/25/YJango学习观笔记及感悟/</id>
    <published>2019-03-25T10:11:47.000Z</published>
    <updated>2019-03-29T02:54:03.283Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p><a href="https://space.bilibili.com/344849038/video" target="_blank" rel="noopener">b站-YJango</a>，个人还是对这位先生的</p><p>视频地址：<a href="https://space.bilibili.com/344849038/video" target="_blank" rel="noopener">b站-Yjango</a>，个人还是对他表示敬意的，他的视频挺具有启发性。</p><p>本文同样参考了<a href="https://www.zhihu.com/question/305705365/answer/621628417" target="_blank" rel="noopener"> 如何评价知乎用户 YJango 的公众号？ - Makise Chris的回答 - 知乎</a> </p></blockquote><a id="more"></a><h3 id="第一季"><a href="#第一季" class="headerlink" title="第一季"></a>第一季</h3><h4 id="01-何为学习"><a href="#01-何为学习" class="headerlink" title="01.何为学习"></a>01.何为学习</h4><p>学习就是从有限例子中找出规律的过程，而这个规律就是知识。</p><h4 id="02-如何学习"><a href="#02-如何学习" class="headerlink" title="02.如何学习"></a>02.如何学习</h4><p>因为人脑机能原因，单单<strong>记忆</strong>一个知识，显然远远达不到掌握它的水平。</p><p>而首先应该做的是明确<strong>问题（输入）</strong>和<strong>答案（输出）</strong>。</p><p>然后通过大量例子来理清二者的关系。接下来根据你的问题和答案，找出这些例子中符合问题及答案的规律。这两步也就是华罗庚的”先把书读厚（大量例子体会关系），再把书读薄（找出知识来压缩例子）。”</p><p>最后，需要用新的问题来验证你的例子是否正确。</p><h4 id="03-学习误区"><a href="#03-学习误区" class="headerlink" title="03.学习误区"></a>03.学习误区</h4><p>学习误区指什么？见图片黄字。</p><p>学习方式分为两类：（1）运动类：语言、运动（2）思考类：数学、逻辑</p><p>举了个语言的例子：</p><p>错误方式：</p><p><img src="https://s2.ax1x.com/2019/03/24/AYs9G8.png" alt=""></p><p>正确方式：</p><p><img src="https://s2.ax1x.com/2019/03/24/AYrzIP.png" alt=""></p><p>老实说我认为这个观点见仁见智，尤其是摒弃中文作为中间媒介这件事，姑且不论对错，可行性就值得怀疑了。</p><h4 id="04-运用误区"><a href="#04-运用误区" class="headerlink" title="04.运用误区"></a>04.运用误区</h4><p>运动类的学习可以同时进行，比如边走路边聊天；而思考类则不行，只能串行操作，所以当我们遇到一个大的问题（输入）时，如果无法直接得到输出，就需要将这个输入拆分，分别解决。</p><h4 id="05-思维导图"><a href="#05-思维导图" class="headerlink" title="05.思维导图"></a>05.思维导图</h4><p>其功能不在于记忆，而在于克服学习和运用中的误区。</p><p>在构建思维导图的过程中，你会先找到<strong>关键词</strong>，然后问自己它是什么？从而不断的联想起具体的例子，进而分析这些例子的共同输入和输出，找出规律；而当问自己它的作用或目的时，实际在思考输入是怎么变成输出的。输入输出可以代表<strong>一类</strong>事物中任何一种<strong>情况</strong>，因此被称为变量。而从输入变成输出的这一过程称为函数。而确定了这个“函数”，等下一次遇到未知的情况时，利用这个函数就可以解答出输出。随后你会用一个更好的关键词来代表你所找出的关系，一种是动宾结构，因为它描述了输入和函数，也就确定了输出，但当人们开始传播知识时，动宾结构也会名词化。</p><p>还有一种知识本身就是名词，会让你觉得没有输出，但这种知识的输出是分类任务中的类型，描述它的是主谓结构：是或不是/是否属于/属于哪个。</p><p>思维导图最强大的地方在于对知识的拆分，构建出知识网络，而知识网络中有些内容是你已经知道的，即具有重用性，学习新内容时，可以利用这些重用性，加快学习速度。拆分知识既可以用在学习未知知识上，也可以用在运用已有知识上（分而治之）。</p><p>而拆分知识的能力也是一种特殊的知识，称为二阶知识，不同于一般的知识用于描述信息与信息的关系，二阶知识用于描述知识与知识的关系。</p><p>视频中举的例子就是<strong>思维导图</strong>，假设我们需要构建一个有关于思维导图内容的思维导图。</p><p>那么首先，找出关键词——思维导图。然后问自己：思维导图是什么？因此联想出思维导图使用的具体例子：任务清单、知识体系、任务关系…，接着分析这些例子的共同输入——杂乱信息，共同输出——知识网络。而当你问它的作用时，也就是在思考输入是怎么变成输出的。一类事物，这个例子中，假设它代表的是任务清单这一类，那么输入的杂乱信息和输出的知识网络就是任务清单这一类事物的特定情况。而这个“函数”：可以理解为思维导图的画图方法。然后是找动宾结构，这里是压缩信息（其中压缩是函数，信息是输出），当人们开始传递知识时，压缩信息就被名词化成了思维导图。</p><p>知识本身属于名词的没说例子。</p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;&lt;a href=&quot;https://space.bilibili.com/344849038/video&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;b站-YJango&lt;/a&gt;，个人还是对这位先生的&lt;/p&gt;
&lt;p&gt;视频地址：&lt;a href=&quot;https://space.bilibili.com/344849038/video&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;b站-Yjango&lt;/a&gt;，个人还是对他表示敬意的，他的视频挺具有启发性。&lt;/p&gt;
&lt;p&gt;本文同样参考了&lt;a href=&quot;https://www.zhihu.com/question/305705365/answer/621628417&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt; 如何评价知乎用户 YJango 的公众号？ - Makise Chris的回答 - 知乎&lt;/a&gt; &lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
      <category term="Reading" scheme="http://yoursite.com/tags/Reading/"/>
    
  </entry>
  
  <entry>
    <title>关于记忆方法</title>
    <link href="http://yoursite.com/2019/03/25/%E5%85%B3%E4%BA%8E%E8%AE%B0%E5%BF%86%E6%96%B9%E6%B3%95/"/>
    <id>http://yoursite.com/2019/03/25/关于记忆方法/</id>
    <published>2019-03-25T09:52:12.000Z</published>
    <updated>2019-03-25T10:04:58.195Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>阅读了几篇关于记忆方法的文章，有所感悟，结合了一下自己经历，选了几个我认为有长期实践意义的方法，在此记录，此文可能会长期更新，毕竟记忆方法也是要不断调整的。</p></blockquote><h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>我认为针对不同问题，应该具有不同的记忆方法。比如：</p><p>单词记忆，那么也许联想类方法比较有用；</p><p>记忆某些抽象概念，如剩余价值的含义，也许更适合用关键词法；</p><p>…</p><p>所以记忆方法不能一概而论，犯教条主义错误。</p><a id="more"></a><h4 id="记忆整体框架"><a href="#记忆整体框架" class="headerlink" title="记忆整体框架"></a>记忆整体框架</h4><p>结构化，也就是常说的<strong>思维导图</strong>，从全局出发，有助于你理清思路，尤其当你面对一个比较庞杂而又相互关联的知识（比如政治一整个章节的内容）。现在很多书籍都会替你整理好，但是请务必要<strong>用自己的方式去整理一遍</strong>，也许有人会说，我自己思考的和书上整理的一样啊，那么也请合上书默写出来。</p><p>实际上你默写也好，自己写也罢，<strong>这里的关键是在这个过程是一定要去==主动思考==（敲重点！）这个庞杂的知识的脉络到底是什么样的。</strong></p><p>思维导图：</p><p><img src="https://s2.ax1x.com/2019/03/25/At22X8.png" alt=""></p><p><img src="https://s2.ax1x.com/2019/03/25/AtRC1x.png" alt=""></p><p>参考资料：<a href="https://zhuanlan.zhihu.com/YJango" target="_blank" rel="noopener">知乎专栏——Yjango</a></p><h4 id="记忆段落-句子内容"><a href="#记忆段落-句子内容" class="headerlink" title="记忆段落/句子内容"></a>记忆段落/句子内容</h4><p>一般来说，我们学习的总是不熟悉的内容，再加上作者和你在表述习惯上的差异，才会导致你无法理解作者想要传达的意思，才会导致只能死记硬背。</p><p>所以同样需要我们用自己的方式理解并记忆作者想要传达给你的意思。那么如何去用自己的语言表述出来呢？</p><p>引用一句话：记忆<strong>书本原有的关键词、知识点</strong>等<strong>“点”信息</strong>，<strong>而后自己去连“点”成“线</strong>”——指的是把这一块的“点”信息串成有逻辑的内容$^{[2]}$。</p><p>举个例子：</p><blockquote><p>类是对一群具有相同特征或者行为的事物的一个统称，是抽象的，不能直接使用。特征被称为”属性“，行为被称为”方法“。</p></blockquote><p>在这个句子中，对我而言，关键词是”类，特征=属性，行为=方法“。所以对我而言只需要记住这几个内容，剩下的用自己的话重复一遍：</p><blockquote><p>类是具有相同的特征和行为的事物的统称，是一个抽象概念。实际运用时，特征又叫做属性，行为又叫方法。</p></blockquote><p><strong>参考资料</strong>：[2].<a href="https://www.zhihu.com/question/50343728" target="_blank" rel="noopener">你有什么值得分享的高效学习方法？</a> 这个问题下最高赞的回答</p><p>我认为这个方法尤其适用于抽象概念——尤其是文科类——的记忆。</p><h4 id="记忆关键词"><a href="#记忆关键词" class="headerlink" title="记忆关键词"></a>记忆关键词</h4><p>联想记忆法，这个方法算是很常见了，但我认为可以看一下这个视频：·<a href="https://weibo.com/1974576991/HmiDNf2ps?type=comment" target="_blank" rel="noopener">TED画图记忆方法</a>。</p><p>视频中提到可以用画图的方式来替代文字记忆，但我认为在记忆大量内容的时候这种方法太低效。不过比起单纯的联想，画图具有更强的可操作性（有时候真是联想不到），而且动笔的过程中又进一步加深了你的思考。所以我认为可以在上一个记忆法中，记忆关键字时，根据需要使用这个方法。</p><p>还是上面的例子，当我们提取关键字后，可能还是会觉得抽象，类是什么？特征和行为又怎么理解？这时候把这几个词语通过联想的方式去理解记忆：</p><p>类，可以看做是制造一样东西（比如飞机）的设计图；属性，就好比飞机都有一对翅膀；方法，就好比飞机如何起飞。还可以接着拓展，比如用类创建一个对象，就相当于用设计图纸去制造一架飞机。</p><p><img src="https://s2.ax1x.com/2019/03/24/AYeagH.png" alt=""></p><p>其实无论哪种方法，归根结底都是要把新的知识转化为自己更熟悉的方式，加以理解并记忆。</p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;阅读了几篇关于记忆方法的文章，有所感悟，结合了一下自己经历，选了几个我认为有长期实践意义的方法，在此记录，此文可能会长期更新，毕竟记忆方法也是要不断调整的。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h4&gt;&lt;p&gt;我认为针对不同问题，应该具有不同的记忆方法。比如：&lt;/p&gt;
&lt;p&gt;单词记忆，那么也许联想类方法比较有用；&lt;/p&gt;
&lt;p&gt;记忆某些抽象概念，如剩余价值的含义，也许更适合用关键词法；&lt;/p&gt;
&lt;p&gt;…&lt;/p&gt;
&lt;p&gt;所以记忆方法不能一概而论，犯教条主义错误。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Reading" scheme="http://yoursite.com/tags/Reading/"/>
    
  </entry>
  
  <entry>
    <title>正向传播的向量化实现</title>
    <link href="http://yoursite.com/2019/03/23/%E6%AD%A3%E5%90%91%E4%BC%A0%E6%92%AD%E7%9A%84%E5%90%91%E9%87%8F%E5%8C%96%E5%AE%9E%E7%8E%B0/"/>
    <id>http://yoursite.com/2019/03/23/正向传播的向量化实现/</id>
    <published>2019-03-23T06:51:39.000Z</published>
    <updated>2019-04-06T10:13:09.799Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>神经网络中正向传播(Forward propagation)的向量化(Vectorized implementation)</p></blockquote><p><img src="https://s2.ax1x.com/2019/03/23/AGrwuR.png" alt=""></p><a id="more"></a><p>其实没什么难的，就是几个符号理解起来可能有点费劲，下面简单解释一下：</p><p>从这个式子入手：$z^{(2)}=\Theta^{(1)}*a^{(1)}$</p><p>$z^{(2)}$等于信号$a^{(1)}$在传输过程中乘上响应的权重$\Theta^{(1)}​$，展开来写的话就是左图中等号右边的灰色线框。</p><p>$a^{(1)}$（activation values of layer one）是指第一层中$x_{0}$~$x_{3}$构成的一维<strong>向量</strong>，上标1就是指第一层。</p><p>$\Theta^{(1)}$是指传递过程中不同信号所对应的权重，反应在公式上就是$\Theta^{(1)}_{10}$~$\Theta^{(1)}_{33}$所构成的<strong>矩阵</strong>。</p><p>$\Theta^{(1)}_{10}$下标的10是指，接收信号的神经元为下一层的第一个，发出信号的为这一层的第0个神经元。</p><p>这里额外说一下这个$\Theta^{(1)}​$的维度，如果在第$j​$层有$s_{j}​$个神经元(units)，在第$j+1​$层有$s_{j+1}​$个神经元，那么对应的权重的维度为$s_{j+1}*(s_{j}+1)​$。</p><p>其中$s_{j}+1$是因为上一层在传递时，默认会传递一个$s_{0}=1​$</p><p>以上是传递过程中的，$z^{(2)}$传递到神经元之后，就通过激励函数（图中是S函数）求出需要传递个下一层的信号$a^{(2)}$</p><p>即公式：$a^{(2)}=g(z^{(2)})$</p><p>依次类推，即可求出最终的假设函数$h_{\Theta}(x)$</p><blockquote><p>本文是从信号发出的角度去阐述，但似乎更多文章时站在信号接收的角度，但意思是一样的</p></blockquote><p>参考文章</p><p>[1]. <a href="https://zhuanlan.zhihu.com/p/28299430" target="_blank" rel="noopener">机器学习笔记（2）—— 神经网络</a></p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;神经网络中正向传播(Forward propagation)的向量化(Vectorized implementation)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img src=&quot;https://s2.ax1x.com/2019/03/23/AGrwuR.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="Machine Learning" scheme="http://yoursite.com/tags/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>sklearn中的函数classification_report</title>
    <link href="http://yoursite.com/2019/03/20/sklearn%E4%B8%AD%E7%9A%84%E5%87%BD%E6%95%B0classification-report/"/>
    <id>http://yoursite.com/2019/03/20/sklearn中的函数classification-report/</id>
    <published>2019-03-20T08:42:54.000Z</published>
    <updated>2019-03-24T01:51:45.678Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>classification_report函数主要用于显示主要分类指标的文本报告</p></blockquote><h4 id="1-前言"><a href="#1-前言" class="headerlink" title="1.前言"></a>1.前言</h4><p>在报告中显示每个类的精确度、召回率等信息（可以用来检测回归算法的准确度）。</p><a id="more"></a><h4 id="2-classification-report-参数"><a href="#2-classification-report-参数" class="headerlink" title="2.classification_report()参数"></a>2.classification_report()参数</h4><p>详见<a href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html" target="_blank" rel="noopener">官方文档</a>，这里只说几个重要参数</p><p><code>y_true</code>：1维数组，目标值</p><p><code>y_pred</code>：1维数组，分类器返回的估计值</p><p><code>label</code>：数组，报告中显示的类标签索引列表</p><p><code>target_names</code>：字符串列表，当和<code>label</code>匹配时作为<code>label</code>的名称</p><h4 id="3-classification-report-返回值"><a href="#3-classification-report-返回值" class="headerlink" title="3.classification_report()返回值"></a>3.classification_report()返回值</h4><blockquote><p>解释一下两个名词</p><p>正样本：与你所要研究的目的相关</p><p>负样本：与你所要研究的目的无关</p><p>举个例子：如果你要做一间教室里的人脸识别，那么正样本就是人脸，负样本就是课桌、门窗之类的</p></blockquote><p>TP(True Positive): 预测为正样本， 实际为正样本（预测正确）</p><p>FP(False Positive): 预测为正样本，  实际为负样本 （预测错误）</p><p>FN(False Negative): 预测为负样本，实际为正样本 （预测错误）</p><p>TN(True Negative): 预测为负样本， 实际为负样本 （预测正确）</p><p>精确度(precision)=正确预测的个数(TP)/预测为正样本的个数(TP+FP)</p><blockquote><p>检索结果中，都是你认为应该为正的样本（第二个字母都是P），但是其中有你判断正确的和判断错误的（第一个字母有T ，F）。</p></blockquote><p>召回率(recall)=正确预测值的个数(TP)/实际为正样本的个数(TP+FN)</p><blockquote><p>检索结果中，你判断为正的样本也确实为正的，以及那些没在检索结果中被你判断为负但是事实上是正的，或者说你没预测到的（FN）。</p></blockquote><p>F1值=2*精度*召回率/(精度+召回率)</p><p>不明白的话参考以下代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">4</span>]: <span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> classification_report</span><br><span class="line">   ...: y_true = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>]</span><br><span class="line">   ...: y_pred = [<span class="number">1</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">2</span>]</span><br><span class="line">   ...: labels =[<span class="number">1</span>,<span class="number">3</span>,<span class="number">2</span>]</span><br><span class="line">   ...: target_names = [<span class="string">'labels_1'</span>,<span class="string">'labels_2'</span>,<span class="string">'labels_3'</span>,<span class="string">'labels-4'</span>]</span><br><span class="line">   ...: print(classification_report(y_true,y_pred,labels=labels,target_names= t</span><br><span class="line">   ...: arget_names,digits=<span class="number">3</span>))</span><br><span class="line">   ...:</span><br><span class="line">              precision    recall  f1-score   support</span><br><span class="line"></span><br><span class="line">    labels_1      <span class="number">0.500</span>     <span class="number">1.000</span>     <span class="number">0.667</span>         <span class="number">1</span></span><br><span class="line">    labels_2      <span class="number">1.000</span>     <span class="number">0.667</span>     <span class="number">0.800</span>         <span class="number">3</span></span><br><span class="line">    labels_3      <span class="number">0.000</span>     <span class="number">0.000</span>     <span class="number">0.000</span>         <span class="number">1</span></span><br><span class="line"></span><br><span class="line">   micro avg      <span class="number">0.600</span>     <span class="number">0.600</span>     <span class="number">0.600</span>         <span class="number">5</span></span><br><span class="line">   macro avg      <span class="number">0.500</span>     <span class="number">0.556</span>     <span class="number">0.489</span>         <span class="number">5</span></span><br><span class="line">weighted avg      <span class="number">0.700</span>     <span class="number">0.600</span>     <span class="number">0.613</span>         <span class="number">5</span></span><br></pre></td></tr></table></figure><p>最右边<code>support</code>列为每个标签的出现次数(权重)。</p><p><code>micro avg</code>：计算所有数据中预测正确的值，比如这里是3/5=0.6</p><p><code>macro avg</code>：每个类别指标中的未加权平均值(一列)，比如准确率(precision)的<code>macro avg</code>是：$(0.5+1.0+0)/3=0.5$</p><p><code>weighted avg</code>：每个类别指标中的加权平均，比如准确率(precision)的<code>weighted avg</code>是：$(0.5<em>1+1.0</em>3+0*1)/3=0.7$  </p><h4 id="4-参考资料"><a href="#4-参考资料" class="headerlink" title="4.参考资料"></a>4.参考资料</h4><p>[1] <a href="http://www.cnblogs.com/akrusher/articles/6442839.html" target="_blank" rel="noopener">博客园</a></p><p>[2] <a href="https://blog.csdn.net/genghaihua/article/details/81155200" target="_blank" rel="noopener">CSDN博客</a></p><p>[3] <a href="https://blog.csdn.net/kancy110/article/details/74937469" target="_blank" rel="noopener">CSDN博客</a></p><p>[4] <a href="https://blog.csdn.net/guyubit/article/details/52276013" target="_blank" rel="noopener">CSDN博客——TP、TN、FP、FN解释说明</a></p><p>[5] <a href="https://www.libinx.com/2018/understanding-sklearn-classification-report/" target="_blank" rel="noopener">读懂 sklearn 的 classification_report</a></p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;classification_report函数主要用于显示主要分类指标的文本报告&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4 id=&quot;1-前言&quot;&gt;&lt;a href=&quot;#1-前言&quot; class=&quot;headerlink&quot; title=&quot;1.前言&quot;&gt;&lt;/a&gt;1.前言&lt;/h4&gt;&lt;p&gt;在报告中显示每个类的精确度、召回率等信息（可以用来检测回归算法的准确度）。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Python" scheme="http://yoursite.com/tags/Python/"/>
    
      <category term="Machine Learning" scheme="http://yoursite.com/tags/Machine-Learning/"/>
    
      <category term="sklearn" scheme="http://yoursite.com/tags/sklearn/"/>
    
  </entry>
  
  <entry>
    <title>Numpy中的矩阵乘法</title>
    <link href="http://yoursite.com/2019/03/19/Numpy%E4%B8%AD%E7%9A%84%E7%9F%A9%E9%98%B5%E4%B9%98%E6%B3%95/"/>
    <id>http://yoursite.com/2019/03/19/Numpy中的矩阵乘法/</id>
    <published>2019-03-19T03:38:01.000Z</published>
    <updated>2019-03-29T08:07:58.601Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>简单介绍一下Numpy中<code>.dot()</code>、<code>*</code>、<code>multiply()</code>和<code>@</code>的区别</p></blockquote><a id="more"></a><h4 id="1-np-multiply-函数"><a href="#1-np-multiply-函数" class="headerlink" title="1. np.multiply()函数"></a>1. np.multiply()函数</h4><p>数组/矩阵对应的位置相乘。</p><h4 id="2-np-dot-函数"><a href="#2-np-dot-函数" class="headerlink" title="2. np.dot()函数"></a>2. np.dot()函数</h4><p>2.1.  当数组/矩阵秩为1(即向量)时，执行点积。</p><p>2.2.  当数组/矩阵秩大于2时，执行矩阵乘法。</p><p>2.3.  .dot可以被数组对象调用，也可以通过numpy库调用（被matrix调用可以执行，但会报错）。</p><h4 id="3-星号-乘法运算"><a href="#3-星号-乘法运算" class="headerlink" title="3.  星号(*)乘法运算"></a>3.  星号(*)乘法运算</h4><p>3.1.  对数组执行对应位置相乘。</p><p>3.2.  对矩阵执行矩阵乘法。</p><h4 id="4-乘法运算"><a href="#4-乘法运算" class="headerlink" title="4. @乘法运算"></a>4. @乘法运算</h4><p>4.1.  对于矩阵乘法而言，完全等价于<code>.dot()</code>。</p><p>4.2.  区别在于，当a和b中有一个是标量时，只能用<code>.dot()</code>否则会报错。</p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;简单介绍一下Numpy中&lt;code&gt;.dot()&lt;/code&gt;、&lt;code&gt;*&lt;/code&gt;、&lt;code&gt;multiply()&lt;/code&gt;和&lt;code&gt;@&lt;/code&gt;的区别&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
      <category term="Python" scheme="http://yoursite.com/tags/Python/"/>
    
      <category term="Machine Learning" scheme="http://yoursite.com/tags/Machine-Learning/"/>
    
      <category term="Numpy" scheme="http://yoursite.com/tags/Numpy/"/>
    
  </entry>
  
  <entry>
    <title>Numpy中的Meshgrid</title>
    <link href="http://yoursite.com/2019/03/16/Numpy%E4%B8%AD%E7%9A%84Meshgrid/"/>
    <id>http://yoursite.com/2019/03/16/Numpy中的Meshgrid/</id>
    <published>2019-03-16T07:42:50.000Z</published>
    <updated>2019-03-16T07:42:50.868Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>Numpy是在利用Python进行科学计算和数据处理时肯定会使用到的模块(moudle)。</p><p>本文用比较通俗的语言讲解一下其中的Meshgrid函数</p></blockquote><h4 id="1-Meshgrid前言"><a href="#1-Meshgrid前言" class="headerlink" title="1.Meshgrid前言"></a>1.Meshgrid前言</h4><p>简单来说，<code>Meshgrid</code>就是在用两个坐标轴上的点在平面上画网格，当然也可以用三个坐标，但是为了方便理解，下面都用两个坐标轴举例。</p><a id="more"></a><h4 id="2-Meshgrid参数"><a href="#2-Meshgrid参数" class="headerlink" title="2.Meshgrid参数"></a>2.Meshgrid参数</h4><p>详情咨询<a href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.meshgrid.html" target="_blank" rel="noopener">官方文档</a>，最常用的就是传入两个一维数组</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x = np.linspace(<span class="number">-10</span>,<span class="number">10</span>,<span class="number">2</span>)</span><br><span class="line">y = np.linspace(<span class="number">-10</span>,<span class="number">10</span>,<span class="number">2</span>)</span><br><span class="line">xv,yv = meshgrid(x,y)</span><br></pre></td></tr></table></figure><h4 id="3-Meshgrid返回值"><a href="#3-Meshgrid返回值" class="headerlink" title="3.Meshgrid返回值"></a>3.Meshgrid返回值</h4><p>xv的返回值：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[[-10,10],</span><br><span class="line"> [-10,10]]</span><br></pre></td></tr></table></figure><p>yv的返回值：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[[-10,-10],</span><br><span class="line"> [10,10]]</span><br></pre></td></tr></table></figure><p>从上面很容易看出，Meshgrid实际上是返回两个<strong>矩阵</strong>，两个矩阵不同之处，下面用一张图来表示一目了然。</p><p><img src="https://s2.ax1x.com/2019/03/15/AEfxHJ.jpg" alt=""></p><p>可以看出就是通过两个矩阵的方式完成了网格的绘制。</p><h4 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h4><p>[1] <a href="https://zhuanlan.zhihu.com/p/33579211" target="_blank" rel="noopener">Python-Numpy模块Meshgrid函数</a></p><p>[2] <a href="https://zhuanlan.zhihu.com/p/33579211" target="_blank" rel="noopener">Numpy中Meshgrid函数介绍及2种应用场景</a></p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;Numpy是在利用Python进行科学计算和数据处理时肯定会使用到的模块(moudle)。&lt;/p&gt;
&lt;p&gt;本文用比较通俗的语言讲解一下其中的Meshgrid函数&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4 id=&quot;1-Meshgrid前言&quot;&gt;&lt;a href=&quot;#1-Meshgrid前言&quot; class=&quot;headerlink&quot; title=&quot;1.Meshgrid前言&quot;&gt;&lt;/a&gt;1.Meshgrid前言&lt;/h4&gt;&lt;p&gt;简单来说，&lt;code&gt;Meshgrid&lt;/code&gt;就是在用两个坐标轴上的点在平面上画网格，当然也可以用三个坐标，但是为了方便理解，下面都用两个坐标轴举例。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Python" scheme="http://yoursite.com/tags/Python/"/>
    
      <category term="Machine Learning" scheme="http://yoursite.com/tags/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>vscode常用快捷键</title>
    <link href="http://yoursite.com/2019/03/14/vscode%E5%B8%B8%E7%94%A8%E5%BF%AB%E6%8D%B7%E9%94%AE/"/>
    <id>http://yoursite.com/2019/03/14/vscode常用快捷键/</id>
    <published>2019-03-14T12:14:52.000Z</published>
    <updated>2019-03-14T12:14:52.543Z</updated>
    
    <content type="html"><![CDATA[<p>Ctrl+D：选中当前单词，再按一次选中下一个。（妈的谁用谁知道）</p><p>Ctrl+L：选中当前行</p><p>ctrl+shift+alt+up/down：多行编辑</p><p>未完待续…</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Ctrl+D：选中当前单词，再按一次选中下一个。（妈的谁用谁知道）&lt;/p&gt;
&lt;p&gt;Ctrl+L：选中当前行&lt;/p&gt;
&lt;p&gt;ctrl+shift+alt+up/down：多行编辑&lt;/p&gt;
&lt;p&gt;未完待续…&lt;/p&gt;

      
    
    </summary>
    
    
      <category term="vscode" scheme="http://yoursite.com/tags/vscode/"/>
    
  </entry>
  
  <entry>
    <title>浅谈分位数</title>
    <link href="http://yoursite.com/2019/03/12/%E6%B5%85%E8%B0%88%E5%88%86%E4%BD%8D%E6%95%B0/"/>
    <id>http://yoursite.com/2019/03/12/浅谈分位数/</id>
    <published>2019-03-12T12:41:18.000Z</published>
    <updated>2019-03-14T12:15:30.449Z</updated>
    
    <content type="html"><![CDATA[<p>在Coursera看Ng的视频时，因为不想用MATLAB和Octave，改用了Python实现，由于用到了pandas这个模块(moudle)中的<code>.describe()</code>方法，对它输出值(见下面代码段)中的分位数（25%，50%，75%）没太弄懂，再查阅各种资料之后，做一个简单的总结。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>s = pd.Series([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>s.describe()</span><br><span class="line">count    <span class="number">3.0</span></span><br><span class="line">mean     <span class="number">2.0</span></span><br><span class="line">std      <span class="number">1.0</span></span><br><span class="line">min      <span class="number">1.0</span></span><br><span class="line"><span class="number">25</span>%      <span class="number">1.5</span></span><br><span class="line"><span class="number">50</span>%      <span class="number">2.0</span></span><br><span class="line"><span class="number">75</span>%      <span class="number">2.5</span></span><br><span class="line">max      <span class="number">3.0</span></span><br><span class="line">dtype: float64</span><br></pre></td></tr></table></figure><a id="more"></a><h2 id="中位数"><a href="#中位数" class="headerlink" title="中位数"></a>中位数</h2><p>首先需要把数据集从小到大<strong>排列</strong>（从大到小也行，取决于实际需求）。</p><p><strong>1、若样本个数n为奇数，那么只需要取中间值（第n/2个）就是数据集的中位数。</strong></p><p>e.g.数据集[1,2,3]，则中位数为2</p><p><strong>2、若n为偶数，则取中间两个数的平均值作为中位数</strong></p><p>e.g.[1,2,3,4]，则中位数为(3+2)/2=2.5</p><p>中位数相对简单且好理解，其往往运用在数据集中某个样本出现明显异常值时。比如[1,2,3,4,1000000]，这时候如果去中位数是3，但如果取平均数，就会产生巨大误差。</p><h2 id="四分位数（25-50-75-）"><a href="#四分位数（25-50-75-）" class="headerlink" title="四分位数（25%,50%,75%）"></a>四分位数（25%,50%,75%）</h2><p>四分位数是作为中位数的拓展，关于如何计算四分位数我发现了两种不同的计算方法，得出的数值会存在差异。wiki上说对于该值的选取仍然存在争议，但是无论用那种方法，都能够将数据集划分，从而进一步分析数据变量的趋势。</p><h4 id="解法一：分步法（自己取的）"><a href="#解法一：分步法（自己取的）" class="headerlink" title="解法一：分步法（自己取的）"></a>解法一：分步法（自己取的）</h4><p>这个方法很简单，就是先排序然后取中位数（50%），然后以中位数为分界线，把数据集切分为两部分再分别取中位值（25%，75%）</p><p><strong>1、若样本个数n为奇数</strong></p><p>比如[1,2,3]</p><p>50%分位：(1+3)/2=2；</p><p>25%分位：(1+2)/2=1.5；</p><p>75%分位：(2+3)/2=2.5；</p><blockquote><p>这个例子举的有点不好，带小数点了，但是意思理解就行，就是取中位数—划分—取中位数</p></blockquote><p><strong>2、若样本个数n为偶数</strong></p><p>[1,2,3,4,5,6,7,8]</p><p>50%分位：(4+5)/2=4.5；</p><p>25%分位：(2+3)/2=2.5；</p><p>75%分位：(6+7)/2=6.5；</p><p>而在pandas中也是用这种方法计算的，wiki词条中的例子也是。</p><h4 id="解法二："><a href="#解法二：" class="headerlink" title="解法二："></a>解法二：</h4><p>这个方法有点麻烦。</p><p><strong>1、若样本个数n为奇数</strong></p><p>首先确定四分位的位置。</p><p>设置下四分位数、中位数和上四分位数，记为Q1、Q2、Q3</p><p>Q1位置=(n+1)/4      (25%分位)</p><p>Q2位置=2(n+1)/4    (50%分位)</p><p>Q3位置=3(n+1)/4    (75%分位)</p><p>e.g.[13、13.5、13.8、13.9、14、14.6、14.8、15、15.2、15.4、15.7]</p><p>Q1位置=(11+1)/4=3</p><p>Q2位置=2(11+1)/4=6</p><p>Q3位置=3(11+1)/4=9</p><p>对应的分位值这分别为13.8、14.6、15.2</p><p><strong>2、若样本个数n为偶数</strong></p><p>依然先确定位置Q1、Q2、Q3，这里直接用一个例子来说明，[14、15、16，16、17、18、18、19、19、20、2l，21、22、22、23、24、24、25、26、26]，例子中n=20。</p><p>Q1位置=(20+1)/4=5.25，这里可以看到位于[5]、[6]位置是17和18，则Q1=17+(18-17)*(5.25-5)=17.25</p><p>同理可得：</p><p>Q2位置=2(20+1)/4=10.5，则Q2=20+(21-20)*(10.5-10)=20.5</p><p>Q3位置=3(20+1)/4=15.75，则Q3=23+(24-23)*(15.75-15)=23.75</p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p>1、<a href="https://zh.wikipedia.org/wiki/%E5%9B%9B%E5%88%86%E4%BD%8D%E6%95%B0" target="_blank" rel="noopener">维基百科</a></p><p>2、<a href="https://wiki.mbalib.com/wiki/%E5%9B%9B%E5%88%86%E4%BD%8D%E6%95%B0" target="_blank" rel="noopener">MBA百科</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在Coursera看Ng的视频时，因为不想用MATLAB和Octave，改用了Python实现，由于用到了pandas这个模块(moudle)中的&lt;code&gt;.describe()&lt;/code&gt;方法，对它输出值(见下面代码段)中的分位数（25%，50%，75%）没太弄懂，再查阅各种资料之后，做一个简单的总结。&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;meta&quot;&gt;&amp;gt;&amp;gt;&amp;gt; &lt;/span&gt;s = pd.Series([&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;])&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;meta&quot;&gt;&amp;gt;&amp;gt;&amp;gt; &lt;/span&gt;s.describe()&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;count    &lt;span class=&quot;number&quot;&gt;3.0&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;mean     &lt;span class=&quot;number&quot;&gt;2.0&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;std      &lt;span class=&quot;number&quot;&gt;1.0&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;min      &lt;span class=&quot;number&quot;&gt;1.0&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;number&quot;&gt;25&lt;/span&gt;%      &lt;span class=&quot;number&quot;&gt;1.5&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;number&quot;&gt;50&lt;/span&gt;%      &lt;span class=&quot;number&quot;&gt;2.0&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;number&quot;&gt;75&lt;/span&gt;%      &lt;span class=&quot;number&quot;&gt;2.5&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;max      &lt;span class=&quot;number&quot;&gt;3.0&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;dtype: float64&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
    
      <category term="Machine Learning" scheme="http://yoursite.com/tags/Machine-Learning/"/>
    
      <category term="Math" scheme="http://yoursite.com/tags/Math/"/>
    
  </entry>
  
  <entry>
    <title>计算机编码问题</title>
    <link href="http://yoursite.com/2019/03/05/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BC%96%E7%A0%81%E9%97%AE%E9%A2%98/"/>
    <id>http://yoursite.com/2019/03/05/计算机编码问题/</id>
    <published>2019-03-05T10:14:56.000Z</published>
    <updated>2019-03-05T10:14:56.476Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>整理了一些我对进制之间转换的理解，有些地方为了记忆方便可能不太准确，欢迎指正。</p></blockquote><p>我们日常最常用的进制当然是十进制DEC，因为最符合人类的习惯（数手指）。</p><p>对于计算机而言，最熟悉的则是二进制BIN（高低电平表示<code>1</code>和<code>0</code>）。</p><p>此外常用的还有八进制OCT，十六进制HEX。</p><a id="more"></a><h4 id="写在前面"><a href="#写在前面" class="headerlink" title="写在前面"></a>写在前面</h4><p>以最常见的十进制为例，我们对于数字的记录方式是通过<strong>数位+基数(0~9)</strong>，比如152可以看成是1*10²+5*10¹+2*10º，数位从低到高为个位、十位、百位…</p><p>这种方式对应到其他进制只是数位和基数都不相同，其组成数字的法则是一样的。</p><p>比如二进制从低位到高位可以看做是个位、二位、四位、八位….</p><p>e.g. 152的二进制表示是10011000，就可以看做是$1<em>2^7+1</em>2^4+1*2^3$</p><blockquote><p>一般而言，<strong>不同进制之间转换的时候都会以BIN作为桥梁</strong>。</p></blockquote><h4 id="不同进制之间转换"><a href="#不同进制之间转换" class="headerlink" title="不同进制之间转换"></a>不同进制之间转换</h4><p><a href="https://www.zhihu.com/question/22205629/answer/61304268" target="_blank" rel="noopener">不同进制之间如何熟练转换-知乎</a></p><p>因为OCT和HEX都是BIN的倍数，所以一般转换时都是分位转换，即OCT的一位数相当于BIN的三位，HEX的一位数相当于BIN的四位。</p><h4 id="常听见的几种编码"><a href="#常听见的几种编码" class="headerlink" title="常听见的几种编码"></a>常听见的几种编码</h4><p>ASCII标准码：最高位奇偶校验位，后七位进行存储，可存128个字符。</p><p>Unicode：因为计算机的普及，中文、日文以及其它文字使得更多的字符需要被编码，为了统一标准，因此Unicode诞生了，用两个字节来表示一个字符。</p><p>UTF-8：因为字母只需要一个字节，而汉字需要两个字节，这样在使用Unicode表示字母的时候就导致内存空间被浪费，因此再Unicode的基础上又诞生了更通用的UTF-8,他的特点是对不同范围的字符使用不同长度的编码。</p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;整理了一些我对进制之间转换的理解，有些地方为了记忆方便可能不太准确，欢迎指正。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;我们日常最常用的进制当然是十进制DEC，因为最符合人类的习惯（数手指）。&lt;/p&gt;
&lt;p&gt;对于计算机而言，最熟悉的则是二进制BIN（高低电平表示&lt;code&gt;1&lt;/code&gt;和&lt;code&gt;0&lt;/code&gt;）。&lt;/p&gt;
&lt;p&gt;此外常用的还有八进制OCT，十六进制HEX。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Computer foundation" scheme="http://yoursite.com/tags/Computer-foundation/"/>
    
      <category term="编码" scheme="http://yoursite.com/tags/%E7%BC%96%E7%A0%81/"/>
    
  </entry>
  
  <entry>
    <title>Python爬虫实践1</title>
    <link href="http://yoursite.com/2019/03/01/Python%E7%88%AC%E8%99%AB%E5%AE%9E%E8%B7%B51/"/>
    <id>http://yoursite.com/2019/03/01/Python爬虫实践1/</id>
    <published>2019-03-01T08:42:57.000Z</published>
    <updated>2019-03-01T08:50:06.761Z</updated>
    
    <content type="html"><![CDATA[<h2 id="项目001：爬取本地html文件"><a href="#项目001：爬取本地html文件" class="headerlink" title="项目001：爬取本地html文件"></a>项目001：爬取本地html文件</h2><h3 id="要求：爬取评分高于4分的文章标题和分类"><a href="#要求：爬取评分高于4分的文章标题和分类" class="headerlink" title="要求：爬取评分高于4分的文章标题和分类"></a>要求：爬取评分高于4分的文章标题和分类</h3><p><img src="https://s2.ax1x.com/2019/02/22/kWLnnP.png" alt=""></p><a id="more"></a><h3 id="解决步骤："><a href="#解决步骤：" class="headerlink" title="解决步骤："></a>解决步骤：</h3><p>step1、用BeautifulSoup解析网页</p><p>step2、查找需要用的tag</p><p>step3、提取tag中有用的信息</p><p>代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"></span><br><span class="line">data = []</span><br><span class="line">path = <span class="string">'1_2/1_2code_of_video/web/new_index.html'</span> <span class="comment">#vscode的相对路径是对于工程目录而非当前目录</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#用with open() as f的方法比f.open()好，因为前者封装了f.close()，省去了清除内存的麻烦。</span></span><br><span class="line"><span class="keyword">with</span> open(path, <span class="string">'r'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    Soup = BeautifulSoup(f.read(), <span class="string">'lxml'</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#这里用select()和find_all()都行</span></span><br><span class="line">    titles = Soup.select(<span class="string">'ul &gt; li &gt; div.article-info &gt; h3 &gt; a'</span>)</span><br><span class="line">    pics = Soup.select(<span class="string">'ul &gt; li &gt; img'</span>)</span><br><span class="line">    descs = Soup.select(<span class="string">'ul &gt; li &gt; div.article-info &gt; p.description'</span>)</span><br><span class="line">    rates = Soup.select(<span class="string">'ul &gt; li &gt; div.rate &gt; span'</span>)</span><br><span class="line">    cates = Soup.select(<span class="string">'ul &gt; li &gt; div.article-info &gt; p.meta-info'</span>)</span><br><span class="line"></span><br><span class="line">    </span><br><span class="line"><span class="comment">#因为要通过一个tag值来判断，随后返回该tag值对应的另一tag值</span></span><br><span class="line"><span class="comment">#所以这里用dict来处理</span></span><br><span class="line"><span class="keyword">for</span> title, pic, desc, rate, cate <span class="keyword">in</span> zip(titles, pics, descs, rates, cates):</span><br><span class="line">    info = &#123;</span><br><span class="line">        <span class="string">'title'</span>: title.get_text(),</span><br><span class="line">        <span class="string">'pic'</span>: pic.get(<span class="string">'src'</span>),</span><br><span class="line">        <span class="string">'descs'</span>: desc.get_text(),</span><br><span class="line">        <span class="string">'rate'</span>: rate.get_text(),</span><br><span class="line">        <span class="string">'cate'</span>: list(cate.stripped_strings)</span><br><span class="line">    &#125;</span><br><span class="line">    data.append(info)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> data:</span><br><span class="line">    <span class="keyword">if</span> len(i[<span class="string">'rate'</span>]) &gt;= <span class="number">3</span>:</span><br><span class="line">        print(i[<span class="string">'title'</span>], i[<span class="string">'cate'</span>])</span><br></pre></td></tr></table></figure><h3 id="最后输出"><a href="#最后输出" class="headerlink" title="最后输出"></a>最后输出</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Sardinia&apos;s top 10 beaches [&apos;fun&apos;, &apos;Wow&apos;]</span><br><span class="line">How to get tanned [&apos;butt&apos;, &apos;NSFW&apos;]</span><br><span class="line">How to be an Aussie beach bum [&apos;sea&apos;]</span><br><span class="line">Summer&apos;s cheat sheet [&apos;bay&apos;, &apos;boat&apos;, &apos;beach&apos;]</span><br></pre></td></tr></table></figure><h3 id="其余笔记"><a href="#其余笔记" class="headerlink" title="其余笔记"></a>其余笔记</h3><p>1、</p><blockquote><p>Beautiful Soup将复杂HTML文档转换成一个复杂的树形结构,每个节点都是Python对象,所有对象可以归纳为4种: <code>Tag</code> , <code>NavigableString</code> , <code>BeautifulSoup</code> , <code>Comment</code> .                                                      ——官方手册</p></blockquote><p>2、select()和find_all()</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#find_all()的返回值类型</span></span><br><span class="line">&lt;<span class="class"><span class="keyword">class</span> '<span class="title">bs4</span>.<span class="title">element</span>.<span class="title">ResultSet</span>'&gt;</span></span><br><span class="line"><span class="class">#<span class="title">select</span><span class="params">()</span>的返回值类型</span></span><br><span class="line"><span class="class">&lt;<span class="title">class</span> '<span class="title">list</span>'&gt;</span></span><br></pre></td></tr></table></figure><p>返回的都是包含标签的<strong>列表</strong>。</p><p>参考资料</p><p>[1] <a href="https://www.cnblogs.com/ymjyqsx/p/6554817.html" target="_blank" rel="noopener">with open() as f和open()的区别</a></p><p>[2] <a href="https://www.jianshu.com/p/9cb390ffec29" target="_blank" rel="noopener">vscode文件路径问题</a></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;项目001：爬取本地html文件&quot;&gt;&lt;a href=&quot;#项目001：爬取本地html文件&quot; class=&quot;headerlink&quot; title=&quot;项目001：爬取本地html文件&quot;&gt;&lt;/a&gt;项目001：爬取本地html文件&lt;/h2&gt;&lt;h3 id=&quot;要求：爬取评分高于4分的文章标题和分类&quot;&gt;&lt;a href=&quot;#要求：爬取评分高于4分的文章标题和分类&quot; class=&quot;headerlink&quot; title=&quot;要求：爬取评分高于4分的文章标题和分类&quot;&gt;&lt;/a&gt;要求：爬取评分高于4分的文章标题和分类&lt;/h3&gt;&lt;p&gt;&lt;img src=&quot;https://s2.ax1x.com/2019/02/22/kWLnnP.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="Python" scheme="http://yoursite.com/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>十分钟世界史</title>
    <link href="http://yoursite.com/2019/01/31/%E5%8D%81%E5%88%86%E9%92%9F%E4%B8%96%E7%95%8C%E5%8F%B2/"/>
    <id>http://yoursite.com/2019/01/31/十分钟世界史/</id>
    <published>2019-01-31T08:21:45.000Z</published>
    <updated>2019-03-05T14:52:23.792Z</updated>
    
    <content type="html"><![CDATA[<p>Crash Coursera：<a href="https://www.bilibili.com/video/av1893599/?p=3" target="_blank" rel="noopener">十分钟世界史</a></p><p>0、农业革命：我们现在所存在的很多问题，都是由农业革命引起的，而这个革命却在不同的文明之间都发生了。尽管很难说清楚，人类不约而同的选择这一步是否正确，但无论如何我们都无法回头了。</p><a id="more"></a><h2 id="Season-1"><a href="#Season-1" class="headerlink" title="Season 1"></a>Season 1</h2><p>1）印度河谷文明：（2300 BC~1750 BC）挖掘出了遗物，但是文字失传无法翻译。</p><p>2）美索不达米亚文明：（3000 BC~2000 BC）发明了书写和税收，早起的政治形态，还有《汉谟拉比法典》。</p><p>其中著名的王朝有巴比伦和亚述。</p><p>3）古埃及：（3000 BC~332 BC）存在时间非常长，比起两河和恒河，这应该感谢一下尼罗河的温柔。现存的金字塔等都是这个文明遗留下的</p><p>4）古希腊人和波斯人（800 BC~146 BC）古希腊的文明有多灿烂？随便举几个例子，雅典，苏格拉底，奥运…还有那些耳熟能详的战争：马拉松，斯巴达勇士…西方文明的源头之一。波斯帝国了解不多，居鲁士大帝，温泉关战役。</p><p>5）印度（区分于古印度）：（1500 BC~）来自高加索的雅利安人进入印度，婆罗门教诞生，确立种姓制度，认为人死后会轮回，后来释迦摩尼悟出佛道，即经过修行，甚至有可能在这一世就打破轮回。</p><p>7）亚历山大：（356 BC~323 BC）雄才大略的君王，把希腊，埃及，波斯统一麾下，甚至大军打到了印度河流域，可惜英年早逝。</p><p>8）罗马帝国：主要人物凯撒（102 BC~44 BC），早起与庞培、克拉苏组成三头同盟，后打败庞培，成为终身独裁官，公元前44年，遭元老院成员暗杀身亡，后其养子屋大维创立罗马帝国，即奥古斯都大帝。</p><p>基督教：诞生于公元1世纪，最早流行与犹太人之间，所以要杀耶稣也应该是罗马人….395年罗马分裂，东罗马帝国信仰基督教分支东正教。</p><p>东罗马帝国：被历史学家称为“拜占庭帝国”，都城君士坦丁堡（今伊斯坦布尔）。期间首都曾被十字军攻陷，后收回。于1435年灭亡。</p><p>10）伊斯兰教（570 CE ~632 CE）</p><p>夹在拜占庭帝国和萨珊王朝（被亚历山大贡献后重建的波斯帝国）之间的古莱什部落即将迎来一位伟大的先知。</p><p>作为世界三大宗教之一的伊斯兰教，其中最著名的人物是被他们称作“最后的先知”——穆罕默德，与另外两个宗教领袖所不同的是，穆罕默德能打仗，在他的领导下，阿拉伯半岛开始以伊斯兰为核心建立统一的穆斯林国家。他死后，因为继承权问题，伊斯兰教主要分为逊尼和什叶两派，直至今日。</p><p>11）中世纪（公元5世纪~公元15世纪）：开始于西罗马帝国灭亡。</p><p>十字军东征：为了收复落入伊斯兰手中的圣城耶路撒冷，教皇发动的东征，总共9次（第4次是针对拜占庭帝国）</p><p>12）非洲：事实证明古代的非洲并非想象的那样贫穷，也出现过许多盛极一时的帝国，比如：马里帝国。</p><p>13）蒙古帝国（1206 CE~1635 CE）：1206年铁木真被推举为可汗，从此开始他南征北战的一生。东起日本海，西至地中海，北跨西伯利亚，南至波斯湾，蒙古帝国盛极一时，在成吉思汗死后，蒙古帝国分裂成四大汗国，其后各自走向终结</p><p>14）印度洋贸易（公元11世纪~公元13世纪）：有很多小国依靠收取船只停靠费和关税发家，但后来的事实证明，依靠贸易是无法稳固国家的。</p><p>15）威尼斯共和国与奥斯曼帝国：</p><p>威尼斯共和国（681 CE~1791 CE）：由威尼斯总督领导，擅长水上贸易，商人当道的国家。后灭于拿破仑之手。</p><p>奥斯曼帝国（1299 CE~1922年 CE）：土耳其人创立的国家，1435年消灭东罗马帝国，定都君士坦丁堡，15-19世纪唯一能和欧洲基督国家相提并论的伊斯兰国家，一战战败后分裂。</p><p>16)基辅罗斯~俄罗斯：公元882年建立基辅罗斯，11世纪分裂为多个公国，13世纪20年代被蒙古国征服，此后莫斯科公国开始做大做强。</p><p>17）大航海时代（15世纪末~16世纪初）：</p><p>视频里提到了3个人：郑和、达伽马和哥伦布。</p><p>我们非常熟悉的哥伦布其实终其一生都认为他所到达的地方是…印度，直到意大利人亚美利哥到达南美之后，才认定哥伦布发现的是亚欧之外的新大陆。</p><p>美洲大陆的发现影响巨大，物种交换（猪，牛等动物运往美洲）、疾病的传播（欧洲人给印第安人带去了天花，印第安人也回赠了花柳）。当然其中最臭名昭著的是奴隶贸易。</p><p>你无法把奴隶贸易归罪与任何一个团体、人物或者国家。在那个时代里，每个参与者都是肮脏的，欧洲人把非洲黑奴运往美洲，然而他们并非用武力去获得黑奴，相反，用的是贸易。为他们提供黑奴的也是非洲人，这些黑奴的同胞，或者说，领导者。</p><p>18）文艺复兴（14世纪~16世纪）：一场欧洲思想文化运动。</p><p>这是百度百科的说法，并不错误，但容易误导人。（就像笛卡尔用x，y来刻画横纵坐标导致后来的学生始终无法把思维转化过来一样）事实上文艺复兴是一个时间跨度两百年，而在这两百年内，间断性而非连续性发生的。</p><p>而且是精英运动而没有群众基础，毕竟大部分人还是以填饱肚子为主。当然这个时代各行业都出现太多伟大人物了。</p><p>19）七年战争（1754年~1763年）：主要是欧洲国家参战，英国和法国和西班牙在贸易和殖民地问题上的争夺，普鲁士与奥地利的争权，俄罗斯也意图从中取利。最终，英国与普鲁士的联盟取得了胜利，而这次胜利也为英国赢得了大半个北美，为日不落帝国的诞生打下了基础。</p><p>20）独立战争（1775~1783年）：1773年“波士顿倾茶事件”，来自英国的北美殖民者不满英国国会所颁布的法案所导致的冲突。1775年，莱克星顿，打响了武装反抗英国统治的第一枪，1776年大陆会议通过了《独立宣言》。</p><p>尽管不否认独立战争带来了许多积极因素，比如民主思想的传播等。但独立战争，其实也就是欧洲来的殖民者对英国政府统治的反抗罢了…跟快被灭光的印第安人这些真正的原住民好像也没多大关系，那些被贩卖到美洲的黑奴也没获得多大的权益。</p><p>21）法国大革命（1789年~1794年）：一场初衷无比美好，过程惨不忍睹的革命。</p><p>1774年，路易十六召开三级会议；</p><p>1789年，攻占巴士底狱，不久通过了《人权宣言》，宣扬“人身自由，权利平等”；</p><p>1791年，法国大革命引起普鲁士和奥地利不安，两国联手攻打法国；</p><p>1793年，处死路易十六；</p><p>1794年，热月党处死罗伯斯庇尔；</p><p>1799年，拿破仑雾月政变；</p><p>1815年，拿破仑滑铁卢战败，路易十八复辟波旁王朝；</p><p>1830年，七月革命推翻查理十世，建立七月王朝，法国大革命结束。</p><p>法国大革命彻底推翻了封建专制，但其过程之惨烈也令后世对革命产生了后怕。国内各种政变，欧洲各国为了扼杀这次革命的五次反法战争都为这次披上一层血色的纱布。</p><p>22）海地革命（1790年~1804年）：海地的黑奴以及混血人种反对法国、西班牙殖民统治的革命。值得一提的是，海地革命的成功也粉碎了拿破仑称霸北美的梦想。</p><p>23）拉丁美洲革命（1810年~1826年）：拉丁美洲为了脱离西班牙和葡萄牙而掀起的革命。</p><p>24）工业革命（18世纪60年代~19世纪40年代）：大多数人认为工业革命起源于英格兰，视频中认为工业革命起源于英国，很大程度上是因为</p><p>1、煤矿开采多，导致比较廉价；    </p><p>2、当时英国的工资全球最高；</p><p>25）第一次世界大战（1914年~1918年）：随着帝国主义之间经济发展的不平衡，矛盾也变得越来越尖锐。1914年奥匈帝国皇储斐迪南大公被暗杀，史称萨拉热窝事件。以这一事件为借口，奥匈帝国对塞尔维亚宣战，紧接着沙俄、德、法、英相继各自组队。1918年，《凡尔赛条约》签订，一战结束。</p><p>一战后期，俄国爆发二月革命，推翻了罗曼诺夫王朝，随后又爆发十月革命，最终列宁领导的苏维埃政权控制了局面。</p><p>一战之后，因为欧洲列强都遭受了巨大打击，美国开始展露头脚。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Crash Coursera：&lt;a href=&quot;https://www.bilibili.com/video/av1893599/?p=3&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;十分钟世界史&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;0、农业革命：我们现在所存在的很多问题，都是由农业革命引起的，而这个革命却在不同的文明之间都发生了。尽管很难说清楚，人类不约而同的选择这一步是否正确，但无论如何我们都无法回头了。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Reading" scheme="http://yoursite.com/tags/Reading/"/>
    
  </entry>
  
  <entry>
    <title>其他喜剧形式</title>
    <link href="http://yoursite.com/2019/01/26/%E5%85%B6%E4%BB%96%E5%96%9C%E5%89%A7%E5%BD%A2%E5%BC%8F/"/>
    <id>http://yoursite.com/2019/01/26/其他喜剧形式/</id>
    <published>2019-01-26T11:04:57.000Z</published>
    <updated>2019-03-05T14:51:44.967Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Monologue"><a href="#Monologue" class="headerlink" title="Monologue"></a>Monologue</h2><p>推荐几个写的好的博主：</p><p><a href="https://mp.weixin.qq.com/s/SESfIqN-5RryangHwxtQ7A" target="_blank" rel="noopener">牙签的喜剧课堂-公众号</a></p><p><a href="https://weibo.com/u/3334372892?is_all=1" target="_blank" rel="noopener">七点半的独角戏-微博</a></p><p>单立人喜剧公众号的一周墙报</p><p>Monologue特点：由两句话构成，前半句是真实新闻，后半句是戏谑调侃。</p><p>先从新闻中归纳出重要部分作为铺垫，然后根据你自己的观点（可以和原文的观点相反，比如原文是好消息，你就想出坏消息。但前提是要和铺垫逻辑通顺。）</p><h4 id="一些思考方向"><a href="#一些思考方向" class="headerlink" title="一些思考方向"></a>一些思考方向</h4><p>1、可以往那种“震惊体”方面去思考</p><p>2、可以把不同的新闻相结合，例如：</p><p><img src="https://s2.ax1x.com/2019/03/05/kjUjY9.png" alt=""></p><p>把翟天临博士论文造假和弹幕审核两个新闻写在一起，简直是不(diao)要(de)命(yi)了(b).</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Monologue&quot;&gt;&lt;a href=&quot;#Monologue&quot; class=&quot;headerlink&quot; title=&quot;Monologue&quot;&gt;&lt;/a&gt;Monologue&lt;/h2&gt;&lt;p&gt;推荐几个写的好的博主：&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://mp.wei
      
    
    </summary>
    
    
      <category term="Open-mic" scheme="http://yoursite.com/tags/Open-mic/"/>
    
  </entry>
  
  <entry>
    <title>手把手教你脱口秀2</title>
    <link href="http://yoursite.com/2019/01/26/%E6%89%8B%E6%8A%8A%E6%89%8B%E6%95%99%E4%BD%A0%E8%84%B1%E5%8F%A3%E7%A7%802/"/>
    <id>http://yoursite.com/2019/01/26/手把手教你脱口秀2/</id>
    <published>2019-01-26T01:35:39.000Z</published>
    <updated>2019-01-26T13:30:20.659Z</updated>
    
    <content type="html"><![CDATA[<p><a href="/手把手教你脱口秀1">第一篇文章</a>中提到了如何写一个段子，这篇文章讲如何写一个段落和如何把多个段子组成一个脱口秀演讲稿。</p><h2 id="从搞笑到爆笑"><a href="#从搞笑到爆笑" class="headerlink" title="从搞笑到爆笑"></a>从搞笑到爆笑</h2><p>1、写新段子的时候快速写完，之后再回去打磨。</p><p>2、不要让观众思考，而是让他们直接笑出来。</p><p>3、每个笑点都有一个关键词、短语或动作来揭示再解读，称为”底“。这个底尽量要放在段子最后，并且和其他信息隔开来。</p><a id="more"></a><p>4、三段式结构。</p><p>5、尽量选择常识性的话题，容易引起共鸣。</p><p>6、尽量代入角色。</p><p>7、少用双关梗</p><p>8、角色具体化，比如：“两个人走进酒吧”可以改为”我和我的朋友李小明走进酒吧“。</p><p>9、素材本地化。</p><p>10、联系热点事件</p><p>11、使用语法错误的语言。</p><p>12、自创词汇。</p><p>13、可以适当借助道具。</p><p>14、（重点）<strong>给段子加连续笑点</strong></p><p>（1）、使用原有的目标假设，使用不同的再解读。</p><p>（2）、使用铺垫中不同的连接点来创作不同的笑点。</p><p>（3）、把上一个笑点作为铺垫，去作出下一个笑点。</p><h2 id="如何把零散的笑话组合成脱口秀段落"><a href="#如何把零散的笑话组合成脱口秀段落" class="headerlink" title="如何把零散的笑话组合成脱口秀段落"></a>如何把零散的笑话组合成脱口秀段落</h2><p>未完待续</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;/手把手教你脱口秀1&quot;&gt;第一篇文章&lt;/a&gt;中提到了如何写一个段子，这篇文章讲如何写一个段落和如何把多个段子组成一个脱口秀演讲稿。&lt;/p&gt;
&lt;h2 id=&quot;从搞笑到爆笑&quot;&gt;&lt;a href=&quot;#从搞笑到爆笑&quot; class=&quot;headerlink&quot; title=&quot;从搞笑到爆笑&quot;&gt;&lt;/a&gt;从搞笑到爆笑&lt;/h2&gt;&lt;p&gt;1、写新段子的时候快速写完，之后再回去打磨。&lt;/p&gt;
&lt;p&gt;2、不要让观众思考，而是让他们直接笑出来。&lt;/p&gt;
&lt;p&gt;3、每个笑点都有一个关键词、短语或动作来揭示再解读，称为”底“。这个底尽量要放在段子最后，并且和其他信息隔开来。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Reading" scheme="http://yoursite.com/tags/Reading/"/>
    
      <category term="Open-mic" scheme="http://yoursite.com/tags/Open-mic/"/>
    
  </entry>
  
  <entry>
    <title>hexo+next修改进度条</title>
    <link href="http://yoursite.com/2019/01/19/hexo+next%E4%BF%AE%E6%94%B9%E8%BF%9B%E5%BA%A6%E6%9D%A1/"/>
    <id>http://yoursite.com/2019/01/19/hexo+next修改进度条/</id>
    <published>2019-01-19T03:41:34.000Z</published>
    <updated>2019-01-19T04:26:54.882Z</updated>
    
    <content type="html"><![CDATA[<p>版本：<code>hexo3.8.0</code> 和 <code>next6.0.0</code></p><h2 id="启用进度条"><a href="#启用进度条" class="headerlink" title="启用进度条"></a>启用进度条</h2><p>找到 <code>blog/themes/next/_config.yml</code> 修改</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pace: true         #false改为true，启用进度条</span><br><span class="line">pace_theme: pace-theme-center-circle      #自定义进度条样式，复制上面Themes list中任意一条</span><br></pre></td></tr></table></figure><h2 id="更改进度条颜色"><a href="#更改进度条颜色" class="headerlink" title="更改进度条颜色"></a>更改进度条颜色</h2><p>找到 <code>blog/themes/next/layout/_partials/head.swig</code> </p><p>找到下面代码</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">&#123;% if theme.pace %&#125;</span><br><span class="line">  &#123;% set pace_css_uri = url_for(theme.vendors._internal + &apos;/pace/&apos;+ theme.pace_theme +&apos;.min.css?v=1.0.2&apos;) %&#125;</span><br><span class="line">  &#123;% set pace_js_uri = url_for(theme.vendors._internal + &apos;/pace/pace.min.js?v=1.0.2&apos;) %&#125;</span><br><span class="line">    &#123;% if theme.vendors.pace %&#125;</span><br><span class="line">      &#123;% set pace_js_uri = theme.vendors.pace %&#125;</span><br><span class="line">    &#123;% endif %&#125;</span><br><span class="line">    &#123;% if theme.vendors.pace_css %&#125;</span><br><span class="line">      &#123;% set pace_css_uri = theme.vendors.pace_css %&#125;</span><br><span class="line">    &#123;% endif %&#125;</span><br><span class="line">  &lt;script src=&quot;&#123;&#123; pace_js_uri &#125;&#125;&quot;&gt;&lt;/script&gt;</span><br><span class="line">  &lt;link href=&quot;&#123;&#123; pace_css_uri &#125;&#125;&quot; rel=&quot;stylesheet&quot;&gt;</span><br><span class="line">  </span><br><span class="line">---------------添加以下代码修改进度条颜色----------------------------</span><br><span class="line">&lt;style&gt;</span><br><span class="line">    .pace .pace-progress &#123;</span><br><span class="line">        background: #181f25; /*进度条颜色*/</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&lt;/style&gt;</span><br><span class="line">------------------------------------------------------</span><br><span class="line"></span><br><span class="line">&#123;% endif %&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;版本：&lt;code&gt;hexo3.8.0&lt;/code&gt; 和 &lt;code&gt;next6.0.0&lt;/code&gt;&lt;/p&gt;
&lt;h2 id=&quot;启用进度条&quot;&gt;&lt;a href=&quot;#启用进度条&quot; class=&quot;headerlink&quot; title=&quot;启用进度条&quot;&gt;&lt;/a&gt;启用进度条&lt;/h2&gt;&lt;p&gt;
      
    
    </summary>
    
    
      <category term="hexo" scheme="http://yoursite.com/tags/hexo/"/>
    
      <category term="next" scheme="http://yoursite.com/tags/next/"/>
    
  </entry>
  
  <entry>
    <title>手把手教你脱口秀1</title>
    <link href="http://yoursite.com/2019/01/19/%E6%89%8B%E6%8A%8A%E6%89%8B%E6%95%99%E4%BD%A0%E8%84%B1%E5%8F%A3%E7%A7%801/"/>
    <id>http://yoursite.com/2019/01/19/手把手教你脱口秀1/</id>
    <published>2019-01-19T00:07:05.000Z</published>
    <updated>2019-01-26T01:40:56.930Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>《手把手教你脱口秀》、石老板《单口喜剧表演手册1.0》的总结和自己的思考，里面有些内容为了让我自己更好理解，略有改动。</p></blockquote><h2 id="写在前面"><a href="#写在前面" class="headerlink" title="写在前面"></a>写在前面</h2><p>《手册》的内容更加通俗化，内容偏向于《喜剧圣经》，新手比较难以体会到其中的真谛，可以作为进阶阅读。</p><p>《手把手》是译本，相对而言翻译腔比较重，但是内容更加丰富，后半部还有对表演节奏的讲解。可以作为新手阅读资料。</p><a id="more"></a><h2 id="段子的结构"><a href="#段子的结构" class="headerlink" title="段子的结构"></a>段子的结构</h2><p>一个段子的基本结构：<strong>铺垫+笑点</strong>。</p><p>这是体现在文字上的，对应体现在心理上就是<em>目标假设+意外解读</em>。</p><h4 id="故事一：顺着逻辑"><a href="#故事一：顺着逻辑" class="headerlink" title="故事一：顺着逻辑"></a>故事一：顺着逻辑</h4><p>当你说出你的铺垫的时候，需要能让观众在心里想象出<em>目标假设</em>，从而能够“写出”<strong>故事一</strong>，也就是顺着逻辑下去，我们平时看到的文章就是这样。</p><h4 id="故事二：意外解读"><a href="#故事二：意外解读" class="headerlink" title="故事二：意外解读"></a>故事二：意外解读</h4><p>而要使人发笑，重要的就是用<em>意外解读</em> 来打破目标假设，最后你说出口也就是<strong>故事二</strong>，而笑点就是故事二中打破目标假设的内容，很多时候就是整个故事二。</p><p>由此可见，这需要你对一件事情具有两种不同的解读方式。而具有两种或以上的解读方式的事件就是连接目标假设与意外解读的点，我们称之为<strong>连接点。</strong></p><p>举个例子</p><blockquote><p>我的朋友非常仗义，每当我周转不开的时候，为了照顾我的面子，总是不等我开口，就纷纷表示他们也没钱。</p></blockquote><p>铺垫：我的朋友非常仗义，每当我周转不开的时候，为了照顾我的面子，总是不等我开口。</p><p>目标假设：朋友会主动借钱给我。</p><p>故事一：就纷纷主动对我伸出援手。</p><p>连接点：照顾的我面子。</p><p>意外解读：朋友主动表示自己也没钱。</p><p>故事二：纷纷表示他们也没钱。</p><h2 id="写出铺垫"><a href="#写出铺垫" class="headerlink" title="写出铺垫"></a>写出铺垫</h2><h3 id="Step-1：找出话题（主题、Topic）"><a href="#Step-1：找出话题（主题、Topic）" class="headerlink" title="Step 1：找出话题（主题、Topic）"></a>Step 1：找出话题（主题、Topic）</h3><p>1、你选择的话题需要在观众和你自己的经历之间找到一个<strong>共鸣</strong>。</p><p>比如：”手机“这个话题，你和观众之间都具有很多的共鸣，而”邮局”这个话题你可能有经历，但是没去过邮局的观众很可能就不知道你在说什么。</p><p>2、<strong>细化</strong>你的话题，你的话题越具体，就越可能打动观众。</p><p>例如：“交通”——&gt;”地铁”——“地铁安检”</p><p>3、话题不带有你的观点，但其中一定隐含着“<strong>错误</strong>”。例如：“地铁安检”本身不带有观点，但其中隐含着”不认真的安检”、“插队的乘客”等不应该的事情。</p><p>BTW，《手册》中的说法很有意思，有些人在和朋友交流时非常健谈，但和陌生人却不知道怎么打开话题。这就是因为你和朋友交谈时，你们之间已经有了足够的了解，但是和陌生人之间确缺乏共鸣的点，找到这个共鸣的点，就能开启第一步。</p><h3 id="Step-2：创作一个笑点前提"><a href="#Step-2：创作一个笑点前提" class="headerlink" title="Step 2：创作一个笑点前提"></a>Step 2：创作一个笑点前提</h3><p><em>Q：对于我选取的话题，能否用四个不同的态度（<strong>困难的，奇怪的，可怕的，愚蠢的</strong>）来谈谈我的感受？</em></p><p> 假设在第一步中，我选择了一个大话题“新年”，细化以后为“新年聚餐”。现在为这个话题创作一个笑点前提，方法很容易，就是把这个话题加上<strong>负面态度</strong>。</p><p>例如：</p><p>1、和长辈一起吃饭很糟心。</p><p>2、应酬老板感觉像加班。</p><h3 id="Step-3：创作一个铺垫前提"><a href="#Step-3：创作一个铺垫前提" class="headerlink" title="Step 3：创作一个铺垫前提"></a>Step 3：创作一个铺垫前提</h3><p>观点和笑点前提对立的就是铺垫前提。</p><p>例如：</p><p>1、和长辈一起吃饭很快乐。</p><p>2、老板像家人一样体贴。</p><h3 id="Step-4：选择一个铺垫前提，创作铺垫"><a href="#Step-4：选择一个铺垫前提，创作铺垫" class="headerlink" title="Step 4：选择一个铺垫前提，创作铺垫"></a>Step 4：选择一个铺垫前提，创作铺垫</h3><p>只要想出能表达铺垫前提的例子和说法就行。例如这里选择“2、老板像家人一样体贴。”</p><p>例如：</p><p>1、每到过年时候，老板会把大家聚到一起。</p><h2 id="根据铺垫写笑点（笑话宝藏）"><a href="#根据铺垫写笑点（笑话宝藏）" class="headerlink" title="根据铺垫写笑点（笑话宝藏）"></a>根据铺垫写笑点（笑话宝藏）</h2><p>这里按照《手把手》的顺序，先谈谈如何写笑点。分成以下几个步骤，通过提问的方式进行。</p><p>我们假设已经有了一个铺垫：</p><blockquote><p>每到过年的时候，大家都会聚在一起</p></blockquote><h3 id="Step-1：选择一个铺垫，列出目标假设"><a href="#Step-1：选择一个铺垫，列出目标假设" class="headerlink" title="Step 1：选择一个铺垫，列出目标假设"></a>Step 1：选择一个铺垫，列出目标假设</h3><p><em>Q：对于这个铺垫，我会做出什么样的假设？</em></p><p>A：1、过年聚在一起是传统习俗</p><p>2、大家的关系很好</p><p>3、只有过年的时候大家才有空聚在一起</p><p>4、大家会聚在一起吃年夜饭</p><p>…</p><h3 id="Step-2：选择一个目标假设，找出连接点"><a href="#Step-2：选择一个目标假设，找出连接点" class="headerlink" title="Step 2：选择一个目标假设，找出连接点"></a>Step 2：选择一个目标假设，找出连接点</h3><p>选择目标假设的时候，优先跟着你感觉最容易出笑点的方向，这个比较主观但一般也最有效。</p><blockquote><p>这里以第4条假设为例,因为我觉得容易写出笑点</p></blockquote><p>接着仍然以提问的方式找出连接点：</p><p><em>Q：是什么使我产生这个目标假设的？</em></p><p>A：可以把假设带入铺垫，“每到过年的时候，大家都会聚在一起吃年夜饭。”——这里我作出这个假设的理由是“聚在一起做的事。”</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">这里有个故事一和目标假设之间的关系我还没想通</span><br></pre></td></tr></table></figure><h3 id="Step-3：列出对连接点的再解读"><a href="#Step-3：列出对连接点的再解读" class="headerlink" title="Step 3：列出对连接点的再解读"></a>Step 3：列出对连接点的再解读</h3><p><em>Q：除了目标假设之外，还有什么针对这个连接点的再解读？</em></p><p>这里尽可能的去细化场景，一个很好的方式就是加<strong>形容词</strong>。比如：“她被吓到了。“ 可以改为 “她被一张丑陋的脸吓到了。”</p><p>A：1、聚在一起加班</p><p>2、虽然聚在一起但其实在玩手机</p><p>再解读的意思要来源于铺垫：每到过年的时候，大家都聚在一起加班。（逻辑上要过的去）</p><p>这里选择第1条。（第2条，因为网络上类似的段子太多了，说出来就太弱了）</p><blockquote><p>我写到这里的时候感觉这个目标假设选的并不好，当遇到这种情况的时候可以回到第一步重新选择目标假设，这里就不改了，硬着头皮写。    </p></blockquote><h3 id="Step-4：选择一个再解读，完成故事2"><a href="#Step-4：选择一个再解读，完成故事2" class="headerlink" title="Step 4：选择一个再解读，完成故事2"></a>Step 4：选择一个再解读，完成故事2</h3><p>再解读本身并非一个笑点，他是故事二的<strong>中心概念</strong>，而完成故事二最好的方法就是加细节，同样也可以通过提问的方式。再解读必须符合<strong>笑点前提</strong>的态度。</p><p><em>Q：关于这个铺垫，有什么具体的情境可以解释我的再解读？</em></p><p>A：通过提问的方式给再解读添加细节以完成故事二。</p><p><em>Q1：为什么他们要聚在一起加班？</em></p><p><em>A1：因为老板要求。</em></p><p><em>Q2：老板会怎么要求他们去加班？</em></p><p><em>A2：打着过年团聚的旗号要求他们加班。</em></p><p>…</p><p>给出故事二：</p><p><em>过年的时候，老板说：“同事之间就像家人一样，而公司就是共同的家。”因此，为了让这个大家庭在过年的时候感受团圆的味道，他召集了所有员工加班。</em></p><h3 id="Step-5：-写一个笑点来阐述故事二"><a href="#Step-5：-写一个笑点来阐述故事二" class="headerlink" title="Step 5： 写一个笑点来阐述故事二"></a>Step 5： 写一个笑点来阐述故事二</h3><p><em>Q：在铺垫之余，需要对故事二如何加工，让他变成一个笑点？</em></p><p>A：很遗憾，关于这个问题，并没有一个确切的答案，只能提供一些大体的思路。</p><p>1、故事二是要被叙述还是表演出来？一个段子，观众听起来好笑和读出来好笑是完全不一样的。</p><p>2、笑点尽量简短，英文的笑点叫punchline，这其中的punch描绘的非常精准，就像是给了观众一拳一样，只不过这拳是打在笑穴。</p><p>3、笑点的关键是要让观众同步你的故事二，用说，用肢体语言来表达都行。</p><p>最后的段子：</p><p><em>我的老板对待公司同事总是像家人一样。为了让员工感受到合家团圆般的温暖，过年那天，他召集了所有员工，来公司加班。</em></p><blockquote><p>这里因为先写的笑点后写的铺垫，所以不怎么接的上，我懒得改了，毕竟真的写段子的时候是先写铺垫后写笑点，但是每一步的规则是不变的。</p></blockquote><p>下面是看过的一些我觉得写得很好的文章作为补充：</p><p>1、<a href="https://www.zhihu.com/question/39131741/answer/85691193" target="_blank" rel="noopener">知乎-哈哈哈的回答</a></p><p>回答里把段子分为了“铺垫+误导+爆点”。</p><p>即铺垫中带有<strong>态度</strong>的部分划分为了误导。因为原文中写的比较详细，我就不画蛇添足，直接把原文放在底下。</p><hr><blockquote><p>栗子，（1）女生和女生的交往方式好奇怪，尤其是第一次见面的两个女生，一个女说：“你好萌啊！”另一个女生说：“你好可爱！”于是她俩就成为好朋友了。（2）在看我看来实在是太虚伪了，之前有个女生第一次见我，就对我说“你好可爱啊！” （3）我听后说到“你再给我说一遍！我刚刚没有听清。</p></blockquote><p>这个段子是通过控制节奏和情绪才能产生效果的段子，</p><p>（1）部分要非常严肃的说一件事情，就像开人大会议一样严肃。</p><p>（2）部分属于<strong>我自己观点部分</strong>，这是要将自己的情绪表达出来，主要表现出不屑和看不惯。</p><p>（3）“你再给我说一遍”这说时情绪一定要爆发出来，就像要骂人一样，一种威胁的情绪，最后“我刚刚没有听清”一定要温柔、卖个萌。（如果非表演性质的段子，这里就是笑点）</p><hr>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;《手把手教你脱口秀》、石老板《单口喜剧表演手册1.0》的总结和自己的思考，里面有些内容为了让我自己更好理解，略有改动。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&quot;写在前面&quot;&gt;&lt;a href=&quot;#写在前面&quot; class=&quot;headerlink&quot; title=&quot;写在前面&quot;&gt;&lt;/a&gt;写在前面&lt;/h2&gt;&lt;p&gt;《手册》的内容更加通俗化，内容偏向于《喜剧圣经》，新手比较难以体会到其中的真谛，可以作为进阶阅读。&lt;/p&gt;
&lt;p&gt;《手把手》是译本，相对而言翻译腔比较重，但是内容更加丰富，后半部还有对表演节奏的讲解。可以作为新手阅读资料。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Reading" scheme="http://yoursite.com/tags/Reading/"/>
    
      <category term="Open-mic" scheme="http://yoursite.com/tags/Open-mic/"/>
    
  </entry>
  
</feed>
