<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[机器学习笔记|第十周]]></title>
    <url>%2F2019%2F05%2F06%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E7%AC%AC%E5%8D%81%E5%91%A8%2F</url>
    <content type="text"><![CDATA[大规模机器学习学习大数据假设我们有一个高偏差模型，那么大规模的数据的确能够帮助你拟合更好的模型。但是大规模的数据可能导致算法运行的效率低下。 所以在收集大量数据之前，应该考虑用小样本数据是否也能获得较好的拟合效果。 随机梯度下降法如果我们确定需要使用到大量数据，也可以使用随机梯度法(Stochastic gradient descent)来代替之前提到过的批量梯度下降法(batch gradient descent)。 由图中可知，随机梯度法的代价函数为一个单一训练实例的代价，注意这里取的是一个样本，计算代价： ​ $cost\left( \theta, \left( {x}^{(i)} , {y}^{(i)} \right) \right) = \frac{1}{2}\left( {h}_{\theta}\left({x}^{(i)}\right)-{y}^{ {(i)} } \right)^{2}$ 随机梯度下降的具体步骤： 第一步：随机打乱数据 第二步：Repeat (usually anywhere between1-10){ for $i = 1:m${ ​ $\theta:={\theta}_{j}-\alpha\left( {h}_{\theta}\left({x}^{(i)}\right)-{y}^{(i)} \right){ {x}_{j} }^{(i)}$ ​ (for $j=0:n$) ​ } } 注意这一步中，对比批量梯度下降法的每次迭代都需要计算所有样本，随机梯度下降法不需要每次迭代（求$\theta$）时都对所有数据累加求平方和，这大大地减少了计算量。 在大样本情况下，随机梯度下降法加大了迭代过程的效率，但同样，它的下降过程不再沿着“正确的方向”，如下图。最终可能无法得到精确的最小值点，而是在最小值值点附近，但对于模型的预测准确度不会有太大的影响。 小批量梯度下降 在每次迭代时，使用b个样本累加求和，具体来说： Repeat { for $i = 1:m${ ​ $\theta:={\theta}_{j}-\alpha\frac{1}{b}\sum_\limits{k=i}^{i+b-1}\left( {h}_{\theta}\left({x}^{(k)}\right)-{y}^{(k)} \right){ {x}_{j} }^{(k)}$ ​ (for $j=0:n$) ​ $ i +=10 $ ​ } } 通常b的取值在2-100之间。 随机梯度下降收敛随机梯度下降法还有一个好处就是方便观察优化过程中的收敛情况。 可以看到，批量梯度下降法需要计算完所有的样本才能画出图像，这在大样本的情况下是非常低效的。而随机梯度下降法，只需要每次更新$\theta$之前，计算出代价值。再画出前1000次迭代情况即可。 前两张图是正确的迭代过程，通常而言，选择样本数量越多，观测到的曲线就越平缓。 就最后两张图而言，我们可以得出一个结论，如果你得到的曲线没有体现出收敛的趋势（图三），那么可以尝试增加样本，如果还是没有（粉红色线）。那么可能就是特征、算法方面的问题了。 如果曲线有递增趋势，也就是发散。那么可以尝试使用小一点的学习速率。 关于学习速率，我们通常是选取固定值。但其实也可以使用变量：$\alpha = \frac{const1}{iterationNumber + const2}$，这可以迫使我们的算法收敛而不是在最小值周围徘徊。但是一般不需要为了学习速率浪费计算资源，我们也可以得到很好的效果。 在线学习如果你有一个连续的用户流引起的数据流，那么就可以设计一个在线学习机制，来根据数据流学习用户的偏好。 其实这和之前的随机梯度下降主要区别在于，我们的数据是流动的，算法也在不断的学习而不是只用静态的数据集。 Repeat forever (as long as the website is running) { Get $\left(x,y\right)$ corresponding to the current user $\theta:={\theta}_{j}-\alpha\left( {h}_{\theta}\left({x}\right)-{y} \right){ {x}_{j} }$ (for $j=0:n$) } 一旦(x,y)训练完成，我们就可以抛弃它，因为数据是动态的，还会有新的数据源源不断的输入。 减少映射与数据并行之前我们提到，如果用批量梯度下降法来训练大规模数据是非常耗费时间的，但如果我们能将数据分为多个子集，分给不同的计算机运行，最后再将结果汇总求和，这样的方法就叫做减少映射。 例如我们有400个例子，需要使用批量梯度下降法，我们可以把数据分配给4台计算机处理：]]></content>
      <tags>
        <tag>Machine Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习笔记|第九周]]></title>
    <url>%2F2019%2F05%2F04%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E7%AC%AC%E4%B9%9D%E5%91%A8%2F</url>
    <content type="text"><![CDATA[异常检测异常检测(Anomaly detection)是一种特殊的无监督学习，但其和监督问题有些类似之处。 问题动机从下图可以看出，对训练集建立模型后，输入$x_{test}$，通过概率来判断它是否异常。 异常检测的运用： 欺诈检测 工业生产领域 检测数据中心的计算机 高斯分布就是正态分布，因为其性质和异常检测的性质差不多，由下图可以很明显看出，样本集中的位置同样也是高斯分布中概率（面积）较大的位置。 参数估计要做的则是确定高斯分布的参数，具体而言就是确定$\mu$和$\sigma^{2}$ 均值：$\mu=\frac{1}{m} \sum_{i=1}^{m} x^{(i)}$ 方差：$\sigma^{2}=\frac{1}{m} \sum_{i=1}^{m}\left(x^{(i)}-\mu\right)^{2}$ 算法： 第一步：选取你认为能够指示出异常例子的特征$x_{i}$ 第二步：拟合参数 $\mu_{1}, \dots, \mu_{n}, \sigma_{1}^{2}, \dots, \sigma_{n}^{2}$ 第三步：给定一个新的输入，计算$p(x)=\prod_{j=1}^{n} p\left(x_{j} ; \mu_{j}, \sigma_{j}^{2}\right)=\prod_{j=1}^{n} \frac{1}{\sqrt{2 \pi} \sigma_{j}} \exp \left(-\frac{\left(x_{j}-\mu_{j}\right)^{2}}{2 \sigma_{j}^{2}}\right)$ 也就是根据不同的特征，计算出每个特征的概率，再将所有概率累乘起来。最后如果$p(x)&lt;\varepsilon$，则标注为异常。 以二维特征为例： 开发和评估异常检测系统前面所提到的内容都是在只有正常样本（y=0，normal）的情况下训练的吗，也就是无监督学习。但如果你还有异常样本（y=1，anomalous），则可以用于评估算法的准确性。而且，当为样本选择特征时，如果不确定是否该纳入某一个特征，可以分别在两种情况下进行训练，在验证集上分别评估效果，从而决定是否纳入该特征(模型选择问题)。 假设现在你有一些带标签数据，即含有正常样本和异常样本。可以把数据集划分为训练集、交叉验证集和测试集。 数据集尽量都为正常样本 异常样本放入交叉验证集和测试集中。 举个例子：飞机引擎，10000个正常引擎，20个有瑕疵的引擎。把正常引擎按60%、20%、20%的比例划分为训练集、CV和测试集。同时把瑕疵引擎平均分配到CV和测试集中。 用6000个正常引擎数据拟合出概率分布$p(x)$。 推广到一般情况下 利用训练集来拟合出概率分布p(x) 将CV中的x输入p(x)，设置好阈值$\varepsilon$，从而预测出结果y 根据y，计算TP、FP、FN、TN。准确率和找回率，计算F1分数 调整$\varepsilon$直到找到最高F1的函数作为最优拟合模型 把测试集代入最优拟合模型。 异常检测 vs 监督学习在有标签的情况下，异常检测和监督学习具有相似之处，但仍然有许多明显的差别。从图中可以看出，如果样本中含有大量的负样本（y=0，正常样本），少量正样本（y=1，非正常样本）。那么常使用异常检测；而当正样本数量和负样本数量都很多时，采用监督学习。 在用途上也略有不同： 选择要使用的功能在应用异常检测之前，有一个很重要的是就是选择合适的特征。下面介绍几个常用的方法： 特征不服从高斯分布 如果特征不服从高斯分布，那就需要通过一些转换方法（比如取对数或是开根号）来构造出符合高斯分布的样子。 异常检测误差分析 也可以尝试用有标签的样本，先选择一个比较原始的特征，计算出$p(x)$，在验证集上验证，找到不符合验证结果的样本，观察它的特征，看看能否构建出新的特征，从而能够达到区分的目的。 例如下图中在只有特征$x_{1}$的情况下，很难看出绿色样本是异常样本。 此时对可以对这个绿色样本进行分析，看能不能构建出一个特征$x_{2}$，把这个异常样本给检测出来： 以数据中心的计算机为例： 通常不会选取值特别大或者特别小的特征。 如果突然我怀疑一台计算机陷入了死循环，那么$x_{3}$的数值会变得很大而$x_{4}$的数值则基本不变。那么我可以通过设置其它特征$x_{5}$和$x_{6}$来判断是哪台计算机出现了该问题。 多变量高斯分布对比单元高斯分布，多元高斯分布在处理某些问题上具有优势。 以数据中心的计算机为例： 我们用到CPU负载和内存使用两个特征，如果使用单元高斯分布，则分别对这两个特征训练出p(x)，再相乘得到最终结果。反映在左图中，其概率的范围应该是粉红色圈。 但事实上，我们知道CPU负载和内存的使用之间的关系应该呈现出左图蓝色线圈那样的正线性关系。具体来说，如果此时有一个异常样本点——小绿叉，根据单元高斯分布计算出的结果，小绿叉落在的粉红色线圈范围是概率较高的范围，也就是会被判定为正常样本（从右图也可以看出）。但这显然不符合真实情况。 要解决这个问题，第一种方法是上一节说过的，可以通过构建新的特征，比如这里可以用x3=x1/x2。还有一种方法就是利用多元高斯分布。 多元高斯分布考虑了每两个特征之间可能存在的关系。 而多元高斯分布公式： 其中，粉色圈内的表示行列式。 在多元高斯分布公式中，$\Sigma$表示的是协方差矩阵，衡量的是方差，也就是$x_{1}$和$x_{2}$之间的变化量。其中对角线上元素上表示维度之间的方差。 从下面的图中可以看出，当对角线元素相等时，图像的投影为圆形。而当对角线元素不等时，投影为椭圆。 非对角的元素表示维度之间的协方差，以下是为正值的情况。 如果非对角线的元素为负值： 我们还可以改变$\mu$值，即移动峰值所在位置： 使用多变量高斯分布的异常检测以下是多变量高斯模型的计算过程： 可以看出计算过程和之前的原始模型（单元高斯分布模型）： $p(x)=p\left(x_{1} ; \mu_{1}, \sigma_{1}^{2}\right) \times p\left(x_{2} ; \mu_{2}, \sigma_{2}^{2}\right) \times \cdots \times p\left(x_{n} ; \mu_{n}, \sigma_{n}^{2}\right)$ 有相似的地方。其实只要把多元高斯分布中的$\Sigma$非对角线元素全部改为0，就消除了不同纬度的相关性，得到的就是原始模型了。 原始模型和多变量高斯分布模型比较 多元高斯分布模型的优点在于能够自动找到不同维度之间的关系；而原始模型的优点在于计算效率高，适用于维度小或是样本少的情况。值得注意的是多元高斯分布模型的$\Sigma$必须可逆，所以必须满足m&gt;n（NG建议m&gt;=10n），而且维度之间必须是线性无关的。 推荐系统问题规划后面几个小节都会通过这个预测电影评分的例子来讲解，目的是预测左下角表格中’?’的评分 基于内容的推荐算法假设我们已经知道了衡量电影类型的属性，$x_{1}$和$x_{2}$。再添加一个偏置，即可以用三个特征来表示一个样本，例如《Love at last》的特征就是[1 , 0.9 , 0]。 对于每个样本而言，接下来就需要求出参数$\theta^{(j)}$（单个用户j的参数向量），再通过$\left(\theta^{(j)}\right)^{T} x^{(i)}$即可求出结果。以第一个粉红色圈圈为例，假设我们已知它的参数$\theta^{(1)}$，那么就能预测出评分为4.95。 那么如何求出参数$\theta^{(j)}$呢？ 这相当于一个线性回归问题： 得到优化目标之后，可以使用梯度下降法、BFGS等方法来进行优化。 使用基于内容的推荐算法的前提是，我们已经有了用于描述算法的不同特征，这个例子中，就是用了描述电影成分的$x_{1}$和$x_{2}$。但是大多数情况下，我们不具有这些特征时又应该怎么做呢？ 协同过滤特征学习：学习算法能自动习得所需要的所有特征。这个和上一节中学习参数$\theta$放在一起，有点类似于先有蛋还是先有鸡的问题。 从图中可以得出，需要知道每个用户对于不同类型电影的喜爱程度，从而计算出电影含有爱情的成分$x_{1}$和含有动作$x_{2}$的成分。 对比之前求$\theta$的步骤如下： 那么到底是先计算$\theta$还是先计算$x_{j}$呢？ 简单来说，就是先猜测一个$\theta$值，然后不断迭代，从而使得参数和特征都收敛于某个最终值。 协同过滤算法除了上个例子中的反复计算特征和参数的方法，我们还可以把特征和参数放在同一个优化目标中计算。实际操作就是把前面提到的两个优化目标放在一起，因为他们本质上也是针对那些被用户评分了的电影（即r(i,j)=1）。 值得注意的是，因为协同过滤算法是自动学习参数和特征，所以不需要像以前一样手动去添加偏置项。可以理解为如果算法需要，他会自己去计算出这一项。 矢量化：低秩矩阵分解我们可以把用户对电影的评分的表格记录成矩阵的形式。 而我们的预测矩阵应该和矩阵Y中的值差不多 具体而言，可以把预测矩阵看成是由对每个特征向量转秩后放入矩阵x中，每位用户对某一电影类型的喜好放在矩阵$\Theta$中： 实施细节：均值规范化如果多了一位用户Eve，她还没对任何电影做出过评分。那么应该如何预测她的评分呢？ 如果我们仍然使用之前说过的方法来计算$\theta$和$x$，那么显然式子中的前两项为0，而只需要最后的正则项，又由于正则化项的性质可知（使得参数接近0），在没有数据时，根本不存在过拟合情况。最后一项中$\theta^{(5)}=0$。再根据$\left(\theta^{(5)}\right)^{T} x^{(i)}=0$可以得到上图中我们对Eve评分的预测值全为0，这显然没有意义。 为了解决这个问题，我们引入均值化。 上图中$\mu$为每一行的平均值，再用$Y-\mu$得到右边的矩阵。 这样在预测评分时，只需要把$\mu$加回去即可，既不会影响已经评分过了的用户，也使得还会评分的用户的预测值不会为0。]]></content>
      <tags>
        <tag>Machine Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习|吴恩达机器学习之PCA]]></title>
    <url>%2F2019%2F05%2F02%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8BPCA%2F</url>
    <content type="text"><![CDATA[K-means Clustering导入模块 1234import numpy as npimport matplotlib.pyplot as pltfrom scipy.io import loadmatfrom skimage import io Implementing K-meansFinding closet centroids 读入数据 12data=loadmat('ex7data2.mat')X=data['X'] 为每个样本找到最近的聚类中心 $c^{(i)} :=j \quad$ that minimizes $\quad\left|x^{(i)}-\mu_{j}\right|^{2}$ 1234567891011121314#========================找聚类中心def find_closet_centroids(X,centroids): ''' 返回每个样本所在的cluster索引 ''' idx=[] max_dist=10000 # 给一个距离限定 for i in range(len(X)): minus=X[i]-centroids # minus是3x2的矩阵，每一行代表了第i个样本到一个centroids的x1,x2距离 dist=minus[:,0]**2+minus[:,1]**2 #求范式，即直线距离,dist是3x1的向量 if dist.min()&lt;max_dist: ci=np.argmin(dist) #返回沿axis的最小值索引 idx.append(ci) return np.array(idx) 可以自己初始化一组聚类中心来测试一下 123init_centroids = np.array([[3, 3], [6, 2], [8, 5]])idx = findClosestCentroids(X, init_centroids)print(idx[0:3]) 结果应该是 1[0 2 1] Computing centroids means12345678910#=========================移动聚类中心def compute_centroids(X,idx): centroids=[] for i in range(len(np.unique(idx))): # 布尔索引，idx==i运算返回bool value，根据值为true的下标来输出X中对应元素值 u_k=X[idx==i].mean(axis=0) centroids.append(u_k) return np.array(centroids) K-means on example dataset 画图函数 12345678910111213141516171819202122232425262728293031323334def plot_data(X,centroids,idx=None): colors = ['b','g','gold','darkorange','salmon','olivedrab', 'maroon', 'navy', 'sienna', 'tomato', 'lightgray', 'gainsboro' 'coral', 'aliceblue', 'dimgray', 'mintcream', 'mintcream'] # 这里centroids需要从ndarray转成list,但centroids[0]仍为ndarray. assert len(centroids[0])&lt;=len(colors),'colors not enough' subX=[] if idx is not None: for i in range(centroids[0].shape[0]): #分成几类就循环几次 x_i=X[idx==i] subX.append(x_i) #把数据按cluster分成不同下标的元素，存储在subx这个list中 else: subX=[X] fig=plt.figure(figsize=(8,5)) for i in range(len(subX)): xx=subX[i] plt.scatter(xx[:,0],xx[:,1],c=colors[i]) plt.xlabel('x') plt.ylabel('y') plt.title('Plot of X Points') xx,yy=[],[] for centroid in centroids: xx.append(centroid[:,0]) yy.append(centroid[:,1]) plt.plot(xx,yy,'rx--') plt.show() 画出聚类中心移动过程 1234567891011#=============================聚类中心移动过程def run_kmeans(X,centroids,max_iters): centroids_all=[] centroids_all.append(centroids) centroid_i=centroids for i in range(max_iters): idx=find_closet_centroids(X,centroid_i) centroid_i=compute_centroids(X,idx) centroids_all.append(centroid_i) #每次移动后的聚类中心坐标都记录下来 return idx,centroids_all 结果： Random initialization关于聚类中心的初始化，一个更好的方法是从样本集中随机选取。 123456#==============================随机初始化聚类中心def random_centroids(X,K): m=X.shape[0] index=np.random.choice(m,K) #在0~m中随机生成K个样本 return X[index] 最后再尝试一下聚类算法： 12345for i in range(3): init_centroids=random_centroids(X,3) idx,centroids_all=run_kmeans(X,init_centroids,20) plot_data(X,centroids_all,idx=idx) #idx为每个样本所在cluster的索引 得到最终结果： Image compression with K-means 导入数据 1234img=io.imread('bird_small.png')plt.imshow(img)plt.show()print(img.shape) 图像为 1(128,128,3) 可以看到图像以3维矩阵的方式存储，前面两个维度代表图像的像素点个数128x128，最后一个维度代表像素点由RGB三个通道表示。又因为一个通道占用8-bit，因此在原始的图像中一个像素点需要24-bit来储存。 在这幅图中包含了上千种种颜色，而在这个实验中，我们把颜色降为16种，也就是说，一像素点只需要4bit就足够了。 1234567891011121314151617img=img/255X=img.reshape(-1,3) #转换成128x128行，3列为RGB三通道K=16 #16个聚类中心，就是把所有颜色压缩为16种RGB颜色，那么每个像素值需要4bit存储即可init_centroids=random_centroids(X,K)idx,centroids_all=run_kmeans(X,init_centroids,10)img_2=np.zeros(X.shape)centroids=centroids_all[-1] # 只需要记录聚类中心最后移动的位置即可for i in range(len(centroids)): img_2[idx==i]=centroids[i]img_2=img_2.reshape((128,128,3))fig,axes=plt.subplots(1,2,figsize=(12,6))axes[0].imshow(img)axes[1].imshow(img_2)plt.show() 比对一下压缩效果： Principle Component Analysis 导入模块 123import numpy as npimport matplotlib.pyplot as pltfrom scipy.io import loadmat Example Dataset 导入数据 1234data = loadmat('ex7data1.mat')X = data['X']print(X.shape)plt.scatter(X[:,0], X[:,1], facecolors='none', edgecolors='b') 1(50,2) 显示图像： Implementing PCA 关于PCA的数学原理可以参考：浅谈SVD的数学原理及应用 PCA主要分为两个计算步骤： 计算数据的协方差矩阵：$\Sigma=\frac{1}{m} \sum_{i=1}^{n}\left(x^{(i)}\right)\left(x^{(i)}\right)^{T}$ 用SVD函数进行奇异值分解 在开始上面的步骤之前，需要先对数据进行特征缩放和归一化： 123456def feature_normalize(X): means = X.mean(axis=0) stds = X.std(axis=0, ddof=1) X_norm = (X - means) / stds return X_norm, means PCA： 123456#===================================PCAdef pca(X): sigma = (1 / len(X)) * (X.T @ X) #求出协方差矩阵 U, S, V = np.linalg.svd(sigma) #奇异值分解 return U, S, V 可视化主成成分： 12345678910111213def plot_reduce(means, U, S): plt.plot([means[0], means[0] + 1.5 * S[0] * U[0, 0]], [means[1], means[1] + 1.5 * S[0] * U[0, 1]], c='r', linewidth=3, label='First Principle Component') plt.plot([means[0], means[0] + 1.5 * S[1] * U[1, 0]], [means[1], means[1] + 1.5 * S[1] * U[1, 1]], c='g', linewidth=3, label='Second Principal Component') plt.axis('equal') plt.legend() 得到结果： Dimensionality Reduction with PCAProjecting the data onto the principal components1234567'''返回值 - Z：投影到主成成分后的样本点'''def project_data(X, U, K): Z = X @ U[:, 0:K] return Z 123K = 1Z = project_data(X_norm, U, K)print(Z[0]) 得到结果： 1array([1.48127391]) Reconstructing an approximation of the data12345#===================================重建数据def recover_data(Z, U, K): X_rec = Z @ U[:, 0:K].T return X_rec 1234# you will recover an approximation of the first example and you should see a value of# about [-1.047 -1.047].X_rec = recover_data(Z, U, 1)X_rec[0] 1array([-1.04741883, -1.04741883]) Visualizing the projections1234567891011121314151617181920212223242526272829#====================================PCA样本投影可视化def visualize_data(X_norm, X_rec): fig, ax = plt.subplots(figsize=(8, 5)) plt.axis('equal') plt.scatter( X_norm[:, 0], X_norm[:, 1], s=30, facecolors='none', edgecolors='blue', label='Original Data Points') plt.scatter( X_rec[:, 0], X_rec[:, 1], s=30, facecolors='none', edgecolors='red', label='PCA Reduced Data Points') plt.title("Example Dataset: Reduced Dimension Points Shown",fontsize=14) plt.xlabel('x1 [Feature Normalized]',fontsize=14) plt.ylabel('x2 [Feature Normalized]',fontsize=14) for x in range(X_norm.shape[0]): plt.plot([X_norm[x,0],X_rec[x,0]],[X_norm[x,1],X_rec[x,1]],'k--') plt.legend(loc=0) # 输入第一项全是X坐标，第二项都是Y坐标 plt.show() 结果如图： Face Image Dataset 导入数据 12data_2=loadmat('ex7faces.mat')X_2=data_2['X'] 数据可视化 12345678def plot_face(X,row,col): fig,ax=plt.subplots(row,col,figsize=(8,8)) for i in range(row): for j in range(col): ax[i][j].imshow(X[i*col+j].reshape(32,32).T,cmap='Greys_r') ax[i][j].set_xticks([]) ax[i][j].set_yticks([]) plt.show() 结果如图： PCA on Faces12X_2_norm,means_2=feature_normalize(X_2)U_2,S_2,V_2=pca(X_2_norm) 显示主成成分 1plot_face(U[:,:36].T, 6, 6) 结果如下： Dimensionality Reduction把图片降维 123Z_2=project_data(X_2_norm,U_2,K=36)X_2_rec=recover_data(Z_2,U_2,K=36)plot_face(X_2_rec,10,10) 结果：]]></content>
      <tags>
        <tag>Machine Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[浅谈SVD的数学原理及应用]]></title>
    <url>%2F2019%2F05%2F01%2F%E6%B5%85%E8%B0%88SVD%E7%9A%84%E6%95%B0%E5%AD%A6%E5%8E%9F%E7%90%86%E5%8F%8A%E5%BA%94%E7%94%A8%2F</url>
    <content type="text"><![CDATA[在做PCA的实验时，遇到的SVD奇异值分解问题，这里记录自己直观的理解 通常在python中我们都是直接调用numpy模块中的函数，那么返回的U,S,V几个值代表什么意思呢？ 1U, S, V = np.linalg.svd(X) 遇事不明，手册先行：官方手册 手册中简单说明了其含义： U：酉矩阵 S：奇异值向量（数学上是奇异值矩阵，但因为这个矩阵类似于对角阵，只有主对角线上的元素非0，程序对其进行了处理，直接提取了对角元素），且元素按降序排列。 V：酉矩阵 首先给出SVD分解公式：$W=U \Sigma V^{T}$ 如果你看的一脸懵逼，没关系，这只能说明你和我一样，线性代数没怎么学好。所以接下来简单解释一下其背后的数学原理，希望有帮助。 举一个具体分解的例子： 有这么一个矩阵：$W=\left[ \begin{array}{ll}{1} &amp; {1} \\ {0} &amp; {1} \\ {1} &amp; {0}\end{array}\right]$我们要如何把它SVD分解？ step-1.1 计算第一个对角矩阵，$C=W^{T} W=\left[ \begin{array}{lll}{1} &amp; {0} &amp; {1} \\ {1} &amp; {1} &amp; {0}\end{array}\right] \left[ \begin{array}{ll}{1} &amp; {1} \\ {0} &amp; {1} \\ {1} &amp; {0}\end{array}\right]=\left[ \begin{array}{ll}{2} &amp; {1} \\ {1} &amp; {2}\end{array}\right]$，注意得到的是方阵。 step-1.2 接着对这个对角矩阵，求它的特征值$\lambda_{1}=3, \lambda_{2}=1$和特征向量$\vec{v}_{1}=\left[ \begin{array}{l}{\frac{1}{\sqrt{2}}} \\ {\frac{1}{\sqrt{2}}}\end{array}\right], \vec{v}_{2}=\left[ \begin{array}{l}{\frac{1}{\sqrt{2}}} \\ {\frac{-1}{\sqrt{2}}}\end{array}\right]$。 step-2.1 计算第二个对角矩阵，$B=W W^{T}=\left[ \begin{array}{ll}{1} &amp; {1} \\ {0} &amp; {1} \\ {1} &amp; {0}\end{array}\right] \left[ \begin{array}{lll}{1} &amp; {0} &amp; {1} \\ {1} &amp; {1} &amp; {0}\end{array}\right]=\left[ \begin{array}{lll}{2} &amp; {1} &amp; {1} \\ {1} &amp; {1} &amp; {0} \\ {1} &amp; {0} &amp; {1}\end{array}\right]$，和step-1.1比起来，能看到区别在于$W^{T}W$和$WW^{T}$。 step-2.2 同样，求它的特征值$\lambda_{1}=3, \lambda_{2}=1, \lambda_{3}=0$和特征向量$\vec{u}_{1}=\left[ \begin{array}{c}{\frac{2}{\sqrt{6} } } \\ {\frac{1}{\sqrt{6} } } \\ {\frac{1}{\sqrt{6} } }\end{array}\right], \vec{u}_{2}=\left[ \begin{array}{c}{0} \\ {\frac{-1}{\sqrt{2} } } \\ {\frac{1}{\sqrt{2} } }\end{array}\right], \vec{u}_{3}=\left[ \begin{array}{c}{\frac{-1}{\sqrt{3} } } \\ {\frac{1}{\sqrt{3} } } \\ {\frac{1}{\sqrt{3} } }\end{array}\right]$。 最终我们根据公式：$W=U \Sigma V^{T}$，就可以得到 $W=\left[ \begin{array}{ll}{1} &amp; {1} \\ {0} &amp; {1} \\ {1} &amp; {0}\end{array}\right]=\left[ \begin{array}{ccc}{\frac{2}{\sqrt{6}}} &amp; {0} &amp; {-\frac{1}{\sqrt{3}}} \\ {\frac{1}{\sqrt{6}}} &amp; {\frac{-1}{\sqrt{2}}} &amp; {\frac{1}{\sqrt{3}}} \\ {\frac{1}{\sqrt{6}}} &amp; {\frac{1}{\sqrt{2}}} &amp; {\frac{1}{\sqrt{3}}}\end{array}\right] \left[ \begin{array}{cc}{\sqrt{3}} &amp; {0} \\ {0} &amp; {1} \\ {0} &amp; {0}\end{array}\right] \left[ \begin{array}{cc}{\frac{1}{\sqrt{2}}} &amp; {\frac{1}{\sqrt{2}}} \\ {\frac{1}{\sqrt{2}}} &amp; {\frac{-1}{\sqrt{2}}}\end{array}\right]$ 其中$U$就是$B$的特征向量拼成的酉矩阵，$V^{T}$就是$C$的特征向量拼成的酉矩阵。而$\Sigma$(即S)就是由特征值开根号得到的。 我们还可以把上面的矩阵写成另外的表达方式： $W=\sqrt{3} \left[ \begin{array}{c}{\frac{2}{\sqrt{6}}} \\ {\frac{1}{\sqrt{6}}} \\ {\frac{1}{\sqrt{6}}}\end{array}\right] \left[ \begin{array}{cc}{\frac{1}{\sqrt{2}}} &amp; {\frac{1}{\sqrt{2}}}\end{array}\right]+\left[ \begin{array}{c}{0} \\ {-\frac{1}{\sqrt{2}}} \\ {\frac{1}{\sqrt{2}}}\end{array}\right] \left[ \begin{array}{cc}{\frac{1}{\sqrt{2}}} &amp; {\frac{-1}{\sqrt{2}}}\end{array}\right]$ 也就是$W=\sqrt\lambda_{1}\vec{u}_{1}\left(\vec{v}_{1}\right)^{T}+\sqrt\lambda_{2}\vec{u}_{2}\left(\vec{v}_{2}\right)^{T}$。再说一次，奇异值的平方=特征值。 这样也就可以解释为什么SVD会被用于PCA，实际上我们只是举了很简单的例子，当矩阵的维数更高时，我们最后得到的式子的项数也就越多，但是有些项数的奇异值很小，舍去这些项虽然会丢失局部细节，但是能够在尽量保真的情况下节约存储空间。 参考资料： [1] 机器学习中的数学(5)-强大的矩阵奇异值分解(SVD)及其应用 [2] 【简化数据】奇异值分解(SVD) [3] 矩阵分析之奇异值分解（SVD）]]></content>
      <tags>
        <tag>Machine Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习笔记-第八周]]></title>
    <url>%2F2019%2F04%2F29%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E7%AC%AC%E5%85%AB%E5%91%A8%2F</url>
    <content type="text"><![CDATA[无监督学习无监督学习之间学习的都是监督学习，也就是样本都有标签。 而无监督学习的样本是没有标签的，也就是只有输入x，没有输出标记。 无监督学习就是找到隐含在这类无标签数据中的结构，这类算法称为聚类算法（clustering algorithm）。 K-Means算法k均值算法，先随机生成两个数据，即聚类中心（cluster centroids）。 然后遍历所有数据，和蓝色聚类中心靠近的，就渲染为蓝色；和红色聚类中心靠近的，就渲染为红色。然后移动聚类中心到同颜色点的均值处。 重复上述步骤，直到聚类完成，聚类中心不再变化，样本的分类也不再变化。 通过图像我们可以直观的了解到，使用K均值算法聚类，输入需要样本特征$x^{(i)}$和聚类中心数K。 具体步骤如下： 循环1是根据聚类中心分类，循环2是根据平均值移动聚类中心。其中$c^{(i)}$表示的是$x^{(i)}$到不同聚类中心，距离最小的那个的聚类中心的索引值。 有时候数据的分类不那么明确，比如T-shirt的尺寸。 同样也可以用K均值算法进行聚类。将T-shirt的尺寸根据身高和体重，分类为S、M、L。 优化目标K均值算法同样有优化目标，或者说代价函数（这里也叫畸变函数）。 代价函数就是样本到聚类中心距离的平方和，而优化目标就是找到代价函数值最小时的$c^{(i)}$和聚类中心$\mu_{k}$，从下图也可以看出两个循环就是为了得到它们。 随机初始化在实际运用中，一般来说，都是随机选取K个样本点作为聚类中心而不是像上面那样选择非样本点。当然，K的值要小于m。 通常我们都期望随机选取的聚类中心具有比较好的性质 但人生不如意之事十之八九，有时候选择的聚类中心不理想 就会在训练的过程中，导致局部最优解，如下图中右下角那样的结果。 解决这个问题的方法简单粗暴，就是多次随机初始化聚类中心，得到多个拟合结果，然后挑选出代价最小的分类方法 。 选取聚类数量关于如何确定K值，是一个很难回答的问题，甚至可以说没有确切的答案。但通常我们会采用以下的方法来帮助确定K的大小。 肘部法则（Elbow method） 但有些时候，得到的图像看起来像是连续的，不容易判断“肘部“位置。 所以这个方法没办法适用于所有情况，所以通常推荐的方法是：根据你聚类的目的来确定K的数目（感觉是废话）。 降维(Dimensionality Reduction)除了聚类之外，还有另一种无监督学习算法，叫做降维。 目标I:数据压缩降维的作用是消除特征冗余，如下图所示，特征$x_{1}$代表cm而特征$x_{2}$代表inches，含义相同，在图像上呈现出线性关系，完全可以只用一个特征来表示。 更一般的，我们可以把两个冗余的特征用一个新的特征来表示，记为$z_{1}$，如下图。从另一个角度来看，也可以认为是把所有数据投影到了绿色的那条线上。 当然，很多时候冗余的特征不只两个，但原理是一样的，比如以3维为例： 目标II：可视化特征很多的情况下没办法直接可视化。 这时候需要用到降维。 降维之后的特征可能不具有物理意义，但方便画图。 主成分分析问题规划I主成分分析问题规划（Principle Component Analysis），是目前最流行的降维算法。举个例子简单说明原理： 图为从2维降到1维，PCA要求样本到投影点的距离的（投影误差）平方和最小。 更一般的情况，从n-D投影到k-D，也就是找到k个向量$u^{(1)},u^{(2)},…,u^{(3)}$作为被投影的线性子空间。再用3D投影到2D为例，记住PCA关键是投影误差要最小 PCA和线性回归的区别 二者毫无联系，虽然有时看上去图像是一样的，但线性回归时预测输出变量，它的代价函数表示的是拟合模型的输出误差，如图左；PCA的代价函数则是投影误差，如图右。 主成分分析问题规划II 特征缩放和均值归一化 求出$u^{(i)}$ 把数据从n维降到k维。 计算协方差矩阵： 其中$x^{(i)}$是nx1维，所以$\Sigma$是nxn维。写成向量形式：$\Sigma=1 / m\left(X^{T} X\right)$ 再用svd对协方差矩阵进行奇异值分解： 最后我们需要的是U矩阵（nxn），取它的k个列向量构成矩阵z，再乘上$x^{(i)}$就可以得到$z^{(i)}$。 主成分数量选择之前提到过如何选择压缩后的维度k是很困难的，但在PCA中有一种很实用的方法，这一节展开说说。 通常来说，选择的k要让平均投影误差/数据总方差小于0.01，这种情况也称为“99%的方差被保留”，这句话代表了投影降维后的效果好。 从上面的公式可以看出，如果需要选择出合适的K，我们可以尝试不同的K值，直到找到能让公式小于0.01为止。 但这种方法显然效率不高，因此最好使用另一种方法。 之前提到的奇异值分解：$[U,S,V]=svd(Sigma)$，其中的矩阵S是一个对角矩阵，如下图所示，取主对角线中的k个值，满足$1-\frac{\sum_{i=1}^{k} S_{i i} }{\sum_{i=1}^{k} S_{ii} } \leqslant 0.01$即可。 压缩重现如果能从n维降至k维，同理也就能从k维升至n维。只需要把$z^{(i)}$代入$X_{appox} $=$ U_{radue} \cdot z^{(i)}$。就能求出x近似值。 应用PCA的建议PCA也可以应用在监督学习算法中，只需要将$x^{(i)}$提取出来即可。另外必须要注意的是，把$x^{(i)}$映射到$z^{(i)}$（即求出U）的过程中只能在训练集中运行。 PCA的应用 （1）压缩数据，加快算法运行效率 （2）可视化：没得办法，我们只能可视化K&lt;3的数据 （3）不要用PCA来防止过拟合，因为PCA不使用标签y，这可能会导致信息的丢失。用正则化就行了，不要皮。 （4）在使用PCA之前，应该先考虑清楚是否真的有必要使用它，直接用原始数据也许就能得出想要的答案。]]></content>
      <tags>
        <tag>Machine Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习-吴恩达机器学习之支持向量机]]></title>
    <url>%2F2019%2F04%2F29%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%2F</url>
    <content type="text"><![CDATA[Support Vector Machine导入库函数 12345import numpy as npimport pandas as pdimport matplotlib.pyplot as pltfrom scipy.io import loadmatfrom sklearn import svm Example Dataset 1画出含有两个特征的样本集的线性边界函数。 数据处理123data=loadmat('ex6data1.mat')X=data['X']y=data['y'].flatten() 不知道标签的话可以查看 1print(data.keys()) # 用于查看标签名称 将数据可视化以便观察，注意要把正负样本分开可视化。 12345678#==================================观察数据def plot_data(X,y): positive=X[y==1] negative=X[y==0] fig,ax=plt.subplots(figsize=(8,5)) plt.scatter(positive[:,0],positive[:,1],marker='+',label='positive') plt.scatter(negative[:,0],negative[:,1],color='red',label='negative') SVM拟合可以通过改变c的大小来观察决策边界的变换。C的大小影响着模型对异常样本的反应。 因为这里是线性分类，所以使用线性核函数，参数kernel=&#39;linear&#39;。使用sklearn中的svm.SVC()来拟合，其结果返回一个分类器对象。最后还需要用clf.fit(X,y)来拟合出最终模型。 123c=1clf=svm.SVC(c,kernel='linear') # 参数 c,kernel 返回一个分类器对象clf.fit(X,y) # 用训练数据拟合分类器模型 可视化决策边界np.meshgrid()生成网格点，再对每个网格点进行预测，最后画出等高线图，即决策边界。 关于np.meshgrid()可以参考 Numpy中的Meshgrid。 12345678910#=============================================可视化决策边界def visualize_boundary(clf,X): x_min,x_max=X[:,0].min()*1.2,X[:,0].max()*1.1 y_min,y_max=X[:,1].min()*1.2,X[:,1].max()*1.1 xx,yy=np.meshgrid(np.arange(x_min,x_max,0.02),np.arange(y_min,y_max,0.02)) # 画网格点 Z=clf.predict(np.c_[xx.ravel(),yy.ravel()]) # 用训练好的分类器对网格点进行预测 Z=Z.reshape(xx.shape) # 转换成对应的网格点 plt.contour(xx,yy,Z,level=[0],colors='black') # 等高线图，画出0/1分界线 plt.show() 像前面提到的，不同的C值对决策边界会产生不同影响 c=1时： c=1000时： 注意左上角的正样本，c较大时，决策边界会过于追求将数据正确分类，而失去大间距的特点。 SVM with Gaussion KernelsGaussion Kernel用SVM做分线性分类，我们需要用到高斯核函数。 公式： 代码： 123#=============================================高斯核函数def gaussKernel(x1,x2,sigma): return np.exp(-((x1-x2)**2).sum()/(2*sigma**2)) Example Dataset 2 数据处理及可视化 123data2=loadmat('ex6data2.mat')X2=data2['X']y2=data2['y'].flatten() 用高斯核函数拟合模型 1234sigma=0.1gamma=np.power(sigma,-2)/2clf=svm.SVC(c,kernel='rbf',gamma=gamma) # 注意这里的参数gamma是整个分母，且要写成乘法形式clf.fit(X2,y2) kernel=&#39;rbf&#39;代表使用高斯核函数，其中gamma值就是公式中的整个分母项，即$\frac{1}{2\sigma^{2} }$。 决策边界 12plot_data(X2,y2)visualize_boundary(clf,X2) Example Dataset 3 数据处理即可视化 123data3=loadmat('ex6data3.mat')X3=data3['X']y3=data3['y'].flatten() 可以发现，有个别样本存在比较大的差异。 使用带高斯核函数的SVM进行训练 12clf=svm.SVC(c,kernel='rbf',gamma=gamma)clf.fit(X3,y3) 决策边界 12plot_data(X3,y3)visualize_boundary(clf,X3) Spam Classification建立一个垃圾邮件分类器，下面这个例子将会告诉你如何通过一封邮件来建立特征向量。 导入库函数 123456789import numpy as npimport matplotlib.pyplot as pltimport pandas as pdfrom scipy.io import loadmatfrom sklearn import svmimport re # 电子邮件处理的正则表达式# 一个英文分词算法(Poter stemmer)import nltk, nltk.stem.porter Preprocesssing Email 读取数据 12with open('emailSample1.txt','r') as f: email=f.read() 打印邮件内容为 123456789&gt; Anyone knows how much it costs to host a web portal ?&gt;Well, it depends on how many visitors you&apos;re expecting.This can be anywhere from less than 10 bucks a month to a couple of $100. You should checkout http://www.rackspace.com/ or perhaps Amazon EC2 if youre running something big..To unsubscribe yourself from this mailing list, send an email to:groupname-unsubscribe@egroups.com 一般邮件都具有一些相似的内容，比如数字、URL、其它邮件地址。因此我们会采取一些”标准化“的方法来处理邮件，这些方法会提高垃圾邮件分类的性能。 例如： 123456781. Lower-casing: 把整封邮件转化为小写。2. Stripping HTML: 移除所有HTML标签，只保留内容。3. Normalizing URLs: 将所有的URL替换为字符串 “httpaddr”.4. Normalizing Email Addresses: 所有的地址替换为 “emailaddr”5. Normalizing Dollars: 所有dollar符号($)替换为“dollar”.6. Normalizing Numbers: 所有数字替换为“number”7. Word Stemming(词干提取): 将所有单词还原为词源。例如，“discount”, “discounts”, “discounted” and “discounting”都替换为“discount”。8. Removal of non-words: 移除所有非文字类型，所有的空格(tabs, newlines, spaces)调整为一个空格. 邮件内容处理： 1234567891011def process_email(email): """做除了Word Stemming和Removal of non-words的所有处理""" email = email.lower() email = re.sub('&lt;[^&lt;&gt;]&gt;', ' ', email) # 匹配&lt;开头，然后所有不是&lt; ,&gt; 的内容，知道&gt;结尾，相当于匹配&lt;...&gt; email = re.sub('(http|https)://[^\s]*', 'httpaddr', email ) # 匹配//后面不是空白字符的内容，遇到空白字符则停止 email = re.sub('[^\s]+@[^\s]+', 'emailaddr', email) email = re.sub('[\$]+', 'dollar', email) email = re.sub('[\d]+', 'number', email) return email 再接下来提取词干，去除非字符内容，并返回一个单词列表。 123456789101112131415161718192021222324def email2TokenList(email): """预处理数据，返回一个干净的单词列表""" # I'll use the NLTK stemmer because it more accurately duplicates the # performance of the OCTAVE implementation in the assignment stemmer = nltk.stem.porter.PorterStemmer() email = process_email(email) # 将邮件分割为单个单词，re.split() 可以设置多种分隔符 tokens = re.split('[ \@\$\/\#\.\-\:\&amp;\*\+\=\[\]\?\!\(\)\&#123;\&#125;\,\'\"\&gt;\_\&lt;\;\%]', email) # 遍历每个分割出来的内容 tokenlist = [] for token in tokens: # 删除任何非字母数字的字符 token = re.sub('[^a-zA-Z0-9]', '', token); # Use the Porter stemmer to 提取词根 stemmed = stemmer.stem(token) # 去除空字符串‘’，里面不含任何字符 if not len(token): continue tokenlist.append(stemmed) return tokenlist Vocabulary List我们得到了邮件的单词列表，接下来需要结合记录实际中经常使用到的单词的词汇表vocab.txt。函数返回邮件单词在词汇表中的索引值。 12345def email2VocabIndices(email, vocab): """提取存在单词的索引""" token = email2TokenList(email) index = [i for i in range(len(vocab)) if vocab[i] in token] return index 得到的索引值如下： Extracting Feature from Emails如果邮件中的单词出现在词汇表的第i个位置，则把特征向量的第i个索引值置为1,；如果没出现，置为0。也就是说，建立一个和词汇表同维度的向量feature，再把上面得到的索引位置的值改写为1，其余为0。可以得到： 123456789def email_feature_vector(email): '''将email的单词转换为特征向量0/1''' df = pd.read_table('vocab.txt', names=['words']) vocab = df.values # Datafram转换为ndarray vector = np.zeros(len(vocab)) vecab_indices = email2VocabIndices(email, vocab) for i in vecab_indices: vector[i] = 1 return vector 该向量长度为1899，其中有45个索引值为1。 Training SVM for Spam Classification用已经预处理过的训练集和测试集来拟合模型，并计算准确度。 123clf = svm.SVC(C=0.1, kernel='linear')clf.fit(X, y)print(clf.score(X,y),clf.score(Xtest,ytest)) 打印结果： 10.99825 0.989 Top predictors for Spam返回权重最大的15个单词，这些单词出现频率高的邮件就是垃圾邮件。 123456789def get_vocab_list(): '''以字典形式获得词汇表''' vocab_dict = &#123;&#125; with open('vocab.txt') as f: #打开txt格式的词汇表 for line in f: (val, key) = line.split() #读取每一行的键和值 vocab_dict[int(val)] = key #存放到字典中 return vocab_dict 获取词汇表 123456vocab_list = get_vocab_list() #得到词汇表 存在字典中indices = np.argsort(clf.coef_).flatten()[::-1] #对权重序号进行从大到小排序 并返回for i in range(15): #打印权重最大的前15个词 及其对应的权重 print('&#123;&#125; (&#123;:0.6f&#125;)'.format(vocab_list[indices[i]], clf.coef_.flatten()[indices[i]])) 最终打印结果: 123456789101112131415otherwis (0.500614)clearli (0.465916)remot (0.422869)gt (0.383622)visa (0.367710)base (0.345064)doesn (0.323632)wife (0.269724)previous (0.267298)player (0.261169)mortgag (0.257298)natur (0.253941)ll (0.253467)futur (0.248297)hot (0.246404) 参考资料[1] 机器学习 | 吴恩达机器学习第七周编程作业(Python版) [2] 吴恩达机器学习作业Python实现(六)：SVM支持向量机 这次的代码几乎完全照抄两位大神的，实在是正则化的内容不懂，而因为用了sklearn库使得拟合变得又很简单。记录一下自己的naive，还得努力。]]></content>
      <tags>
        <tag>Machine Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[numpy中向量的表示方法]]></title>
    <url>%2F2019%2F04%2F27%2Fnumpy%E4%B8%AD%E5%90%91%E9%87%8F%E7%9A%84%E8%A1%A8%E7%A4%BA%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[行向量1np.array([[1,2,3]]) 结果: 1[[1,2,3]] .shape为（1,3）。是一个一行三列的行向量。 数组1np.array([1,2,3]) 结果： 1[1,2,3] .shape为（3，）。是一个ndarray数组，严格来说并不是向量，注意和行向量区分。 列向量1np.array([[1],[2],[3]]) 结果： 123[[1], [2], [3]] .shape为（3,1）。是一个三行一列的列向量。]]></content>
      <tags>
        <tag>Python</tag>
        <tag>Numpy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习笔记|第七周]]></title>
    <url>%2F2019%2F04%2F26%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E7%AC%AC%E4%B8%83%E5%91%A8%2F</url>
    <content type="text"><![CDATA[支持向量机优化目标在监督学习中，重要的不是你选择的算法，而是应用这些算法时，选择的特征、正则化参数等等诸如此类。其中有一种非常强大的分类工具，称为支持向量机。（Support Vector Machine，SVM） 下面展示如何通过修改逻辑回归，来得到SVM。 首先回顾一下逻辑回归假设函数，注意y=1和y=0的情况。 进一步写出代价函数 当y=1时，此时代价函数剩左边一项$-y \log \frac{1}{1+e^{-\theta^{T} x} }$，图像为 其中的曲线为逻辑回归代价函数，两段直线为SVM的代价函数，记为$cost_{1}(z)$ 同理，当y=0时，代价函数剩右边一项$(1-y) \log \left(1-\frac{1}{1+e^{-\theta^{T} x} }\right)$，图像为 其中的SVM代价函数，记为$cost_{0}(z)$。 完整的逻辑回归代价函数写成：$\min _{\theta} \frac{1}{m}\left[\sum_{i=1}^{m} y^{(i)}\left(-\log h_{\theta}\left(x^{(i)}\right)\right)+\left(1-y^{(i)}\right)\left(\left(-\log \left(1-h_{\theta}\left(x^{(i)}\right)\right)\right)\right]+\frac{\lambda}{2 m} \sum_{j=1}^{n} \theta_{j}^{2}\right.$ 可以看出由代价项和正则化项两部分组成，基本形式为$A+\lambda B$，$\lambda$可以看做是控制两部分的权重。 要写出SVM的代价函数，首先我们把逻辑回归代价函数的$\frac{1}{m}$去掉，不影响$\theta$值。然后我们把正则化项的系数$\lambda$去掉，在代价项前面添上一个系数C，效果等同于$\frac{1}{\lambda}$。 因此，最终SVM的代价函数为：$\min _{\theta} C \sum_{i=1}^{m}\left[y^{(i)} \cos t_{1}\left(\theta^{T} x^{(i)}\right)+\left(1-y^{(i)}\right) \operatorname{cost}_{0}\left(\theta^{T} x^{(i)}\right)\right]+\frac{1}{2} \sum_{i=1}^{n} \theta_{j}^{2}$ 假设函数为： 注意，和逻辑回归假设函数计算出的是概率不同，SVM假设函数直接计算结果(1/0)。 直观理解大间距SVM又被称为大间距分类器，以下直观讲解为什么。 从图中可以看出： y=1时，需要$\theta^{T}x\ge1$，区别于逻辑回归中$\ge0$，此时$cost_{1}(z)=0$，即SVM在正确分类的基础上还构建了一个安全距离。 y=0时，需要$\theta^{T}x\le-1$，区别于逻辑回归中$\le0$，此时$cost_{0}(z)=0$，即SVM在正确分类的基础上还构建了一个安全距离。 接下来，如果我们把代价函数中的常熟C设置成一个非常大的值，例如C=100000，此时为了使代价最小，就必须让最优化函数 中蓝色框部分等于0，又根据y=1或y=0时$\theta^{T}x$的取值，可以把优化问题和约束条件改为： 当你把它最小化时，可以得到决策边界。 其中黑线为SVM的决策边界，比起粉红线和绿线，显然黑线具有更好的鲁棒性（robustness，又叫健壮性，在计算机中代表运行过程中处理错误，或是算法、输入异常时仍然正确运行的能力），距离正负样本的最小距离最大。也因此我们把SVM称为最大间距分类。 如果把C设置的非常大时，可能会出现下面的情况 由于左下角出现了一个异常点，决策边界会从黑线变为粉红线。相当于$\lambda$过小导致过拟合。如果把C值设置的小一点，则会忽略异常点的影响。因此对C的讨论，其实也是过拟合和欠拟合的问题。 大间隔分类器数学原理本节解释了为什么SVM能够大间隔分类 向量内积 简单来说，就是内积等于$u^{T}v$，也等于$p\lVert u \rVert$，其中p为向量v对向量u的投影，$\lVert u \rVert$为范数（norm，意思是具有“长度”概念的函数，这里可以简单理解为向量的长度）。内积具有正负，当向量夹角大于90度，内积为负；小于90度，内积为正。 那么回到优化问题上 我们假设n=2，$\theta_{0}=0$，然后通过之前提到的内容，可以改写优化问题：$\min _{\theta} \frac{1}{2} \sum_{j=1}^{n} \theta_{j}^{2}=\frac{1}{2}|\theta|^{2}$ 也可以改写约束条件，即把$\theta^{T} x^{(i)}$看成是向量相乘，最终能改写为下面的式子： 那么为什么SVM不会选择下图中的决策边界呢？ 其原因是，由下图可以看出，位于一、四象限的样本$x^{(1)}$，因为和决策边界接近，当它投影到$\theta$上时得到的$p^{(1)}$非常小，因此如果要满足$p^{(i)} \cdot|\theta| \geq 1$，那么$|\theta|$就必须非常大，这跟我们的优化问题$\min _{\theta} \frac{1}{2} \sum_{j=1}^{n} \theta_{j}^{2}=\frac{1}{2}|\theta|^{2}$矛盾。 位于二、三象限样本同理。 正确的样本边界如下图 额外说明一下，这里的$\theta_{0}=0$意味着决策边界一定会经过原点，如果令其不等于0，结论也是同样成立的。 核函数1本质上说，核函数跟SVM没有必然联系，但是在用于求分线性分类器时具有很好的效果。 考虑下图中的数据集： 这种非线性的情况，显然用原始输入的两个特征是无法表示的，因此我们需要增加多项式特征。 这里我们可以将特征重新编号为$f_{i}$： 但问题在于，如何去选取新的特征呢？之前在逻辑回归时，我们是列举出了多个不同组合方式，通过交叉验证集进行验证，挑选出最合适的。而在SVM中，我们可以通过核函数对原始输入特征进行映射进而得到新的特征。 举例说明： 假设此时原始输入x有两个特征$x_{1}$和$x_{2}$（不考虑$x_{0}$），需要得到新的特征$f_{1}$、$f_{2}$、$f_{3}$。 首先我们选取了3个点$l_{1}$、$l_{2}$、$l_{3}$，称为标记（landmark）。 从图中可以看出，$f_{i}$为x和$l^{(i)}$相似度，而这个求相似度的函数就被称为核函数（kernel），这里用到的是高斯核函数（Gaussion Kernels），${ {f}_{1} }=similarity(x,{ {l}^{ (1) } } )=e(-\frac{ { {\left| x-{ {l}^{(1)} } \right|}^{2} } }{2{ {\sigma }^{2} } } )$。 那么右边的式子究竟是什么含义？ 图中可以很明显的看出，$f_{i}$的取值在0~1之间。 绘制出核函数的图像如下 图中水平面坐标代表$x_{1}$和$x_{2}$，垂直的坐标代表$f$，可以看出，只有当x和$l$重合时，$f$才具有最大值。 同时也可以看出$\sigma$对$f$改变速率的影响。$\sigma$较小时，中间凸起的部分较窄，当x远离$l$时，$f$下降的较快；当$\sigma$较大时则刚好相反。 得到新的特征值$f$之后，就可以写出假设函数$h(\theta)=\theta_{0}+\theta_{1} f_{1}+\theta_{2} f_{2}+\theta_{3} f_{3}$，进而在图像上画出决策边界如下： 可以看出，当样本位于粉红色点的位置时，x靠近$l^{(1)}$，y=1；位于绿色点时，靠近$l^{(2)}$，y=1；位于蓝色点时，靠近$l^{(3)}$，y=0。 核函数2在上一节中，我们提到一个重要问题，那就是如何选取标记？ 通常根据训练集的样本数量来选择对应地标，即如果训练集中有m个样本，就选取m个标记。并且令：$l^{(1)}=x^{(1)}, l^{(2)}=x^{(2)}, \ldots, l^{(m)}=x^{(m)}$，这样做的好处在于我们得到的新标记是基于原有特征与其他样本特征的距离。 由上图我们可以得出结论，对于一个样本$x^{(i)}$，我们可以将它的特征映射为$f^{(i)}$，从维度上看，我们也将原始输入特征从n+1维，映射为m+1维（+1为偏置项，m为标记个数也是样本个数） 下面将核函数运用到SVM中，首先我们将原始输入特征映射为新特征$f$，然后根据代价函数求出最优化的$\theta$ $min C\sum\limits_{i=1}^{m}{ [ { {y}^{ (i)} }cos { {t}_{1} } }( { {\theta }^{T} }{ {f}^{(i)} })+(1-{ {y}^{(i)} })cos { {t}_{0} }( { {\theta }^{T} }{ {f}^{(i)} })]+\frac{1}{2}\sum\limits_{j=1}^{m}{\theta _{j}^{2} }$ 注意这里的最后的正则化项，$j$是从1-m而不是1-n，因为我们新特征$f \in \mathbb{R}^{m+1}$。而在实际运用中，我们还得对这一项的计算做一些修改，通常我们对正则化项的计算是用矩阵相乘$\sum_{j=1}^{n=m} \theta_{j}^{2}=\theta^{T} \theta$，但在SVM中，我们用$θ^TMθ$代替$θ^Tθ$，其中M根据我们选择的核函数来确定，这样可以简化运算。 最后再代入新的假设函数中。 使用SVM时还需要选择几个参数： 使用SVM使用成熟的软件包来计算参数 不过还是有些问题需要你自己解决 关于内核函数的选择 1.可以使用线性内核函数，即不用内核函数，这种情况适用于n较大，m较小。因为如果样本个数少，特征多，去拟合复杂的非线性函数，容易导致过拟合。 2.也可以使用核函数（高斯核函数），适用于n较小，m较大。 以一个高斯核函数为例，输入两个特征向量，输出一个实数 注意在使用高斯核函数时，一定要对原始输入特征进行缩放： 否则会如图中那样，不同范围的特征在生成新特征时会产生不同的影响。 除此之外还可以选择其他的核函数，但是都必须满足莫塞尔定理，不过除了线性核函数和高斯核函数，其他的一般也不太常用。 多类分类问题 解决多分类问题的方法 1.可以使用封装好的模块 2.和之前逻辑回归中多分类问题类似，也可以使用把多分类问题转换成多个二分类问题，最终对新样本进行预测时，只要看它属于哪个正类的假设函数值最大，就属于哪个类别。 关于逻辑回归、核函数SVM、不含核函数SVM的选择 SVM的效果可能不如神经网络，但训练速度快，且是一个凸优化问题（即一定能得到全局最优解）。]]></content>
      <tags>
        <tag>Machine Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习笔记|第六周]]></title>
    <url>%2F2019%2F04%2F24%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E7%AC%AC%E5%85%AD%E5%91%A8%2F</url>
    <content type="text"><![CDATA[运用机器学习建议决定下一步做什么？当你的模型运用于新的样本时，如果产生巨大的误差该怎么办？ 一般来说，有以下几种处理方式： 获得更多的训练样本——通常是有效的，但代价较大，下面的方法也可能有效，可考虑先采用下面的几种方法。 尝试减少特征的数量 尝试获得更多的特征 尝试增加多项式特征 尝试减少正则化程度$\lambda$ 尝试增加正则化程度$\lambda$ 当然我们不可能随机去一个个方法尝试，所以需要一点手段来预测。 评估假设函数首先，如果你计算出的误差（代价函数值）非常大，那么选取的假设函数就可能存在问题；即便误差小，也有可能引起过拟合。 那么如何来判断过拟合问题？ 可以将数据集按比例划分成，训练集（Training set）：测试集（Test set）=70：30。 然后将训练集得到的模型运用于测试集，用来计算误差： 1.对于线性回归模型，我们直接计算 2.对于逻辑回归模型，除了计算代价函数 更常见的方法是计算误分类的比例（0/1错误分类度量） 模型选择和交叉验证集当模型确定的时候可以使用上一节的方法来验证，但是如何确定一个模型呢？首先需要知道，正则化惩罚项系数$\lambda$的选择；增加多项式特征时，多项式的次数等这类问题，称为模型选择问题。 假设我们的输入特征只有一个时，拟合效果是非常不理想的。因此，我们通常会增加特征项，那么问题又来了，多项式特征的次数应该怎么选取，即到底选取什么样的模型（假设）？ 我们可以罗列出多种情况 这时再将数据集划分成训练集和测试集，对这些假设分别在训练集上训练，通过最小化训练集的代价，求出最优参数$\Theta_{1}$~$\Theta_{10}$。将其代入测试集，计算每个模型的误差，选择误差最小的那组作为假设，并把这组的误差值作为泛化误差。 然而其中存在一个问题：通过测试集来选取模型，又用测试集来求泛化误差，显然是不是坠吼滴。 因此，我们重新划分数据集的比例，训练集：交叉验证集(Cross Validation set)：测试集=60:20:20 然后计算选择出模型： 1.用训练集训练出$\Theta_{1}$~$\Theta_{10}$; 2.用交叉验证集计算出最小误差，选择误差最小的模型; 3.用第2步中选择的模型计算测试集得出泛化误差。 Train/validation/test error Training error: ​ Cross Validation error: ​ Test error: ​ 上面说到的，是关于如何改变特征来减小误差，而接下来的内容则和正则化$\lambda$有关 诊断偏差和方差当运行结果不理想时，多半有两种情况：过拟合或者欠拟合。而这两种情况，哪种和高偏差有关？哪种和高方差有关？还是都有关系？ 从图中可以看出，欠拟合时高偏差，过拟合时高方差。那么在没法画图的情况下（基本都是这种情况）如何来确定是高偏差还是高方差呢？ 方法如下： 将训练集误差和验证集误差绘制在图中，其中横坐标为多项式次数。 Training error: Cross Validation error: 可以明显看出 对于训练集，d越大，误差越小 对于验证集，随着d增长，误差会先减小后增大，其中的最低点就是开始过拟合的情况。 那么当我们在没有图像的情况下，得出验证集误差较大时，只需要根据训练集误差的大小就能得出是高偏差还是高方差了。 具体来说，就是： 训练集误差和交叉验证集误差近似时：偏差/欠拟合 交叉验证集误差远大于训练集误差时：方差/过拟合 正则化和偏差、方差正则化常常用来解决过拟合问题，但是对于正则化项参数$\lambda$，如下图所示不同的选取可能会导致不同的偏差问题，那么应该如何选择合适的值呢？ 我们先选择一系列想要测试的$\lambda$，通常是0~12之间呈现2倍的关系，例如：$0,0.01,0.02,0.04,0.08,0.15,0.32,0.64,1.28,2.56,5.12,10$（共12个）。 再像之前那样，把数据集划分为训练集、验证集和测试集。但是因为之前我们都是不带正则化，所以$J(\theta)$和$J_{train}(\theta)$是一样的，但是这里就要如下图中的那样分开。 先在训练集上用最小化代价函数，求得最优的一组参数。然后再用该参数在训练/验证/测试集上根据公式计算相应误差。 （这里存疑？关于$J(\theta)$和$J_{train}(\theta)$什么时候使用） 具体步骤如下： 1.先对每个$\lambda$值，求对应最小化代价函数的参数 2.将上述求得的参数，代入交叉验证集计算代价函数$J_{cv}(\theta)$，选择对应最小代价函数的$\theta$作为最优模型参数。 3.代入测试集$J_{test}(\theta)$计算泛化误差。 还可以把验证集误差和训练集误差绘制成图，横坐标为$\lambda$。 可以看出： 当$\lambda$较小时，$J_{train}(\theta)$值较小，$J_{cv}(\theta)$值较大，过拟合 当$\lambda$较大时，$J_{train}(\theta)$值较大，$J_{cv}(\theta)$值较大，欠拟合 学习曲线学习曲线是一个非常好的工具，用于判断模型偏差、方差问题。它是一个以样本个数为横坐标，以误差为纵坐标绘制的图像。 1.如何利用学习曲线识别高偏差？ 从图中可以看出，对于欠拟合而言，增加样本个数没有意义，因为模型拟合能力差，没有能力去关注”细节“。 从这张图中也可以看出，当样本到达一定数量时，训练集合测试集的误差会非常接近且不再变化，但高于我们期望的误差。 如何利用学习曲线识别高方差？ 可以看出在过拟合情况下，增加数据，可能可以提高算法效果。 增加样本个数会小幅度提高训练集误差，但是始终维持在一个相对较低的水平。而验证集个数增加为增进模型对数据的了解，因此会验证集误差会减小 上面的test error和cross validation error用哪个效果都一样。 决定下一步做什么？如何通过这些诊断法来帮助我们选择改进模型的方法呢？回到最初的问题上 获得更多的训练样本——解决高方差 尝试减少特征的数量——解决高方差 尝试获得更多的特征——解决高偏差 尝试增加多项式特征——解决高偏差 尝试减少正则化程度λ——解决高偏差 尝试增加正则化程度λ——解决高方差 神经网络 使用神经元较少的神经网络（左图）跟参数较少时情况类似，容易导致高偏差和欠拟合；同理，神经元较多（右图）这容易导致高方差，可以使用正则化手段来解决。 一般使用神经元较多的情况比较好处理。当然你也可以同样把数据集划分为训练集、交叉验证集和测试集。然后从一层隐藏层开始逐一尝试。找到验证集误差最小的作为模型。 机器学习系统设计确定执行的优先级以建立一个垃圾邮件分类器为例 step1.用向量表示邮件输入变量x为邮件的特征；y表示邮件的标签，1代表垃圾邮件，0代表不是。 我们可以人工选择100个单词作为词典，然后比对邮件中单词是否出现，出现则记为1，未出现记为0（注意不是记录出现个数），因此能够得到下图中的100维向量，这个向量作为输入。 实际上，我们不会人工选择单词构成字典，而是在训练集中自动选择出现频率最高的n(10000-50000)个单词构成字典,然后用一个n维的特征向量来表示邮件。 step2.想办法降低分类的错误率1.收集更多的训练数据，但这种要视情况而定。 2.为每个邮件设计更复杂的特征，比如把邮件正文标题也考虑进去 3.为邮件的正文设计更复杂的特征，比如单词的单复数，discount和discounts是否应该看成一个单词；首字母大小、后缀，deal和Dealer是否应该看作一个单词；是否应该考虑标点符号，可能垃圾邮件中叹号会比较多。 4.构建更复杂的算法来检测邮件中的错误拼写，比如垃圾邮件发送者经常把一些容易被检测到的单词写错，如m0rtgage，med1cine，w4tches等，从而避免被检测到。 当然上述方法如何选择，同样也是个问题。 误差分析当你准备好构建一个机器学习系统时，最好的方式是先用简单的方法快速实现。具体来说： 1.从一个简单的能快速实现的算法开始，实现该算法并用交叉验证集数据测试。 2.绘制学习曲线，判断是高偏差或是高方差，决定是增加更多的数据还是添加更多的特征。 3.误差分析：人工观察交叉验证集，看看被分错的样本是否具有某些规律。 举个误差分析的例子： 假设验证集有500个样本，$m_{cv}=500$，分类器分错了100个样本，此时可以人工检查这100个错误：首先分析这100个样本的错分类型和数量。如下所示： 卖药的邮件12封，卖假货的邮件4封，钓鱼的邮件53封，其他的31封；那么我们就可以把关注点放在钓鱼邮件上。 然会继续对钓鱼邮件分析，是否可以发现一些新的特征。能够提高分类器的的性能，比如这些钓鱼邮件存在以下几个问题： 我们又可以把关注点放在“不常用的标点”这一项上，毕竟有32封邮件都存在该问题，进而设计一些包含标点的更复杂的特征，以此提高性能。 数值评估： 在做垃圾邮件分类时，可能会遇到这种问题： 应不应该把discount,discounts,discounted,discounting看作是同一个单词,即是否看成一个特征。 在自然语言处理中会使用词干提取，在这种情况下，上述单词会被看作是同一个。但是词干提取有利有弊，这种方法通常指关注前几个字母，比如它可能会把universe/university看作一个单词，这显然不合理。 那么我们到底要不要使用词干提取？这时我们可以使用数值评估的方法： 首先使用词干提取，训练一个分类器，在验证集上计算它的错误率；然后不使用词干提取，训练一个分类器，在验证集上计算它的错误率。二者进行比较，哪个错误率低，就使用哪种方法。 类似的这种问题，还包括是否应该区分单词大小写，如mom和Mom是否应该看作一个单词，都可以使用上述数值评估的方法，选择一个合理的做法。 不对称分类的误差评估我们通常用错误率/正确率来评估一个算法，但有时这种方法是不合适的，比如偏斜类问题。 以这个判断肿瘤恶性/良性的分类器为例，如果分类器错误率为1%，而实际只有0.5%的病人肿瘤为恶性。这种情况下，尽管错误率看起来非常低，也可能造成严重后果。此时哪怕直接把所有病人都认为良性，也只有0.5%的错误率，高于分类器。 这就是偏斜类问题，情况表现为训练集中同一种类样本非常多，而其他类样本样本比较少。 那么如何解决偏斜类问题？换句话说，如何知道算法把所有病人都认为良性而没有做出真正的分类？ 可以使用查准率(Precision)和召回率(Recall)。 召回率又可以叫做查全率，事实上叫查全率显然更符合其含义 我的理解，查准率指预测正确的样本个数占样本总数的多少；查全率指预测正确的样本个数占实际正确的样本个数。 还是以上面肿瘤的例子 准确率等于True positive除以一行； 召回率等于True positive除以一列； 由此可以看出，当使用y=0（把所有病人判断为良性）的方法预测时，召回率为0，由此可以排除。 查准率和召回率的均衡在肿瘤例子中，我们一般情况下设置分类的阈值为0.5。 我们可能需要在非常准确的情况下，才去预测肿瘤为恶性，否则让良性肿瘤的病人去接受治疗，那肯定要被骂，因此这时候我们需要高查准率，因此可以调高阈值为0.7或0.9，可以更准确的预测出恶性肿瘤。但此时明显，召回率就被降低了 但召回率的降低可能导致更多恶性肿瘤的情况被归类为良性肿瘤，这种情况也是我们不愿见到的，因此我们也可能想调高召回率。 由此可以画出查准率和召回率的图像： 综上，我们需要向办法设置合适的阈值，以达到查准率和召回率的平衡。 通常使用$F_{1}score$来权衡，计算公式为${ {F}_{1} }Score:2\frac{PR}{P+R}$，$F_{1}score$值越高，模型的性能越好。 机器学习数据之前曾经说过，不要盲目的收集大量训练数据。 但是有些时候，收集大量数据会得到一个性能良好的学习算法。 即当你的算法有很多参数，数据特征充足（当一个人类专家拿到输入x时，能做出良好的判断，证明特征充足）时，更多的训练数据会带来更好的性能。 参考资料 [1] 机器学习 | 吴恩达机器学习第六周学习笔记 [2] 机器学习笔记（3）—— 优化，偏差和方差，偏斜数据 - 关右的文章 - 知乎]]></content>
      <tags>
        <tag>Machine Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[matplotlib坐标原点不重合]]></title>
    <url>%2F2019%2F04%2F15%2Fmatplotlib%E5%9D%90%E6%A0%87%E5%8E%9F%E7%82%B9%E4%B8%8D%E9%87%8D%E5%90%88%2F</url>
    <content type="text"><![CDATA[用matplotlib画图时会遇到原点不重合在左下角的情况 12345678fig,ax=plt.subplots(figsize=(8,5)) ax.plot(range(1,13),error_train,label="Train") ax.plot(range(1,13),error_cv,label="Cross Validation",color="green") ax.legend() plt.xlabel("Number of training examples") plt.ylabel('error') plt.title('Learning curve of linear regression') plt.show() 只需要添加两行代码即可 12plt.ylim(bottom=0)plt.xlim(left=0)]]></content>
      <tags>
        <tag>matplotlib</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习|吴恩达机器学习之神经网络反向传播]]></title>
    <url>%2F2019%2F04%2F12%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%2F</url>
    <content type="text"><![CDATA[正向传播和机器学习|吴恩达机器学习之神经网络中的内容差不多，都是在给出$\Theta_{(1)}$和$\Theta_{(2)}$的情况下通过正向传播求个代价值或是验证一下准确率。 导入数据123456789101112131415def load_mat(path): '''读取数据''' data = loadmat('ex4data1.mat') # return a dict X = data['X'] y = data['y'].flatten() return X, y def load_weight(): '''读取权重''' weight = loadmat('ex4weights.mat') theta1 = weight['Theta1'] theta2 = weight['Theta2'] return theta1, theta2 这里需要对y向量做一个处理： 1234567def expand_y(y): result = [] for i in y: y_array = np.zeros(10) y_array[i - 1] = 1 result.append(y_array) return np.array(result) 原来的y是用数字表示： 1[10 10 10 ... 9 9 9] 转换为矩阵，用1的位置来表示： 1234567[[0. 0. 0. ... 0. 0. 1.] [0. 0. 0. ... 0. 0. 1.] [0. 0. 0. ... 0. 0. 1.] ... [0. 0. 0. ... 0. 1. 0.] [0. 0. 0. ... 0. 1. 0.] [0. 0. 0. ... 0. 1. 0.]] 可视化数据随机选100张图，可视化观察一波 123456789101112131415def plot_data(X): '''随机画100个数字''' index = np.random.choice(range(5000), 100) # np.random.choice(arrange,size),返回ndarray images = X[index] # 随机选择100个样本 fig, ax_array = plt.subplots( 10, 10, sharex=True, sharey=True, figsize=(8, 8)) # ax_array为Axes对象 for r in range(10): for c in range(10): ax_array[r, c].matshow( images[r * 10 + c].reshape(20, 20), cmap='gray_r' ) # matshow() 第一个参数为要显示的矩阵（Display an array as a matrix in a new figure window） plt.yticks([]) plt.xticks([]) plt.show() 关于plt.subplots展开，参考subplots画图。 计算因为反向传播和正向传播的公式，代价函数，正则化都是一样的，在后面反向传播再展开讲。 正向传播1.公式： 2.代码： 12345678def feed_forward(theta1, theta2, X): z2 = X @ theta1.T a2 = sg.sigmoid(z2) #(5000,25) a2 = np.insert(a2, 0, 1, axis=1) #(5000,26) z3 = a2 @ theta2.T a3 = sg.sigmoid(z3) return z2, a2, z3, a3 代价函数1.公式：$J(\theta)=\frac{1}{m} \sum_{i=1}^{m} \sum_{k=1}^{K}\left[-y_{k}^{(i)} \log \left(\left(h_{\theta}\left(x^{(i)}\right)\right)_{k}\right)-\left(1-y_{k}^{(i)}\right) \log \left(1-\left(h_{\theta}\left(x^{(i)}\right)\right)_{k}\right)\right.$ 2.代码： 1234567def cost(theta1, theta2, X, y): z2, a2, z3, h = feed_forward(theta1, theta2, X) # 这里的y是矩阵而不是向量了 first = -y * np.log(h) second = (1 - y) * np.log(1 - h) return (np.sum(first - second)) / len(X) # 这里不能用np.mean()，否则会相差10倍 代价函数正则化1.公式： $\begin{aligned} J(\theta)=&amp; \frac{1}{m} \sum_{i=1}^{m} \sum_{k=1}^{K}\left[-y_{k}^{(i)} \log \left(\left(h_{\theta}\left(x^{(i)}\right)\right)_{k}\right)-\left(1-y_{k}^{(i)}\right) \log \left(1-\left(h_{\theta}\left(x^{(i)}\right)\right)_{k}\right)\right]+\\ &amp; \frac{\lambda}{2 m}\left[\sum_{j=1}^{25} \sum_{k=1}^{400}\left(\Theta_{j, k}^{(1)}\right)^{2}+\sum_{j=1}^{10} \sum_{k=1}^{25}\left(\Theta_{j, k}^{(2)}\right)^{2}\right] \end{aligned}$ 2.代码： 12345def cost_reg(theta1, theta2, X, y, lmd): c = cost(theta1, theta2, X, y) reg = (lmd / (2 * len(X))) * ( np.sum(theta1[:, 1:]**2) + np.sum(theta2[:, 1:]**2)) return reg + c 主函数1234567891011def main(): path = 'ex4data1.mat' X, y = load_mat(path) X = np.insert(X, 0, 1, axis=1) y = expand_y(y) theta1, theta2 = load_weight() print(cost_reg(theta1, theta2, X, y, 1)) #0.38376985909092354 if __name__ == "__main__": main() 反向传播随机初始化之前我们是把$\theta​$初始化为0向量，但是在神经网络中如果把$\theta_{1}​$初始化为0，那么$S_{2}​$中的激活单元都为相同值。同理，只要初始化为相同的数，那么结果都一样。 因此我们通常随机初始化，即在（-$\varepsilon$，$\varepsilon$）之间随机取值，为了保证效率，需要取值足够小，所以选择$\varepsilon=0.12$ 12def random_init(size): return np.random.uniform(-0.12, 0.12, size) 处理参数使用优化参数opt.minimize()，需要把参数展开。 12345678def serialize(a, b): '''展开参数''' return np.r_[a.flatten(), b.flatten()] # 按行拼接def deserialize(seq): '''提取参数''' return seq[:25 * 401].reshape(25, 401), seq[25 * 401:].reshape(10, 26) 代价函数（带正则项）以多分类为例 1.公式： 其中 L：神经网络的层数 $s_{l}​$：$l​$层中的神经元个数（不包括bias unit） K：输出层中的神经元个数 m：样本个数 累加项中表示从第一项累加到第k项（why？） 正则项表示神经网络中所有权重的平方和。 梯度项 一般情况下，我们只知道Input Layer和Output Layer两层的神经元个数，中间的Hidden Layer很难确定，不过对于初学者而言，都是参考别人算法里的，所以这里也直接给出了Hidden Layer的层数（1层）以及$\theta_{1}$（25，401）和$\theta_{2}$（10，26）的维度，一个神经元为一列。 计算前馈(feedforward)参数含义及传递过程如下 1.参数含义： $\Theta^{i}​$第$i​$层的参数矩阵 $z^{(l)}$第$l$层的输入 $a^{(l)}$第$l$层的输出 2.传递过程： $a^{(1)}=x​$ $z^{(2)}=\Theta^{(1)} a^{(1)}​$ $a^{(2)}=g\left(z^{(2)}\right)\left(a d d \ bias \ \ a_{0}^{(2)}\right)​$ $z^{(3)}=\Theta^{(2)} a^{(2)}​$ $h=a^{(3)}=g\left(z^{(3)}\right)​$ 3.前馈代码： 12345678def feed_forward(theta1, theta2, X): z2 = X @ theta1.T a2 = sg.sigmoid(z2) #(5000,25) a2 = np.insert(a2, 0, 1, axis=1) #(5000,26) z3 = a2 @ theta2.T a3 = sg.sigmoid(z3) return z2, a2, z3, a3 计算梯度项计算梯度项，也就是代价函数的偏导数$\frac{\partial}{\partial\Theta^{(l)}_{ij}}J\left(\Theta\right)​$。通过前馈的计算我们得到了$h​$，接下来计算“误差”，这里之所以用引号，是因为误差的实质是$\delta^{(l)}=\frac{\partial J}{\partial z^{(l)}}​$ $\delta^{(3)}=h-y​$ ……(1) $\delta^{(2)}=(\Theta^{(2)})^{T}\delta^{(3)}g^{\prime}\left(z^{(2)}\right)$……(2) 第一层没有误差，接下去计算每层参数矩阵的梯度值，用$\Delta^{(l)}$表示 $\Delta^{(2)}=a^{(2)} \delta^{(3)}$……(3) $\Delta^{(1)}=a^{(1)} \delta^{(2)}​$……(4) 最后网络的总梯度为： （这里并不是真的相加，而是将$\Delta^{(1)}$和$\Delta^{(2)}$合成为一个向量，方便后面计算） $D=\frac{1}{m}\left(\Delta^{(1)}+\Delta^{(2)}\right)$ 求梯度项代码： 1234567891011def gradient(theta1,theta2,X,y): z2,a2,z3,h=feed_forward(theta1,theta2,X) d3=h-y # (5000,10) d2=d3@theta2[:,1:]*sg.sigmoid_gradient(z2) # (5000,25) D2=d3.T@a2 # (10,26) D1=d2.T@X # (25,401) # 这里合并成1-D array是为了方便后面用优化函数处理 D=(1/len(X))*serialize(D1,D2) #(10285,) return D 正则化 1.原理 2.代码： 123456789def regularized_gradient(theta,X,y,lmd=1): theta1,theta2=deserialize(theta) D1,D2=deserialize(gradient(theta1,theta2,X,y)) theta1[:,0]=0 theta2[:,0]=0 reg_D1=D1+(lmd/len(X))*theta1 reg_D2=D2+(lmd/len(X))*theta2 return serialize(reg_D1,reg_D2) *推导$\delta$和$\Delta$从代价函数入手，假设我们只有一个输入样本，那么代价函数为：$J(\theta)=-y \operatorname{logh}(x)-(1-y) \log (1-h)$，所谓梯度项，就是将代价函数对参数求导，即$\frac{\partial}{\partial \Theta^{(2)}} J(\theta), \frac{\partial}{\partial \Theta^{(1)}} J(\theta)$。而由传递过程函数： $a^{(1)}=x$ $z^{(2)}=\Theta^{(1)} a^{(1)}$ $a^{(2)}=g\left(z^{(2)}\right)\left(a d d \ bias \ \ a_{0}^{(2)}\right)$ $z^{(3)}=\Theta^{(2)} a^{(2)}$ $h=a^{(3)}=g\left(z^{(3)}\right)​$ 我们可以使用链式求导法则，因此有$\frac{\partial J}{\partial \Theta^{(2)}}=\frac{\partial J}{\partial a^{(3)}} \frac{\partial a^{(3)}}{\partial z^{(3)}} \frac{\partial z^{(3)}}{\partial \Theta^{(2)}}=(h-y)a^{(2)}​$ 其中令$\delta^{(3)}=h-y​$得到公式(1)；令$\Delta^{(2)}=\frac{\partial J}{\partial \Theta^{(2)}}​$则得到公式(3)。 接着求$\frac{\partial J}{\partial \Theta^{(1)}}​$=$\frac{\partial J}{\partial a^{(3)}} \frac{\partial a^{(3)}}{\partial z^{(3)}} \frac{\partial z^{(3)}}{\partial a^{(2)}} \frac{\partial a^{(2)}}{\partial z^{(2)}} \frac{\partial z^{(2)}}{\partial \Theta^{(1)}}​$ ​ =$\delta^{(3)} \Theta^{(2)} g^{\prime}\left(Z^{(2)}\right) a^{(1)}​$ ​ =$\delta^{(2)} a^{(1)}​$ 同样可以看出，令$\delta^{(3)}=\frac{\partial J}{\partial a^{(3)}} \frac{\partial a^{(3)}}{\partial z^{(3)}}$ ，$\delta^{(2)}=\delta^{(3)} \Theta^{(2)} g^{\prime}\left(Z^{(2)}\right)$则得到公式(2)。令$\frac{\partial J}{\partial \Theta^{(1)}}=\Delta^{(1)}$得到公式(4)。 梯度检验目的：在反向传播的过程中，因为需要计算的参数很多，因此容易导致误差，使得最终的结果并非最优解。因此为了确定反向传播计算的梯度是否正确，需要用到梯度检验(gradient check)。 原理：通过计算$\frac{\partial}{\partial \Theta}J(\Theta)=\lim _{\varepsilon \rightarrow 0} \frac{J(\theta+\varepsilon)-J(\theta-\varepsilon)}{2 \varepsilon}$，估计出$J(\theta)$在$\theta$的值，和反向传播计算的梯度值$\Delta$进行对比。具体来说，对于某个特定的 $\theta$，我们计算出在 $\theta$-$\varepsilon $ 处和 $\theta$+$\varepsilon $ 的代价值（$\varepsilon $是一个非常小的值，通常选取 0.001），然后求两个代价的平均，用以估计在 $\theta$ 处的代价值。 具体做法：先将$\Theta_{(1)}$和$\Theta_{(2)}$展开成一个向量$\theta$，维度（10285，）。然后循环，对每个$\theta_{j}$求$\frac{\partial}{\partial \theta_{j}}J(\theta)$，以$\theta_{1}$为例，$\frac{\partial}{\partial \theta_{1}}=\frac{J\left(\theta_{1}+\varepsilon_{1}, \theta_{2}, \theta_{3} \dots \theta_{n}\right)-J\left(\theta_{1}-\varepsilon_{1}, \theta_{2}, \theta_{3} \ldots \theta_{n}\right)}{2 \varepsilon}$，注意这里虽然只有$\theta_{j}$每次都要把整个$J(\theta)$带进去，因此需要每次都复制整个$\theta$。 1234567891011def gradient_check(theta1,theta2,X,y,e): theta_temp=serialize(theta1,theta2) # (10285,) numeric_grad=[] for i in range(len(theta_temp)): plus=copy.copy(theta_temp) # 复制theta minus=copy.copy(theta_temp) # 复制theta plus[i]+=e # 只改变当前的一个值 minus[i]-=e # 只改变当前的一个值 grad_i=(cost_reg(plus,X,y,1)-cost_reg(minus,X,y,1))/(e*2) numeric_grad.append(grad_i) 再和$\Theta_{(1)}$和$\Theta_{(2)}$比较求得准确度。 这里简单介绍一下数值梯度(numerical gradient)和解析梯度(analytic gradient)，数值梯度的优点是编程可以直接实现，不要求函数可微，缺点是运行速度非常慢，也就是上面中numeric_grad，且只能求出近似解；解析梯度能求出近似解，也是我们通常使用的方法，即analytic_grad。 得到数值梯度和解析梯度之后，要求他们的进行相似性度量(Similarity Measurement)，这里用标准化欧氏距离（存疑？）$diff=\frac{ | \text { numeric_grad }-\text {analytic_grad}\left|_{2}\right.}{ | \text { numeric_grad }\left|_{2}+\right| \text { analytic_grad }\left|_{2}\right.}$，当距离(diff)小于10e-9时为计算正确。 123456reg_D1,reg_D2=regularized_gradient(theta_temp,X,y)analytic_grad=serialize(reg_D1,reg_D2) diff = np.linalg.norm(numeric_grad - analytic_grad) / np.linalg.norm(numeric_grad + analytic_grad) print('If your backpropagation implementation is correct,\nthe relative difference will be smaller than 10e-9 (assume epsilon=0.0001).\nRelative Difference: &#123;&#125;\n'.format(diff)) np.linalg.norm()：linalg=linear(线性)+algebra(代数)，norm()表示范数$^{[1]}$。 优化参数123456789101112def nn_trainning(X,y): init_theta = random_init(10285) #随机初始化theta1和theta2 res = opt.minimize( fun=cost_reg, x0=init_theta, args=(X, y, 1), method='TNC', jac=regularized_gradient, options=&#123;'maxiter': 400&#125;) return res 可视化隐藏层一种明白神经网络是如何学习的方法就是将隐藏层捕获的内容可视化，通俗来说就是输入一个x，激活这个隐藏层。在我们的这个训练样本中，$\theta_{1}$（25,401）有401个参数，去掉偏置单元，将剩下的400个参数reshape为(20,20)。 12345678910def plot_hidden(theta): t1,_=deserialize(theta) t1=t1[:,1:] fig,ax_array=plt.subplots(5,5,sharex=True,sharey=True,figsize=(6,6)) for r in range(5): for c in range(5): ax_array[r,c].matshow(t1[r*5+c].reshape(20,20),cmap='gray_r') plt.xticks([]) plt.yticks([]) plt.show() 计算精确度123456def accuracy(theta, X, y): theta1, theta2 = deserialize(theta) _, _, _, h = feed_forward(theta1, theta2, X) y_pred = np.argmax(h, axis=1) + 1 print(classification_report(y, y_pred)) 主函数12345678910111213def main(): path = 'ex4data1.mat' X, raw_y = ff.load_mat(path) X = np.insert(X, 0, 1, axis=1) y = ff.expand_y(raw_y) # y的一行表示一个样本 # theta1, theta2 = ff.load_weight() # gradient_check(theta1,theta2,X,y,0.0001) theta_unroll = nn_training(X, y) accuracy(theta_unroll.x, X, raw_y)if __name__ == "__main__": main() 参考资料[1] np.linalg.norm(求范数) [2] 吴恩达机器学习作业Python实现(四)：神经网络(反向传播)]]></content>
      <tags>
        <tag>Machine Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[subplots画图]]></title>
    <url>%2F2019%2F04%2F07%2Fsubplots%E7%94%BB%E5%9B%BE%2F</url>
    <content type="text"><![CDATA[matplotlib是python领域中很常见的绘图模块 几乎只在需要用到时查手册，这里记录一些比较常用的函数，有机会再来系统学习一下 以一段画图代码为例 123456789101112131415161718def plot_data(X): '''随机画100个数字''' # np.random.choice(arrange,size),返回ndarray index=np.random.choice(range(5000),100) images=X[index] # 随机选择100个样本 # ax_array为Axes对象 fig,ax_array=plt.subplots(10,10,sharex=True,sharey=True,figsize=(8,8)) for r in range(10): for c in range(10): # matshow() 第一个参数为要显示的矩阵 #Display an array as a matrix in a new figure window ax_array[r,c].matshow(images[r*10+c].reshape(20,20),cmap='gray_r') plt.yticks([]) plt.xticks([]) plt.show() 1.matplotlib.pyplot.subplots()：创建一个figure和一组subplots 参数： nrows，ncols：axes的数量，这里是10*10 sharex，sharey：共享所有axesX轴和y轴的属性，设置True开启 返回值： figure ax：一个或多个axes对象 * axes和subplot的区别：简单来说，如果把figure看做是电脑桌面，那么axes就是可自由移动的图标，subplot则是不可自由移动的图标。 2.matplotlib.pyplot.matshow()：在窗口用矩阵显示一个数组 参数： array-like(M,N)：要显示的(M,N)矩阵]]></content>
      <tags>
        <tag>matplotlib</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习|吴恩达机器学习之神经网络]]></title>
    <url>%2F2019%2F04%2F06%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%2F</url>
    <content type="text"><![CDATA[多类分类问题就是把多个类别细分为多个0-1类别来分析 代码：导入与处理数据1.导入模块 123456import numpy as npimport matplotlib.pyplot as pltfrom scipy.io import loadmatfrom scipy.optimize import minimizeimport Sigmoid 2.导入数据 因为图像的灰度值是用.mat存储的，所以用loadmat来导入到python中 123456# 5000个训练样本，每个样本20*20的灰度值，展开为400维向量。输出为0~9的数字def load_data(path): data=loadmat(path) X=data['X'] y=data['y'] return X,y 3.可以随机显示一张图片来观察 123456789def plot_an_image(X,y): pick_one=np.random.randint(0,5000) image=X[pick_one,:] fig,ax=plt.subplots(figsize=(1,1)) ax.matshow(image.reshape(20,20),cmap='gray_r') plt.xticks([]) plt.yticks([]) plt.show() print('this should be &#123;&#125;'.format(y[pick_one])) 结果如图： 代价函数正则化项逻辑回归，和之前的没区别 12345def cost_reg(theta,X,y,lmd): theta_reg=theta[1:] first=y*np.log(Sigmoid.sigmoid(X@theta))+(1-y)*np.log(1-Sigmoid.sigmoid(X@theta)) reg=(lmd/(2*len(X)))*(theta_reg@theta_reg)# 惩罚项不从第一项开始 return -np.mean(first)+reg 梯度函数和之前的没区别，只不过这里用了np.concatenate()来拼接。 12345def gradient_reg(theta,X,y,lmd): theta_reg=theta[1:] first=(1/len(X))*(X.T@(Sigmoid.sigmoid(X@theta)-y)) reg=np.concatenate([np.array([0]),(lmd/len(X))*theta_reg]) return first+reg one vs all每次循环都求出0~9中任一数字的$\theta$值（将该数字和其他数字二分类），最后拼接成一个矩阵。 12345678910111213def one_vs_all(X,y,lmd,K): all_theta=np.zeros((K,X.shape[1])) for i in range(1,K+1): theta=np.zeros(X.shape[1]) y_i=np.array([1 if label==i else 0 for label in y]) ret=minimize(fun=cost_reg,x0=theta,args=(X,y_i,lmd),method='TNC', jac=gradient_reg,options=&#123;'disp':True&#125;) all_theta[i-1,:]=ret.x return all_theta 计算精确度123456789101112'''计算精确度返回值 h_argmax是一个存放预测数字的一维ndarray'''def predict_all(X, all_theta): h = Sigmoid.sigmoid(X @ all_theta.T) # 返回指定方向上的最大值的索引 axis=0:按列索引，axis=1：按行索引 h_argmax = np.argmax(h, axis=1) h_argmax = h_argmax + 1 return h_argmax main函数12345678def main(): X, y = load_data('ex3data1.mat') plot_an_image(X,y) X = np.insert(X, 0, 1, axis=1) y = y.flatten() all_theta = one_vs_all(X, y, 1, 10) y_pred = predict_all(X, all_theta) accuracy = np.mean(y_pred == y) # 精确度94.46% 神经网络原理模仿人类大脑的神经元： 进一步设计出神经网络： 代码这个实验是神经网络的正向传播过程，不涉及如何训练。 导入模块1234from scipy.io import loadmatimport numpy as npimport Sigmoidimport multiClassClassification as mc 导入数据这里的权重已经给出，导入即可 1234def load_weigth(path): data=loadmat(path) # Theta1是输入层和隐藏层之间的参数；Theta2是隐藏层和输出层之间的参数 return data['Theta1'],data['Theta2'] main函数1234567891011121314151617def main(): theta1, theta2 = load_weigth('ex3weights.mat') X, y = mc.load_data('ex3data1.mat') y = y.flatten() X = np.insert(X, 0, values=np.ones(X.shape[0]), axis=1) # 输入层到隐藏层 z2 = X @ theta1.T z2 = np.insert(z2, 0, 1, axis=1) # 隐藏层到输出层 a2 = Sigmoid.sigmoid(z2) z3 = a2 @ theta2.T a3 = Sigmoid.sigmoid(z3) y_pred = np.argmax(a3, axis=1) + 1 accurcy = np.mean(y_pred == y) # 精确度97.52% 参考资料正向传播的向量化实现]]></content>
      <tags>
        <tag>Machine Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习|吴恩达机器学习之线性回归]]></title>
    <url>%2F2019%2F04%2F06%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%2F</url>
    <content type="text"><![CDATA[对《机器学习》这门课程的回顾，系列文章的目的是希望能够把原理和代码实现统一起来，增进理解，所以对一些我认为简单的知识，可能会一笔带过。 前言这里对第一周的内容做一些简单的回顾： 线性回归-单变量(one variable)基本内容假设函数（1）公式：$h_{\theta}(x)=\theta_{0}+\theta_{1}x​$ （2）原理：输入一个单变量x，输出预测值 代价函数（1）公式：$J(\theta_{0},\theta_{1})=\frac{1}{2m}\sum_{i=1}^{m}(h_{\theta}(x_{i})-y_{i})^{2}$，又称“平方误差函数”，这里的1/2是方便后面求导时约掉平方。 （2）原理：最小二乘法判断误差。 梯度下降法（1）公式： 重复直到收敛。 代码在实际代码中，数据都是用矩阵的方式来表示。 1.导入几个常用库 123456import numpy as npimport pandas as pdimport matplotlib.pyplot as plt #画图函数from mpl_toolkits.mplot3d import Axes3Dfrom matplotlib.colors import LogNorm 数据处理1.导入csv文件，用DataFrame结构存储。 （1）关于csv文件：简单来说就是纯文本，以行为一条记录，每条记录被分隔符分隔。CSV文件 （2）pandas中的DataFrame：官方手册 （3）关于.describe()返回值中的分位数：浅谈分为数 123456789101112131415161718192021'''导入数据'''path = 'Data\ex1data1.txt''''参数： path:路径 header:列名，等于None，是因为接下去列名会显示传递 names:需要传递的列名返回值： DataFrame形式数据结构''' data = pd.read_csv( path, header=None, names=['Population', 'Profit'])# 导入数据以后可视化观察一下data.head() # 查看记录，默认返回前5条# 查看数据集的描述信息 # count:样本个数 mean：均值 std:标准差 min:最小值 25%:四分之一位数 50%:中位数 75%:四分之# 三位数 max:最大值data.describe() 2.对得到的数据进行加工，方便计算 12345678910'''从样本集中分离出X和y'''data.insert(0, 'one', 1)# 插入第1列：X0=1cols = data.shape[1] # .shape返回一个元组，[0]为行数，[1]为列数# 提取X，y的值X = data.iloc[:, 0:cols - 1]y = data.iloc[:, cols - 1:cols] 初始化数据矩阵，这里转化成了np.matrix，但建议使用np.ndarray 1234567891011'''数据处理'''# 将dataframe结构转化成np的matrix# 当Theta取0时计算平均误差X = np.matrix(X.values)y = np.matrix(y.values)theta = np.matrix([0, 0])# 初始化学习速率和迭代次数alpha = 0.01epoch = 1000 计算函数1.代价函数 公式：$J(\theta_{0},\theta_{1})=\frac{1}{2m}\sum_{i=1}^{m}(h_{\theta}(x_{i})-y_{i})^{2}$，其中主要矩阵化：假设函数$h_{\theta}(x_{i})=\theta^{T}X$和$y_{(i)}$ （1）向量化：计算过程中注意矩阵乘法或是向量乘法的合法性。 （2）数据可以用np.matrix存储，但更建议使用np.ndarray （3）np.matrix和np.ndarray在乘法上有所区别:Numpy中的矩阵乘法 123456789101112131415161718'''函数名： 代价函数 参数： X：矩阵 -可用.shape查看维度(97,2) y：向量，用numpy.matrix的结构存储 -可用.shape查看维度(97,1) theta:向量，np.matrix的结构存储 -维度(1,2) 返回值： 代价函数值 '''def costFunciton(X, y, theta): inner = np.power(((X * theta.T) - y), 2) return np.sum(inner) / (2 * len(X)) 2.批量梯度下降： （1）公式： （2）作用：通过迭代的方式来寻找代价函数最小时的参数（$\theta_{j}$） （3）学习速率：如果过大，会导致无法到达代价值最小点（函数发散或震荡）；如果过小，则会使得迭代时间过长。 123456789101112131415161718192021222324252627'''函数名： 梯度下降法参数： X:矩阵 y:向量 theta:向量 alpha:学习速率 epoch：迭代次数返回值： theta:最后得到的两个参数theta_0,theta_1 cost:最后得到的误差'''def gradientDescent(X, y, theta, alpha, epoch): temp = np.matrix(np.zeros(theta.shape)) cost = np.zeros(epoch) # epoch为迭代次数 m = X.shape[0] # 样本数量 for i in range(epoch): temp = theta - (alpha / m) * (X.dot(theta.T) - y).T.dot(X) theta = temp # 记录一下每次更新后的误差 cost[i] = costFunciton(X, y, theta) return theta, cost 2*.正规方程法(Normal Equation) （1）同样也可以用于寻找代价函数最小时候的参数取值，与梯度下降法(Gradient Descent)比较 （2）公式：$\theta=(X^{T}X)^{-1}X^{T}y$ 12345'''特征方程'''def norEquation(X,y): theta=np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y) return theta 画图函数123456789101112x = np.linspace( data.Population.min(), data.Population.max(), 100) # 横坐标:linspace(start,end, num)从start开始到end结束，平均分成num份，返回一个数组f = final_theta[0, 0] + (final_theta[0, 1] * x) # 假设函数'''函数和散点图'''plt.figure(figsize=(8, 5))plt.plot(x, f, 'r', label='Prediction')plt.scatter(data['Population'], data.Profit, label='Training Data')plt.xlabel('Population')plt.ylabel('Profit')plt.title('Predicted Profit vs. Population Size') 结果图： 可以看出拟合效果还可以。 1234567'''绘制代价函数与迭代次数的图像'''plt.figure(figsize=(8, 5))plt.plot(np.arange(epoch), cost, 'r')plt.xlabel('iteration')plt.ylabel('cost') 结果图： 随着迭代次数的增加，代价函数值单调递减。 123456789101112131415161718192021222324'''绘制代价函数3D图像'''fig = plt.figure(figsize=(8, 5))ax = Axes3D(fig)# 绘制网格# X,Y valuetheta0 = np.linspace(-10, 10, 100) # 网格theta0范围theta1 = np.linspace(-1, 4, 100) # 网格theta1范围x1, y1 = np.meshgrid(theta0, theta1) # 画网格# height valuez = np.zeros(x1.shape)for i in range(0, theta0.size): for j in range(0, theta1.size): t = np.matrix([theta0[i], theta1[j]]) z[i][j] = costFunciton(X, y, t)# 由循环可以看出，这里是先取x=-10时，y的所有取值，然后计算代价函数传入z的第一行# 因此在绘图过程中，需要把行和列转置过来z = z.Tax.set_xlabel(r'$\theta_0$')ax.set_ylabel(r'$\theta_1$')# 绘制函数图像ax.plot_surface(x1, y1, z, rstride=1, cstride=1) 结果图： 关于3D图中的Numpy中的Meshgrid 1234567'''绘制等高线图'''plt.figure(figsize=(8, 5))lvls = np.logspace(-2, 3, 20)plt.contour(x1, y1, z, levels=lvls, norm=LogNorm()) # 画出等高线plt.plot(final_theta[0, 0], final_theta[0, 1], 'r', marker='x') # 标出代价函数最小值点 结果图： 线性回归-多变量(multiple variables)特征缩放(Feature scaling)也叫均值归一化 （1）公式：$x=\frac{x-\mu}{s_{1}}$，其中$\mu$为$x$的均值，$s_{1}$为$x$的最大值减去最小值，或者使用标准差。 （2）作用：在多个特征值情况下，如果某个特征值$x_{i}$的取值范围和另一个特征值$x_{j}$的取值范围相差太大，会减慢梯度下降的速度，因此需要用特征缩放，将不同特征值的取值限定在差不多的范围内。 e.g. $x_{1}$为房子的面积，取值范围0~2000；$x_{2}$为卧室的数量，取值0-5；那么对二者使用特征缩放，可得： （3）代码： 12data = (data - data.mean()) / data.std() # 除数可以用标准差也可以用max-min，因为pandas方便，所以使用标准差 参考资料[1] 机器学习 | 吴恩达机器学习第一周学习笔记 [2] 机器学习 | 吴恩达机器学习第二周编程作业(Python版） [3] 吴恩达机器学习作业Python实现(一)：线性回归 代码scp-1024/Coursera-ML-Ng]]></content>
      <tags>
        <tag>Machine Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习|吴恩达机器学习之逻辑回归]]></title>
    <url>%2F2019%2F04%2F03%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%2F</url>
    <content type="text"><![CDATA[一个名为回归确用于解决分类的算法，吴恩达coursera第三周 逻辑回归(Logic Regression)代码数据处理（1）常规读取数据的方法，值得注意的是X = data.iloc[:, :-1].values中.values作用是将DataFrame转化为ndarray。根据手册，更推荐使用.to_numpy()。 12345678910path = 'ex2data1.txt'data = pd.read_csv(path, names=('exam1', 'exam2', 'admitted'))data_copy = pd.read_csv(path, names=('exam1', 'exam2', 'admitted'))# 进一步准备数据，对结构初始化data.insert(0, 'One', 1)X = data.iloc[:, :-1].values y = data.iloc[:, -1].valuestheta = np.zeros( X.shape[1]) # 注意这里theta创建的是一维的数组，对于ndarray一定要注意一维时它的shape（和matrix有很大区别） 数据可视化1234567891011121314151617'''数据可视化'''def plot_data(data): cols = data.shape[1] feature = data.iloc[:, 0:cols - 1] label = data.iloc[:, cols - 1] # iloc 根据列的位置索引来切片 postive = feature[label == 1] negtive = feature[label == 0] # plt.figure(figsize=(8, 5)) plt.scatter(postive.iloc[:, 0], postive.iloc[:, 1]) plt.scatter(negtive.iloc[:, 0], negtive.iloc[:, 1], c='r', marker='x') plt.legend(['Admitted', 'Not admitted'], loc=1) plt.xlabel('Exam1 score') plt.ylabel('Exam2 score') 假设函数假设函数：（1）公式：$h_{\theta}(x)=\frac{1}{1+e^{-\theta^{T}x}}$，又名Sigmoid function，函数图像如下图所示，从图中可以看出$h_{\theta}$的取值范围0~1 （2）$h_{\theta}$的含义是：特征x的情况下，y=1的概率 12def sigmoid(z): return 1 / (1 + np.exp(-z)) 代价函数：如果这时再使用线性回归的$J(\theta_{0},\theta_{1})=\frac{1}{2m}\sum_{i=1}^{m}(h_{\theta}(x_{i})-y_{i})^{2}$将会导致函数图像不光滑。这样使用最小值法时容易导致无法找到全局最优解。因此需要使用新的代价函数。 （1）公式： 具体解释如下： 1234567'''cost function可以用矩阵实现也可以用ndarray实现，更建议使用后者'''def cost(theta, X, y): first = (-y) * np.log(Sigmoid.sigmoid(X @ theta)) # 这里*号是对应位置相乘而不是矩阵运算 second = (1 - y) * np.log(1 - Sigmoid.sigmoid(X @ theta)) return np.mean(first - second) 高级优化法：除了梯度下降法之外，还有其他几种优化方法，比起gradient descent，这些方法更适合处理大型数据且不需要你设置学习速率。 这些方法的具体原理非常复杂，但是，python的模块往往是非常强大的，因此往往只需要你计算一下到导数项即可 12def gradient(theta, X, y): return (1 / len(X)) * (X.T @ (Sigmoid.sigmoid(X @ theta) - y)) 再使用import scipy.optimize as opt中的.fmin_tnc来迭代,得到最终的$\theta$值。 123# 这里不使用梯度下降法，换成其他优化算法来迭代result = opt.fmin_tnc(func=cost, x0=theta, fprime=gradient, args=(X, y))final_theta = result[0] 检测准确率1234567891011'''预测标签参数： -参数：theta -样本：X返回值： -预测标签'''def predict(theta, X): probability = Sigmoid.sigmoid(X @ theta) return [1 if x &gt;= 0.5 else 0 for x in probability] 将得到的预测标签同数据原有的标签进行比对，得到准确率 123predictions = predict(final_theta, X)correct = [1 if a == b else 0 for (a, b) in zip(predictions, y)]accurcy = np.sum(correct) / len(X) # 准确率89% 也可以自定义一个样本预测一发 123# 输入一个数据进行预测test = np.array([1, 45, 85]).reshape(1, 3)predict_result = predict(final_theta, test) # 预测值y=1，概率为0.776 决策边界$h_{\theta}=0.5$时的直线，由假设函数图像可以看出，当$h_{\theta}=0.5$时，$z=\theta^{T}x=0$，这里图中是以$x2$作为纵坐标。 12345# 决策边界def plot_decision_boundary(theta, X): plot_x = np.linspace(20, 110) plot_y = -(theta[0] + plot_x * theta[1]) / theta[2] plt.plot(plot_x, plot_y, c='y') 画图函数1234plt.figure(figsize=(8, 5))plot_data(data_copy)plot_decision_boundary(final_theta, X)plt.show() 结果图为： 带正则化项的逻辑回归函数读入数据：12345path = 'ex2data2.txt'data = pd.read_csv( path, names=('Microchip Test1', 'Microchip Test2', 'Accept'))x1 = data.iloc[:, 0].valuesx2 = data.iloc[:, 1].values 可视化数据123456789101112def plot_data(data): feature = data.iloc[:, 0:2] label = data.iloc[:, 2] positive = feature[label == 1] negative = feature[label == 0] plt.scatter(positive.iloc[:, 0].values, positive.iloc[:, 1].values) plt.scatter( negative.iloc[:, 0].values, negative.iloc[:, 1].values, c='r', marker='x') 特征映射由可视化数据可知，如果单独用两个特征，是无法表示出决策边界的（欠拟合underfit）。因此需要映射多个特征。 这里将$x1$和$x2$映射为6个特征值 1234567891011def map_feature(x1, x2): degree = 6 x1 = x1.reshape((x1.size, 1)) # ndarray.size：数组中元素的个数 x2 = x2.reshape((x2.size, 1)) result = np.ones(x1.shape[0]) # 初始化一个值为1的数组(列向量) for i in range(1, degree + 1): for j in range(0, i + 1): result = np.c_[result, (x1**(i - j) * (x2**j))] # np.c_：列拼接 return result # 返回值即为特征值X 但这种映射可能导致过拟合（overfit），泛化能力差，因此还需要正则化（regularization）。 *正则化（1）原理： 因为过拟合是由于特征项过多引起的，减少特征的数量固然可以，还有一种方法就是正则化：减小$\theta_{j}$的值。 由图中可以看出，线性回归算法中添加两个正则化项——也叫惩罚项，1000$\theta_{3}$和1000$\theta_{4}$。上图可以看出，在利用优化算法求解参数时，要想让代价函数值变小，会使得$\theta_{3}$和$\theta_{4}$变得非常小，也就导致了$\theta_{3}x^{3}$和$\theta_{4}x^{4}$非常小，那么图中右边的假设函数就近似与左边的函数了。 实际操作中，我们很多时候并不知道究竟应该惩罚哪一项，所以实际上除了$\theta_{0}​$（全是1），所有项都会惩罚。 回到逻辑回归算法上也是一样的 （2）公式： 对于线性回归算法也类似： 代价函数：123456789def cost_reg(theta,X, y, lmd): # 不惩罚第一项 _theta = theta[1:] reg = (lmd / (2 * len(X))) * (_theta @ _theta) first = (y) * np.log(Sigmoid.sigmoid(X @ theta)) second = (1 - y) * np.log(1 - Sigmoid.sigmoid(X @ theta)) final = -np.mean(first + second) return final + reg 梯度函数（1）公式： （2）向量化：$\theta_{j}:=\theta-\alpha[\frac{1}{m} X^{T} (g(X^{T}\theta)-y)+\frac{\lambda}{m}\theta_{j}]$ 代码只需要计算蓝色括号中的内容，然后用优化算法迭代： 12345def gradient_reg(theta,X, y, lmd): # 因为不惩罚第一项，所以要分开计算 grad = (1 / len(X)) * (X.T @ (Sigmoid.sigmoid(X @ theta) - y)) grad[1:] += (lmd / len(X)) * theta[1:] return grad 优化算法：用于迭代计算$\theta_{j}$值。 123456result = opt.fmin_tnc(func=cost_reg,x0=theta,fprime=gradient_reg,args=(X, y, 1),) 画出决策边界不是代入假设函数来画！！在逻辑回归中假设函数时Sigmoid function，用于计算概率的！！ 12345678def plot_decision_boundary(theta): x=np.linspace(-1,1.5,50) plot_x,plot_y=np.meshgrid(x,x) # 先画网格 z=map_feature(plot_x,plot_y) z=z@theta # 画出边界 z=z.reshape(plot_x.shape) plt.contour(plot_x,plot_y,z,0,colors='yellow')]]></content>
      <tags>
        <tag>Machine Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[信息熵]]></title>
    <url>%2F2019%2F03%2F29%2F%E4%BF%A1%E6%81%AF%E7%86%B5%2F</url>
    <content type="text"><![CDATA[我们在生活中常常听到“这句话信息量好大。”、“这都是废话，没什么信息量。“，而我们却很少思考，这里的信息量究竟是什么意思？是否可以给出精确定义，甚至，量化它呢？ 前言1984年香农提出了”信息熵“的概念，解决了这个问题。 信息中的熵 因为熵不只在信息，还在物理等其他领域有定义，虽然他们本质一样，但表达上和所研究的问题上略有差异，这里特作说明。 可以这么理解：当一件事情（宏观态）具有多种情况时，对于观察者而言，其具体情况（微观态）的不确定度，称为熵。 显然这个定义看上去肯定是懵逼的，没事，用一个例子来解释。 假设我们在做一道单项选择题，这道选择题的答案具有4个选项。那么对你这个观察者而言，这个选择题正确答案的可能选择情况（四种选择情况：选A、选B、选C、选D）就是宏观态，而其中的每一个选择情况就被称为微观态。我们目前并不能确定谁是正确答案，所以其微观态就具有不确定性。那么此时此刻，”选择正确选项”这件事情对你而言，熵为2bit。 （1）、而正在你陷入迷茫之际，你的好基友小明来到了你的身边，悄悄地对你说：”有50%可能性选C。“ 这条消息中含有的信息量，帮助你调整了每个微观态的概率：从等概率（A：25%，B：25%，C：25%，D：25%）调整为（A：16.6%，B：16.6%，C：50%，D：16.6%），而这条消息中含有的信息量为0.21bit，所以才“使用”了这条消息之后，“选出正确选项”这件事情的熵为$2-0.21=1.79bit$ （2）、这时候，你突然想起来可以直接看参考答案啊，于是你打开了书的最后一页发现——这题的正确答案选C。同样，这条消息含有的信息量同样也帮你调整了概率，这次更直接的把C的概率改为了100%。那么这条消息的信息量为2bit，因为它帮助你完全消除了“选出正确选项”这件事情的不确定性。 从上面的例子中可以发现，信息量的作用，就是用来消除熵。 到这里，你应该对熵的概念有了大体的理解，但还有一个问题，就是信息量是如何计算的？ 信息量是如何计算的我们都知道，在定义重量的单位时，我们先是选择了一个参照物，比如一块砖头，我们把这块砖头的重量定义为1千克，剩下的所有物体的重量都可以利用这块砖头来表示，比如把我和一堆砖头放在天平上，当达到水平状态时，只要数出砖头个数——65个，那么就可以知道我的重量是65kg。 基于同样的道理，我们先找一个参照物事件——扔硬币！它只有2种等概率情况，即50%正面50%反面（微观态）。它的熵（不确定性）我们记为1bit，那么再回顾一下你的选择题，在刚开始时，它的等概率情况有4种，所以它的熵就是2bit。 同样推断，如果一件事情的等概率情况有8种，那么它的熵就是4bit…那是不可能的。 你先仔细想想，什么事情具有8种等概率情况？对的，扔3枚硬币，所以熵是3bit而不是4bit。原因是，扔硬币结果的个数和硬币之间是指数关系而不是线性关系。 所以我们可以总结：在各个微观态之间是等概率的情况下，把宏观态的熵记为n，微观态的个数记为m，就可以得到 $n=log_{2}m​$ 但问题又来了，如果各个微观态之间不是等概率的情况呢？ 显然无法直接用上面的公式，这里回到“选择题事件”的（1）中，当我们得知了小明给出的消息之后，熵的bit数为：$n=p_{A}log_{2}(1/p_{A})+p_{B}log_{2}(1/p_{B})+p_{C}log_{2}(1/p_{C})+p_{D}log_{2}(1/p_{D})$ 其中$p$为每个选项的概率。这里对数中概率取倒数的原因：概率的倒数等于其发生在等概率情况的个数。 计算后熵 $n=1.79bit$ ，则小明的消息中的信息量就是0.21bit 。 以上的内容均来自 B站—YJango中10~11期的内容。 计算熵的公式那么可以做出一个总结，如果一个随机变量A有k中可能取值，其中第i种发生的概率为P(i)，那么信息熵的公式为： 由此也可见，一个取值的概率越低，他的熵就越高，你确定他所需要的信息量就越大。也可以证明，等概率情况下，信息熵的值最大$^{[1]}$。 而在等概率（每个情况发生的概率都为P(a)）情况下，我们需要了解一个随机变量所需要的信息量，为$L=log_{2}(1/P(a))$，这种说法由R.V.L.哈莱特与1928年提出$^{[2]}$，早于香农，实际上这里求的也就是熵，只不过那时候没这么叫。（见 信息量和信息熵的区别） 当时看到这里我有一个巨大的疑问，如果按这个公式计算，那么“选择题”中的（1），小明的消息提供的信息量应该为1bit啊怎么会是0.21bit。 后来我是这么理解的，如果单独把”50%的可能性选C”看成随机变量的话，你作为观察者所面对的情况实际上只有 2种：选C或者不选C。那么的确熵为1bit。 而放回到原来的事件中，情况为4种，这条信息只不过使得每个情况的概率都改变了，或者说，仅仅是4种情况之间的权重发生了改变，需要通过期望的方式来计算熵（1.79bit），再相减，得出结果（0.21bit）代表了这条信息在当前所讨论的宏观态下消除的不确定性。 更进一步说，其实这里隐藏了一个条件，我们默认把”50%的可能性选C“这件条信息当成是确定的（熵为0而不是单独情况时的1bit），并且在宏观态中根据信息调整了微观态的权重，而这时这条信息对这个宏观态产生了影响，使其熵减了0.21bit。 信息量和信息熵 信息熵就是上面说的熵，用“信息熵”的叫法是为了和信息量对称 信息熵：描述的是一个事件（宏观态）的不确定度，也就是说，信息熵是宏观态的一个属性，并不因为观察者而产生变化，比如你知道了一道选择题的答案，那么这道题的信息熵仅仅对你而言是0bit（也就是说你对这道题已经是确定了的），这时另外有人看到了这道选择题，那么对他而言，这道题的信息熵仍然为2bit。 信息量：你解开一个不确定事件的过程，在这个过程中你不断的获得信息量，来消除信息熵。 二者的本质上存在差异，但其实是角度不同导致的。 比较好的表述是: 信息熵是 对于事件A，我们对A不了解的程度。 换句话说， 就是我们还需要多少信息量才能完全了解事件A 所需要的信息量就是信息熵。 而信息熵很大的意思是事件A本身所携带的信息量很大。 引用自信息熵越大，信息量到底是越大还是越小？ - 捣衣的回答 - 知乎 熵在物理中的运用简单来说，热力学第二定律推导出熵增原理：孤立系统的熵永不减少。进而判断宇宙处在一个不断熵增的状态，因此总有一天会达到“热寂寞” 有兴趣可参考几个视频 热力学第二定律是什么？“麦克斯韦妖”是什么鬼？李永乐老师告诉你 熵到底是什么？一副牌中抽三张为同花的概率是多大？ 【学习观12】阻碍人类永生的原因是？ 参考资料[1].信息熵是什么？ - 柯伟辰的回答 - 知乎 [2].信息量—百度百科]]></content>
      <tags>
        <tag>Reading</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[YJango学习观笔记及感悟]]></title>
    <url>%2F2019%2F03%2F25%2FYJango%E5%AD%A6%E4%B9%A0%E8%A7%82%E7%AC%94%E8%AE%B0%E5%8F%8A%E6%84%9F%E6%82%9F%2F</url>
    <content type="text"><![CDATA[b站-YJango，个人还是对这位先生的 视频地址：b站-Yjango，个人还是对他表示敬意的，他的视频挺具有启发性。 本文同样参考了 如何评价知乎用户 YJango 的公众号？ - Makise Chris的回答 - 知乎 第一季01.何为学习学习就是从有限例子中找出规律的过程，而这个规律就是知识。 02.如何学习因为人脑机能原因，单单记忆一个知识，显然远远达不到掌握它的水平。 而首先应该做的是明确问题（输入）和答案（输出）。 然后通过大量例子来理清二者的关系。接下来根据你的问题和答案，找出这些例子中符合问题及答案的规律。这两步也就是华罗庚的”先把书读厚（大量例子体会关系），再把书读薄（找出知识来压缩例子）。” 最后，需要用新的问题来验证你的例子是否正确。 03.学习误区学习误区指什么？见图片黄字。 学习方式分为两类：（1）运动类：语言、运动（2）思考类：数学、逻辑 举了个语言的例子： 错误方式： 正确方式： 老实说我认为这个观点见仁见智，尤其是摒弃中文作为中间媒介这件事，姑且不论对错，可行性就值得怀疑了。 04.运用误区运动类的学习可以同时进行，比如边走路边聊天；而思考类则不行，只能串行操作，所以当我们遇到一个大的问题（输入）时，如果无法直接得到输出，就需要将这个输入拆分，分别解决。 05.思维导图其功能不在于记忆，而在于克服学习和运用中的误区。 在构建思维导图的过程中，你会先找到关键词，然后问自己它是什么？从而不断的联想起具体的例子，进而分析这些例子的共同输入和输出，找出规律；而当问自己它的作用或目的时，实际在思考输入是怎么变成输出的。输入输出可以代表一类事物中任何一种情况，因此被称为变量。而从输入变成输出的这一过程称为函数。而确定了这个“函数”，等下一次遇到未知的情况时，利用这个函数就可以解答出输出。随后你会用一个更好的关键词来代表你所找出的关系，一种是动宾结构，因为它描述了输入和函数，也就确定了输出，但当人们开始传播知识时，动宾结构也会名词化。 还有一种知识本身就是名词，会让你觉得没有输出，但这种知识的输出是分类任务中的类型，描述它的是主谓结构：是或不是/是否属于/属于哪个。 思维导图最强大的地方在于对知识的拆分，构建出知识网络，而知识网络中有些内容是你已经知道的，即具有重用性，学习新内容时，可以利用这些重用性，加快学习速度。拆分知识既可以用在学习未知知识上，也可以用在运用已有知识上（分而治之）。 而拆分知识的能力也是一种特殊的知识，称为二阶知识，不同于一般的知识用于描述信息与信息的关系，二阶知识用于描述知识与知识的关系。 视频中举的例子就是思维导图，假设我们需要构建一个有关于思维导图内容的思维导图。 那么首先，找出关键词——思维导图。然后问自己：思维导图是什么？因此联想出思维导图使用的具体例子：任务清单、知识体系、任务关系…，接着分析这些例子的共同输入——杂乱信息，共同输出——知识网络。而当你问它的作用时，也就是在思考输入是怎么变成输出的。一类事物，这个例子中，假设它代表的是任务清单这一类，那么输入的杂乱信息和输出的知识网络就是任务清单这一类事物的特定情况。而这个“函数”：可以理解为思维导图的画图方法。然后是找动宾结构，这里是压缩信息（其中压缩是函数，信息是输出），当人们开始传递知识时，压缩信息就被名词化成了思维导图。 知识本身属于名词的没说例子。]]></content>
      <tags>
        <tag>Reading</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关于记忆方法]]></title>
    <url>%2F2019%2F03%2F25%2F%E5%85%B3%E4%BA%8E%E8%AE%B0%E5%BF%86%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[阅读了几篇关于记忆方法的文章，有所感悟，结合了一下自己经历，选了几个我认为有长期实践意义的方法，在此记录，此文可能会长期更新，毕竟记忆方法也是要不断调整的。 前言我认为针对不同问题，应该具有不同的记忆方法。比如： 单词记忆，那么也许联想类方法比较有用； 记忆某些抽象概念，如剩余价值的含义，也许更适合用关键词法； … 所以记忆方法不能一概而论，犯教条主义错误。 记忆整体框架结构化，也就是常说的思维导图，从全局出发，有助于你理清思路，尤其当你面对一个比较庞杂而又相互关联的知识（比如政治一整个章节的内容）。现在很多书籍都会替你整理好，但是请务必要用自己的方式去整理一遍，也许有人会说，我自己思考的和书上整理的一样啊，那么也请合上书默写出来。 实际上你默写也好，自己写也罢，这里的关键是在这个过程是一定要去==主动思考==（敲重点！）这个庞杂的知识的脉络到底是什么样的。 思维导图： 参考资料：知乎专栏——Yjango 记忆段落/句子内容一般来说，我们学习的总是不熟悉的内容，再加上作者和你在表述习惯上的差异，才会导致你无法理解作者想要传达的意思，才会导致只能死记硬背。 所以同样需要我们用自己的方式理解并记忆作者想要传达给你的意思。那么如何去用自己的语言表述出来呢？ 引用一句话：记忆书本原有的关键词、知识点等“点”信息，而后自己去连“点”成“线”——指的是把这一块的“点”信息串成有逻辑的内容$^{[2]}$。 举个例子： 类是对一群具有相同特征或者行为的事物的一个统称，是抽象的，不能直接使用。特征被称为”属性“，行为被称为”方法“。 在这个句子中，对我而言，关键词是”类，特征=属性，行为=方法“。所以对我而言只需要记住这几个内容，剩下的用自己的话重复一遍： 类是具有相同的特征和行为的事物的统称，是一个抽象概念。实际运用时，特征又叫做属性，行为又叫方法。 参考资料：[2].你有什么值得分享的高效学习方法？ 这个问题下最高赞的回答 我认为这个方法尤其适用于抽象概念——尤其是文科类——的记忆。 记忆关键词联想记忆法，这个方法算是很常见了，但我认为可以看一下这个视频：·TED画图记忆方法。 视频中提到可以用画图的方式来替代文字记忆，但我认为在记忆大量内容的时候这种方法太低效。不过比起单纯的联想，画图具有更强的可操作性（有时候真是联想不到），而且动笔的过程中又进一步加深了你的思考。所以我认为可以在上一个记忆法中，记忆关键字时，根据需要使用这个方法。 还是上面的例子，当我们提取关键字后，可能还是会觉得抽象，类是什么？特征和行为又怎么理解？这时候把这几个词语通过联想的方式去理解记忆： 类，可以看做是制造一样东西（比如飞机）的设计图；属性，就好比飞机都有一对翅膀；方法，就好比飞机如何起飞。还可以接着拓展，比如用类创建一个对象，就相当于用设计图纸去制造一架飞机。 其实无论哪种方法，归根结底都是要把新的知识转化为自己更熟悉的方式，加以理解并记忆。]]></content>
      <tags>
        <tag>Reading</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[正向传播的向量化实现]]></title>
    <url>%2F2019%2F03%2F23%2F%E6%AD%A3%E5%90%91%E4%BC%A0%E6%92%AD%E7%9A%84%E5%90%91%E9%87%8F%E5%8C%96%E5%AE%9E%E7%8E%B0%2F</url>
    <content type="text"><![CDATA[神经网络中正向传播(Forward propagation)的向量化(Vectorized implementation) 其实没什么难的，就是几个符号理解起来可能有点费劲，下面简单解释一下： 从这个式子入手：$z^{(2)}=\Theta^{(1)}*a^{(1)}$ $z^{(2)}$等于信号$a^{(1)}$在传输过程中乘上响应的权重$\Theta^{(1)}​$，展开来写的话就是左图中等号右边的灰色线框。 $a^{(1)}$（activation values of layer one）是指第一层中$x_{0}$~$x_{3}$构成的一维向量，上标1就是指第一层。 $\Theta^{(1)}$是指传递过程中不同信号所对应的权重，反应在公式上就是$\Theta^{(1)}_{10}$~$\Theta^{(1)}_{33}$所构成的矩阵。 $\Theta^{(1)}_{10}$下标的10是指，接收信号的神经元为下一层的第一个，发出信号的为这一层的第0个神经元。 这里额外说一下这个$\Theta^{(1)}​$的维度，如果在第$j​$层有$s_{j}​$个神经元(units)，在第$j+1​$层有$s_{j+1}​$个神经元，那么对应的权重的维度为$s_{j+1}*(s_{j}+1)​$。 其中$s_{j}+1$是因为上一层在传递时，默认会传递一个$s_{0}=1​$ 以上是传递过程中的，$z^{(2)}$传递到神经元之后，就通过激励函数（图中是S函数）求出需要传递个下一层的信号$a^{(2)}$ 即公式：$a^{(2)}=g(z^{(2)})$ 依次类推，即可求出最终的假设函数$h_{\Theta}(x)$ 本文是从信号发出的角度去阐述，但似乎更多文章时站在信号接收的角度，但意思是一样的 参考文章 [1]. 机器学习笔记（2）—— 神经网络]]></content>
      <tags>
        <tag>Machine Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sklearn中的函数classification_report]]></title>
    <url>%2F2019%2F03%2F20%2Fsklearn%E4%B8%AD%E7%9A%84%E5%87%BD%E6%95%B0classification-report%2F</url>
    <content type="text"><![CDATA[classification_report函数主要用于显示主要分类指标的文本报告 1.前言在报告中显示每个类的精确度、召回率等信息（可以用来检测回归算法的准确度）。 2.classification_report()参数详见官方文档，这里只说几个重要参数 y_true：1维数组，目标值 y_pred：1维数组，分类器返回的估计值 label：数组，报告中显示的类标签索引列表 target_names：字符串列表，当和label匹配时作为label的名称 3.classification_report()返回值 解释一下两个名词 正样本：与你所要研究的目的相关 负样本：与你所要研究的目的无关 举个例子：如果你要做一间教室里的人脸识别，那么正样本就是人脸，负样本就是课桌、门窗之类的 TP(True Positive): 预测为正样本， 实际为正样本（预测正确） FP(False Positive): 预测为正样本， 实际为负样本 （预测错误） FN(False Negative): 预测为负样本，实际为正样本 （预测错误） TN(True Negative): 预测为负样本， 实际为负样本 （预测正确） 精确度(precision)=正确预测的个数(TP)/预测为正样本的个数(TP+FP) 检索结果中，都是你认为应该为正的样本（第二个字母都是P），但是其中有你判断正确的和判断错误的（第一个字母有T ，F）。 召回率(recall)=正确预测值的个数(TP)/实际为正样本的个数(TP+FN) 检索结果中，你判断为正的样本也确实为正的，以及那些没在检索结果中被你判断为负但是事实上是正的，或者说你没预测到的（FN）。 F1值=2*精度*召回率/(精度+召回率) 不明白的话参考以下代码： 1234567891011121314151617In [4]: from sklearn.metrics import classification_report ...: y_true = [1, 2, 3, 3, 3] ...: y_pred = [1, 1, 3, 3, 2] ...: labels =[1,3,2] ...: target_names = ['labels_1','labels_2','labels_3','labels-4'] ...: print(classification_report(y_true,y_pred,labels=labels,target_names= t ...: arget_names,digits=3)) ...: precision recall f1-score support labels_1 0.500 1.000 0.667 1 labels_2 1.000 0.667 0.800 3 labels_3 0.000 0.000 0.000 1 micro avg 0.600 0.600 0.600 5 macro avg 0.500 0.556 0.489 5weighted avg 0.700 0.600 0.613 5 最右边support列为每个标签的出现次数(权重)。 micro avg：计算所有数据中预测正确的值，比如这里是3/5=0.6 macro avg：每个类别指标中的未加权平均值(一列)，比如准确率(precision)的macro avg是：$(0.5+1.0+0)/3=0.5$ weighted avg：每个类别指标中的加权平均，比如准确率(precision)的weighted avg是：$(0.51+1.03+0*1)/3=0.7$ 4.参考资料[1] 博客园 [2] CSDN博客 [3] CSDN博客 [4] CSDN博客——TP、TN、FP、FN解释说明 [5] 读懂 sklearn 的 classification_report]]></content>
      <tags>
        <tag>Python</tag>
        <tag>Machine Learning</tag>
        <tag>sklearn</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Numpy中的矩阵乘法]]></title>
    <url>%2F2019%2F03%2F19%2FNumpy%E4%B8%AD%E7%9A%84%E7%9F%A9%E9%98%B5%E4%B9%98%E6%B3%95%2F</url>
    <content type="text"><![CDATA[简单介绍一下Numpy中.dot()、*、multiply()和@的区别 1. np.multiply()函数数组/矩阵对应的位置相乘。 2. np.dot()函数2.1. 当数组/矩阵秩为1(即向量)时，执行点积。 2.2. 当数组/矩阵秩大于2时，执行矩阵乘法。 2.3. .dot可以被数组对象调用，也可以通过numpy库调用（被matrix调用可以执行，但会报错）。 3. 星号(*)乘法运算3.1. 对数组执行对应位置相乘。 3.2. 对矩阵执行矩阵乘法。 4. @乘法运算4.1. 对于矩阵乘法而言，完全等价于.dot()。 4.2. 区别在于，当a和b中有一个是标量时，只能用.dot()否则会报错。]]></content>
      <tags>
        <tag>Python</tag>
        <tag>Numpy</tag>
        <tag>Machine Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Numpy中的Meshgrid]]></title>
    <url>%2F2019%2F03%2F16%2FNumpy%E4%B8%AD%E7%9A%84Meshgrid%2F</url>
    <content type="text"><![CDATA[Numpy是在利用Python进行科学计算和数据处理时肯定会使用到的模块(moudle)。 本文用比较通俗的语言讲解一下其中的Meshgrid函数 1.Meshgrid前言简单来说，Meshgrid就是在用两个坐标轴上的点在平面上画网格，当然也可以用三个坐标，但是为了方便理解，下面都用两个坐标轴举例。 2.Meshgrid参数详情咨询官方文档，最常用的就是传入两个一维数组 123x = np.linspace(-10,10,2)y = np.linspace(-10,10,2)xv,yv = meshgrid(x,y) 3.Meshgrid返回值xv的返回值，按行相等： 12[[-10,10], [-10,10]] yv的返回值，按列相等： 12[[-10,-10], [10,10]] 从上面很容易看出，Meshgrid实际上是返回两个矩阵，两个矩阵不同之处，下面用一张图来表示一目了然。 可以看出就是通过两个矩阵的方式完成了网格的绘制。 参考资料[1] Python-Numpy模块Meshgrid函数 [2] Numpy中Meshgrid函数介绍及2种应用场景]]></content>
      <tags>
        <tag>Python</tag>
        <tag>Machine Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[vscode常用快捷键]]></title>
    <url>%2F2019%2F03%2F14%2Fvscode%E5%B8%B8%E7%94%A8%E5%BF%AB%E6%8D%B7%E9%94%AE%2F</url>
    <content type="text"><![CDATA[Ctrl+D：选中当前单词，再按一次选中下一个。（妈的谁用谁知道） Ctrl+L：选中当前行 ctrl+shift+alt+up/down：多行编辑 未完待续…]]></content>
      <tags>
        <tag>vscode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[浅谈分位数]]></title>
    <url>%2F2019%2F03%2F12%2F%E6%B5%85%E8%B0%88%E5%88%86%E4%BD%8D%E6%95%B0%2F</url>
    <content type="text"><![CDATA[在Coursera看Ng的视频时，因为不想用MATLAB和Octave，改用了Python实现，由于用到了pandas这个模块(moudle)中的.describe()方法，对它输出值(见下面代码段)中的分位数（25%，50%，75%）没太弄懂，再查阅各种资料之后，做一个简单的总结。 1234567891011&gt;&gt;&gt; s = pd.Series([1, 2, 3])&gt;&gt;&gt; s.describe()count 3.0mean 2.0std 1.0min 1.025% 1.550% 2.075% 2.5max 3.0dtype: float64 中位数首先需要把数据集从小到大排列（从大到小也行，取决于实际需求）。 1、若样本个数n为奇数，那么只需要取中间值（第n/2个）就是数据集的中位数。 e.g.数据集[1,2,3]，则中位数为2 2、若n为偶数，则取中间两个数的平均值作为中位数 e.g.[1,2,3,4]，则中位数为(3+2)/2=2.5 中位数相对简单且好理解，其往往运用在数据集中某个样本出现明显异常值时。比如[1,2,3,4,1000000]，这时候如果去中位数是3，但如果取平均数，就会产生巨大误差。 四分位数（25%,50%,75%）四分位数是作为中位数的拓展，关于如何计算四分位数我发现了两种不同的计算方法，得出的数值会存在差异。wiki上说对于该值的选取仍然存在争议，但是无论用那种方法，都能够将数据集划分，从而进一步分析数据变量的趋势。 解法一：分步法（自己取的）这个方法很简单，就是先排序然后取中位数（50%），然后以中位数为分界线，把数据集切分为两部分再分别取中位值（25%，75%） 1、若样本个数n为奇数 比如[1,2,3] 50%分位：(1+3)/2=2； 25%分位：(1+2)/2=1.5； 75%分位：(2+3)/2=2.5； 这个例子举的有点不好，带小数点了，但是意思理解就行，就是取中位数—划分—取中位数 2、若样本个数n为偶数 [1,2,3,4,5,6,7,8] 50%分位：(4+5)/2=4.5； 25%分位：(2+3)/2=2.5； 75%分位：(6+7)/2=6.5； 而在pandas中也是用这种方法计算的，wiki词条中的例子也是。 解法二：这个方法有点麻烦。 1、若样本个数n为奇数 首先确定四分位的位置。 设置下四分位数、中位数和上四分位数，记为Q1、Q2、Q3 Q1位置=(n+1)/4 (25%分位) Q2位置=2(n+1)/4 (50%分位) Q3位置=3(n+1)/4 (75%分位) e.g.[13、13.5、13.8、13.9、14、14.6、14.8、15、15.2、15.4、15.7] Q1位置=(11+1)/4=3 Q2位置=2(11+1)/4=6 Q3位置=3(11+1)/4=9 对应的分位值这分别为13.8、14.6、15.2 2、若样本个数n为偶数 依然先确定位置Q1、Q2、Q3，这里直接用一个例子来说明，[14、15、16，16、17、18、18、19、19、20、2l，21、22、22、23、24、24、25、26、26]，例子中n=20。 Q1位置=(20+1)/4=5.25，这里可以看到位于[5]、[6]位置是17和18，则Q1=17+(18-17)*(5.25-5)=17.25 同理可得： Q2位置=2(20+1)/4=10.5，则Q2=20+(21-20)*(10.5-10)=20.5 Q3位置=3(20+1)/4=15.75，则Q3=23+(24-23)*(15.75-15)=23.75 参考资料1、维基百科 2、MBA百科]]></content>
      <tags>
        <tag>Machine Learning</tag>
        <tag>Math</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[计算机编码问题]]></title>
    <url>%2F2019%2F03%2F05%2F%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BC%96%E7%A0%81%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[整理了一些我对进制之间转换的理解，有些地方为了记忆方便可能不太准确，欢迎指正。 我们日常最常用的进制当然是十进制DEC，因为最符合人类的习惯（数手指）。 对于计算机而言，最熟悉的则是二进制BIN（高低电平表示1和0）。 此外常用的还有八进制OCT，十六进制HEX。 写在前面以最常见的十进制为例，我们对于数字的记录方式是通过数位+基数(0~9)，比如152可以看成是1*10²+5*10¹+2*10º，数位从低到高为个位、十位、百位… 这种方式对应到其他进制只是数位和基数都不相同，其组成数字的法则是一样的。 比如二进制从低位到高位可以看做是个位、二位、四位、八位…. e.g. 152的二进制表示是10011000，就可以看做是$12^7+12^4+1*2^3$ 一般而言，不同进制之间转换的时候都会以BIN作为桥梁。 不同进制之间转换不同进制之间如何熟练转换-知乎 因为OCT和HEX都是BIN的倍数，所以一般转换时都是分位转换，即OCT的一位数相当于BIN的三位，HEX的一位数相当于BIN的四位。 常听见的几种编码ASCII标准码：最高位奇偶校验位，后七位进行存储，可存128个字符。 Unicode：因为计算机的普及，中文、日文以及其它文字使得更多的字符需要被编码，为了统一标准，因此Unicode诞生了，用两个字节来表示一个字符。 UTF-8：因为字母只需要一个字节，而汉字需要两个字节，这样在使用Unicode表示字母的时候就导致内存空间被浪费，因此再Unicode的基础上又诞生了更通用的UTF-8,他的特点是对不同范围的字符使用不同长度的编码。]]></content>
      <tags>
        <tag>Computer foundation</tag>
        <tag>编码</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python爬虫实践1]]></title>
    <url>%2F2019%2F03%2F01%2FPython%E7%88%AC%E8%99%AB%E5%AE%9E%E8%B7%B51%2F</url>
    <content type="text"><![CDATA[项目001：爬取本地html文件要求：爬取评分高于4分的文章标题和分类 解决步骤：step1、用BeautifulSoup解析网页 step2、查找需要用的tag step3、提取tag中有用的信息 代码如下： 1234567891011121314151617181920212223242526272829303132from bs4 import BeautifulSoupdata = []path = '1_2/1_2code_of_video/web/new_index.html' #vscode的相对路径是对于工程目录而非当前目录#用with open() as f的方法比f.open()好，因为前者封装了f.close()，省去了清除内存的麻烦。with open(path, 'r') as f: Soup = BeautifulSoup(f.read(), 'lxml') #这里用select()和find_all()都行 titles = Soup.select('ul &gt; li &gt; div.article-info &gt; h3 &gt; a') pics = Soup.select('ul &gt; li &gt; img') descs = Soup.select('ul &gt; li &gt; div.article-info &gt; p.description') rates = Soup.select('ul &gt; li &gt; div.rate &gt; span') cates = Soup.select('ul &gt; li &gt; div.article-info &gt; p.meta-info') #因为要通过一个tag值来判断，随后返回该tag值对应的另一tag值#所以这里用dict来处理for title, pic, desc, rate, cate in zip(titles, pics, descs, rates, cates): info = &#123; 'title': title.get_text(), 'pic': pic.get('src'), 'descs': desc.get_text(), 'rate': rate.get_text(), 'cate': list(cate.stripped_strings) &#125; data.append(info)for i in data: if len(i['rate']) &gt;= 3: print(i['title'], i['cate']) 最后输出1234Sardinia&apos;s top 10 beaches [&apos;fun&apos;, &apos;Wow&apos;]How to get tanned [&apos;butt&apos;, &apos;NSFW&apos;]How to be an Aussie beach bum [&apos;sea&apos;]Summer&apos;s cheat sheet [&apos;bay&apos;, &apos;boat&apos;, &apos;beach&apos;] 其余笔记1、 Beautiful Soup将复杂HTML文档转换成一个复杂的树形结构,每个节点都是Python对象,所有对象可以归纳为4种: Tag , NavigableString , BeautifulSoup , Comment . ——官方手册 2、select()和find_all() 1234#find_all()的返回值类型&lt;class 'bs4.element.ResultSet'&gt;#select()的返回值类型&lt;class 'list'&gt; 返回的都是包含标签的列表。 参考资料 [1] with open() as f和open()的区别 [2] vscode文件路径问题]]></content>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[十分钟世界史]]></title>
    <url>%2F2019%2F01%2F31%2F%E5%8D%81%E5%88%86%E9%92%9F%E4%B8%96%E7%95%8C%E5%8F%B2%2F</url>
    <content type="text"><![CDATA[Crash Coursera：十分钟世界史 0、农业革命：我们现在所存在的很多问题，都是由农业革命引起的，而这个革命却在不同的文明之间都发生了。尽管很难说清楚，人类不约而同的选择这一步是否正确，但无论如何我们都无法回头了。 Season 11）印度河谷文明：（2300 BC~1750 BC）挖掘出了遗物，但是文字失传无法翻译。 2）美索不达米亚文明：（3000 BC~2000 BC）发明了书写和税收，早起的政治形态，还有《汉谟拉比法典》。 其中著名的王朝有巴比伦和亚述。 3）古埃及：（3000 BC~332 BC）存在时间非常长，比起两河和恒河，这应该感谢一下尼罗河的温柔。现存的金字塔等都是这个文明遗留下的 4）古希腊人和波斯人（800 BC~146 BC）古希腊的文明有多灿烂？随便举几个例子，雅典，苏格拉底，奥运…还有那些耳熟能详的战争：马拉松，斯巴达勇士…西方文明的源头之一。波斯帝国了解不多，居鲁士大帝，温泉关战役。 5）印度（区分于古印度）：（1500 BC~）来自高加索的雅利安人进入印度，婆罗门教诞生，确立种姓制度，认为人死后会轮回，后来释迦摩尼悟出佛道，即经过修行，甚至有可能在这一世就打破轮回。 7）亚历山大：（356 BC~323 BC）雄才大略的君王，把希腊，埃及，波斯统一麾下，甚至大军打到了印度河流域，可惜英年早逝。 8）罗马帝国：主要人物凯撒（102 BC~44 BC），早起与庞培、克拉苏组成三头同盟，后打败庞培，成为终身独裁官，公元前44年，遭元老院成员暗杀身亡，后其养子屋大维创立罗马帝国，即奥古斯都大帝。 基督教：诞生于公元1世纪，最早流行与犹太人之间，所以要杀耶稣也应该是罗马人….395年罗马分裂，东罗马帝国信仰基督教分支东正教。 东罗马帝国：被历史学家称为“拜占庭帝国”，都城君士坦丁堡（今伊斯坦布尔）。期间首都曾被十字军攻陷，后收回。于1435年灭亡。 10）伊斯兰教（570 CE ~632 CE） 夹在拜占庭帝国和萨珊王朝（被亚历山大贡献后重建的波斯帝国）之间的古莱什部落即将迎来一位伟大的先知。 作为世界三大宗教之一的伊斯兰教，其中最著名的人物是被他们称作“最后的先知”——穆罕默德，与另外两个宗教领袖所不同的是，穆罕默德能打仗，在他的领导下，阿拉伯半岛开始以伊斯兰为核心建立统一的穆斯林国家。他死后，因为继承权问题，伊斯兰教主要分为逊尼和什叶两派，直至今日。 11）中世纪（公元5世纪~公元15世纪）：开始于西罗马帝国灭亡。 十字军东征：为了收复落入伊斯兰手中的圣城耶路撒冷，教皇发动的东征，总共9次（第4次是针对拜占庭帝国） 12）非洲：事实证明古代的非洲并非想象的那样贫穷，也出现过许多盛极一时的帝国，比如：马里帝国。 13）蒙古帝国（1206 CE~1635 CE）：1206年铁木真被推举为可汗，从此开始他南征北战的一生。东起日本海，西至地中海，北跨西伯利亚，南至波斯湾，蒙古帝国盛极一时，在成吉思汗死后，蒙古帝国分裂成四大汗国，其后各自走向终结 14）印度洋贸易（公元11世纪~公元13世纪）：有很多小国依靠收取船只停靠费和关税发家，但后来的事实证明，依靠贸易是无法稳固国家的。 15）威尼斯共和国与奥斯曼帝国： 威尼斯共和国（681 CE~1791 CE）：由威尼斯总督领导，擅长水上贸易，商人当道的国家。后灭于拿破仑之手。 奥斯曼帝国（1299 CE~1922年 CE）：土耳其人创立的国家，1435年消灭东罗马帝国，定都君士坦丁堡，15-19世纪唯一能和欧洲基督国家相提并论的伊斯兰国家，一战战败后分裂。 16)基辅罗斯~俄罗斯：公元882年建立基辅罗斯，11世纪分裂为多个公国，13世纪20年代被蒙古国征服，此后莫斯科公国开始做大做强。 17）大航海时代（15世纪末~16世纪初）： 视频里提到了3个人：郑和、达伽马和哥伦布。 我们非常熟悉的哥伦布其实终其一生都认为他所到达的地方是…印度，直到意大利人亚美利哥到达南美之后，才认定哥伦布发现的是亚欧之外的新大陆。 美洲大陆的发现影响巨大，物种交换（猪，牛等动物运往美洲）、疾病的传播（欧洲人给印第安人带去了天花，印第安人也回赠了花柳）。当然其中最臭名昭著的是奴隶贸易。 你无法把奴隶贸易归罪与任何一个团体、人物或者国家。在那个时代里，每个参与者都是肮脏的，欧洲人把非洲黑奴运往美洲，然而他们并非用武力去获得黑奴，相反，用的是贸易。为他们提供黑奴的也是非洲人，这些黑奴的同胞，或者说，领导者。 18）文艺复兴（14世纪~16世纪）：一场欧洲思想文化运动。 这是百度百科的说法，并不错误，但容易误导人。（就像笛卡尔用x，y来刻画横纵坐标导致后来的学生始终无法把思维转化过来一样）事实上文艺复兴是一个时间跨度两百年，而在这两百年内，间断性而非连续性发生的。 而且是精英运动而没有群众基础，毕竟大部分人还是以填饱肚子为主。当然这个时代各行业都出现太多伟大人物了。 19）七年战争（1754年~1763年）：主要是欧洲国家参战，英国和法国和西班牙在贸易和殖民地问题上的争夺，普鲁士与奥地利的争权，俄罗斯也意图从中取利。最终，英国与普鲁士的联盟取得了胜利，而这次胜利也为英国赢得了大半个北美，为日不落帝国的诞生打下了基础。 20）独立战争（1775~1783年）：1773年“波士顿倾茶事件”，来自英国的北美殖民者不满英国国会所颁布的法案所导致的冲突。1775年，莱克星顿，打响了武装反抗英国统治的第一枪，1776年大陆会议通过了《独立宣言》。 尽管不否认独立战争带来了许多积极因素，比如民主思想的传播等。但独立战争，其实也就是欧洲来的殖民者对英国政府统治的反抗罢了…跟快被灭光的印第安人这些真正的原住民好像也没多大关系，那些被贩卖到美洲的黑奴也没获得多大的权益。 21）法国大革命（1789年~1794年）：一场初衷无比美好，过程惨不忍睹的革命。 1774年，路易十六召开三级会议； 1789年，攻占巴士底狱，不久通过了《人权宣言》，宣扬“人身自由，权利平等”； 1791年，法国大革命引起普鲁士和奥地利不安，两国联手攻打法国； 1793年，处死路易十六； 1794年，热月党处死罗伯斯庇尔； 1799年，拿破仑雾月政变； 1815年，拿破仑滑铁卢战败，路易十八复辟波旁王朝； 1830年，七月革命推翻查理十世，建立七月王朝，法国大革命结束。 法国大革命彻底推翻了封建专制，但其过程之惨烈也令后世对革命产生了后怕。国内各种政变，欧洲各国为了扼杀这次革命的五次反法战争都为这次披上一层血色的纱布。 22）海地革命（1790年~1804年）：海地的黑奴以及混血人种反对法国、西班牙殖民统治的革命。值得一提的是，海地革命的成功也粉碎了拿破仑称霸北美的梦想。 23）拉丁美洲革命（1810年~1826年）：拉丁美洲为了脱离西班牙和葡萄牙而掀起的革命。 24）工业革命（18世纪60年代~19世纪40年代）：大多数人认为工业革命起源于英格兰，视频中认为工业革命起源于英国，很大程度上是因为 1、煤矿开采多，导致比较廉价； 2、当时英国的工资全球最高； 25）第一次世界大战（1914年~1918年）：随着帝国主义之间经济发展的不平衡，矛盾也变得越来越尖锐。1914年奥匈帝国皇储斐迪南大公被暗杀，史称萨拉热窝事件。以这一事件为借口，奥匈帝国对塞尔维亚宣战，紧接着沙俄、德、法、英相继各自组队。1918年，《凡尔赛条约》签订，一战结束。 一战后期，俄国爆发二月革命，推翻了罗曼诺夫王朝，随后又爆发十月革命，最终列宁领导的苏维埃政权控制了局面。 一战之后，因为欧洲列强都遭受了巨大打击，美国开始展露头脚。]]></content>
      <tags>
        <tag>Reading</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[其他喜剧形式]]></title>
    <url>%2F2019%2F01%2F26%2F%E5%85%B6%E4%BB%96%E5%96%9C%E5%89%A7%E5%BD%A2%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[Monologue推荐几个写的好的博主： 牙签的喜剧课堂-公众号 七点半的独角戏-微博 单立人喜剧公众号的一周墙报 Monologue特点：由两句话构成，前半句是真实新闻，后半句是戏谑调侃。 先从新闻中归纳出重要部分作为铺垫，然后根据你自己的观点（可以和原文的观点相反，比如原文是好消息，你就想出坏消息。但前提是要和铺垫逻辑通顺。） 一些思考方向1、可以往那种“震惊体”方面去思考 2、可以把不同的新闻相结合，例如： 把翟天临博士论文造假和弹幕审核两个新闻写在一起，简直是不(diao)要(de)命(yi)了(b).]]></content>
      <tags>
        <tag>Open-mic</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[手把手教你脱口秀2]]></title>
    <url>%2F2019%2F01%2F26%2F%E6%89%8B%E6%8A%8A%E6%89%8B%E6%95%99%E4%BD%A0%E8%84%B1%E5%8F%A3%E7%A7%802%2F</url>
    <content type="text"><![CDATA[第一篇文章中提到了如何写一个段子，这篇文章讲如何写一个段落和如何把多个段子组成一个脱口秀演讲稿。 从搞笑到爆笑1、写新段子的时候快速写完，之后再回去打磨。 2、不要让观众思考，而是让他们直接笑出来。 3、每个笑点都有一个关键词、短语或动作来揭示再解读，称为”底“。这个底尽量要放在段子最后，并且和其他信息隔开来。 4、三段式结构。 5、尽量选择常识性的话题，容易引起共鸣。 6、尽量代入角色。 7、少用双关梗 8、角色具体化，比如：“两个人走进酒吧”可以改为”我和我的朋友李小明走进酒吧“。 9、素材本地化。 10、联系热点事件 11、使用语法错误的语言。 12、自创词汇。 13、可以适当借助道具。 14、（重点）给段子加连续笑点 （1）、使用原有的目标假设，使用不同的再解读。 （2）、使用铺垫中不同的连接点来创作不同的笑点。 （3）、把上一个笑点作为铺垫，去作出下一个笑点。 如何把零散的笑话组合成脱口秀段落未完待续]]></content>
      <tags>
        <tag>Reading</tag>
        <tag>Open-mic</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hexo+next修改进度条]]></title>
    <url>%2F2019%2F01%2F19%2Fhexo%2Bnext%E4%BF%AE%E6%94%B9%E8%BF%9B%E5%BA%A6%E6%9D%A1%2F</url>
    <content type="text"><![CDATA[版本：hexo3.8.0 和 next6.0.0 启用进度条找到 blog/themes/next/_config.yml 修改 12pace: true #false改为true，启用进度条pace_theme: pace-theme-center-circle #自定义进度条样式，复制上面Themes list中任意一条 更改进度条颜色找到 blog/themes/next/layout/_partials/head.swig 找到下面代码 12345678910111213141516171819202122&#123;% if theme.pace %&#125; &#123;% set pace_css_uri = url_for(theme.vendors._internal + &apos;/pace/&apos;+ theme.pace_theme +&apos;.min.css?v=1.0.2&apos;) %&#125; &#123;% set pace_js_uri = url_for(theme.vendors._internal + &apos;/pace/pace.min.js?v=1.0.2&apos;) %&#125; &#123;% if theme.vendors.pace %&#125; &#123;% set pace_js_uri = theme.vendors.pace %&#125; &#123;% endif %&#125; &#123;% if theme.vendors.pace_css %&#125; &#123;% set pace_css_uri = theme.vendors.pace_css %&#125; &#123;% endif %&#125; &lt;script src=&quot;&#123;&#123; pace_js_uri &#125;&#125;&quot;&gt;&lt;/script&gt; &lt;link href=&quot;&#123;&#123; pace_css_uri &#125;&#125;&quot; rel=&quot;stylesheet&quot;&gt; ---------------添加以下代码修改进度条颜色----------------------------&lt;style&gt; .pace .pace-progress &#123; background: #181f25; /*进度条颜色*/ &#125;&lt;/style&gt;------------------------------------------------------&#123;% endif %&#125;]]></content>
      <tags>
        <tag>hexo</tag>
        <tag>next</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[手把手教你脱口秀1]]></title>
    <url>%2F2019%2F01%2F19%2F%E6%89%8B%E6%8A%8A%E6%89%8B%E6%95%99%E4%BD%A0%E8%84%B1%E5%8F%A3%E7%A7%801%2F</url>
    <content type="text"><![CDATA[《手把手教你脱口秀》、石老板《单口喜剧表演手册1.0》的总结和自己的思考，里面有些内容为了让我自己更好理解，略有改动。 写在前面《手册》的内容更加通俗化，内容偏向于《喜剧圣经》，新手比较难以体会到其中的真谛，可以作为进阶阅读。 《手把手》是译本，相对而言翻译腔比较重，但是内容更加丰富，后半部还有对表演节奏的讲解。可以作为新手阅读资料。 段子的结构一个段子的基本结构：铺垫+笑点。 这是体现在文字上的，对应体现在心理上就是目标假设+意外解读。 故事一：顺着逻辑当你说出你的铺垫的时候，需要能让观众在心里想象出目标假设，从而能够“写出”故事一，也就是顺着逻辑下去，我们平时看到的文章就是这样。 故事二：意外解读而要使人发笑，重要的就是用意外解读 来打破目标假设，最后你说出口也就是故事二，而笑点就是故事二中打破目标假设的内容，很多时候就是整个故事二。 由此可见，这需要你对一件事情具有两种不同的解读方式。而具有两种或以上的解读方式的事件就是连接目标假设与意外解读的点，我们称之为连接点。 举个例子 我的朋友非常仗义，每当我周转不开的时候，为了照顾我的面子，总是不等我开口，就纷纷表示他们也没钱。 铺垫：我的朋友非常仗义，每当我周转不开的时候，为了照顾我的面子，总是不等我开口。 目标假设：朋友会主动借钱给我。 故事一：就纷纷主动对我伸出援手。 连接点：照顾的我面子。 意外解读：朋友主动表示自己也没钱。 故事二：纷纷表示他们也没钱。 写出铺垫Step 1：找出话题（主题、Topic）1、你选择的话题需要在观众和你自己的经历之间找到一个共鸣。 比如：”手机“这个话题，你和观众之间都具有很多的共鸣，而”邮局”这个话题你可能有经历，但是没去过邮局的观众很可能就不知道你在说什么。 2、细化你的话题，你的话题越具体，就越可能打动观众。 例如：“交通”——&gt;”地铁”——“地铁安检” 3、话题不带有你的观点，但其中一定隐含着“错误”。例如：“地铁安检”本身不带有观点，但其中隐含着”不认真的安检”、“插队的乘客”等不应该的事情。 BTW，《手册》中的说法很有意思，有些人在和朋友交流时非常健谈，但和陌生人却不知道怎么打开话题。这就是因为你和朋友交谈时，你们之间已经有了足够的了解，但是和陌生人之间确缺乏共鸣的点，找到这个共鸣的点，就能开启第一步。 Step 2：创作一个笑点前提Q：对于我选取的话题，能否用四个不同的态度（困难的，奇怪的，可怕的，愚蠢的）来谈谈我的感受？ 假设在第一步中，我选择了一个大话题“新年”，细化以后为“新年聚餐”。现在为这个话题创作一个笑点前提，方法很容易，就是把这个话题加上负面态度。 例如： 1、和长辈一起吃饭很糟心。 2、应酬老板感觉像加班。 Step 3：创作一个铺垫前提观点和笑点前提对立的就是铺垫前提。 例如： 1、和长辈一起吃饭很快乐。 2、老板像家人一样体贴。 Step 4：选择一个铺垫前提，创作铺垫只要想出能表达铺垫前提的例子和说法就行。例如这里选择“2、老板像家人一样体贴。” 例如： 1、每到过年时候，老板会把大家聚到一起。 根据铺垫写笑点（笑话宝藏）这里按照《手把手》的顺序，先谈谈如何写笑点。分成以下几个步骤，通过提问的方式进行。 我们假设已经有了一个铺垫： 每到过年的时候，大家都会聚在一起 Step 1：选择一个铺垫，列出目标假设Q：对于这个铺垫，我会做出什么样的假设？ A：1、过年聚在一起是传统习俗 2、大家的关系很好 3、只有过年的时候大家才有空聚在一起 4、大家会聚在一起吃年夜饭 … Step 2：选择一个目标假设，找出连接点选择目标假设的时候，优先跟着你感觉最容易出笑点的方向，这个比较主观但一般也最有效。 这里以第4条假设为例,因为我觉得容易写出笑点 接着仍然以提问的方式找出连接点： Q：是什么使我产生这个目标假设的？ A：可以把假设带入铺垫，“每到过年的时候，大家都会聚在一起吃年夜饭。”——这里我作出这个假设的理由是“聚在一起做的事。” 1这里有个故事一和目标假设之间的关系我还没想通 Step 3：列出对连接点的再解读Q：除了目标假设之外，还有什么针对这个连接点的再解读？ 这里尽可能的去细化场景，一个很好的方式就是加形容词。比如：“她被吓到了。“ 可以改为 “她被一张丑陋的脸吓到了。” A：1、聚在一起加班 2、虽然聚在一起但其实在玩手机 再解读的意思要来源于铺垫：每到过年的时候，大家都聚在一起加班。（逻辑上要过的去） 这里选择第1条。（第2条，因为网络上类似的段子太多了，说出来就太弱了） 我写到这里的时候感觉这个目标假设选的并不好，当遇到这种情况的时候可以回到第一步重新选择目标假设，这里就不改了，硬着头皮写。 Step 4：选择一个再解读，完成故事2再解读本身并非一个笑点，他是故事二的中心概念，而完成故事二最好的方法就是加细节，同样也可以通过提问的方式。再解读必须符合笑点前提的态度。 Q：关于这个铺垫，有什么具体的情境可以解释我的再解读？ A：通过提问的方式给再解读添加细节以完成故事二。 Q1：为什么他们要聚在一起加班？ A1：因为老板要求。 Q2：老板会怎么要求他们去加班？ A2：打着过年团聚的旗号要求他们加班。 … 给出故事二： 过年的时候，老板说：“同事之间就像家人一样，而公司就是共同的家。”因此，为了让这个大家庭在过年的时候感受团圆的味道，他召集了所有员工加班。 Step 5： 写一个笑点来阐述故事二Q：在铺垫之余，需要对故事二如何加工，让他变成一个笑点？ A：很遗憾，关于这个问题，并没有一个确切的答案，只能提供一些大体的思路。 1、故事二是要被叙述还是表演出来？一个段子，观众听起来好笑和读出来好笑是完全不一样的。 2、笑点尽量简短，英文的笑点叫punchline，这其中的punch描绘的非常精准，就像是给了观众一拳一样，只不过这拳是打在笑穴。 3、笑点的关键是要让观众同步你的故事二，用说，用肢体语言来表达都行。 最后的段子： 我的老板对待公司同事总是像家人一样。为了让员工感受到合家团圆般的温暖，过年那天，他召集了所有员工，来公司加班。 这里因为先写的笑点后写的铺垫，所以不怎么接的上，我懒得改了，毕竟真的写段子的时候是先写铺垫后写笑点，但是每一步的规则是不变的。 下面是看过的一些我觉得写得很好的文章作为补充： 1、知乎-哈哈哈的回答 回答里把段子分为了“铺垫+误导+爆点”。 即铺垫中带有态度的部分划分为了误导。因为原文中写的比较详细，我就不画蛇添足，直接把原文放在底下。 栗子，（1）女生和女生的交往方式好奇怪，尤其是第一次见面的两个女生，一个女说：“你好萌啊！”另一个女生说：“你好可爱！”于是她俩就成为好朋友了。（2）在看我看来实在是太虚伪了，之前有个女生第一次见我，就对我说“你好可爱啊！” （3）我听后说到“你再给我说一遍！我刚刚没有听清。 这个段子是通过控制节奏和情绪才能产生效果的段子， （1）部分要非常严肃的说一件事情，就像开人大会议一样严肃。 （2）部分属于我自己观点部分，这是要将自己的情绪表达出来，主要表现出不屑和看不惯。 （3）“你再给我说一遍”这说时情绪一定要爆发出来，就像要骂人一样，一种威胁的情绪，最后“我刚刚没有听清”一定要温柔、卖个萌。（如果非表演性质的段子，这里就是笑点）]]></content>
      <tags>
        <tag>Reading</tag>
        <tag>Open-mic</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何阅读《如何阅读一本书》4：主题阅读]]></title>
    <url>%2F2019%2F01%2F14%2F%E5%A6%82%E4%BD%95%E9%98%85%E8%AF%BB%E3%80%8A%E5%A6%82%E4%BD%95%E9%98%85%E8%AF%BB%E4%B8%80%E6%9C%AC%E4%B9%A6%E3%80%8B4%2F</url>
    <content type="text"><![CDATA[第四篇 阅读的最终目标第二十章 主题阅读主题阅读可以分为两个阶段 阶段一：准备阶段 1）针对你想研究的主题设计出参考书目清单。 2）检视阅读这份清单，确定哪些书是你需要的，并且明确你主题的概念。 阶段二：阅读所有第一阶段的书籍 1）浏览最终选择好的参考书目，检视阅读确定哪些篇章是有用的； 2）根据主题创作出一套中立词汇，让作者与你达成共识； 3）建立一个中立的主旨，列出一连串的问题； 4）分清楚主要议题和次要议题，记住作者认为的主要议题和次要议题未必和你的一致，你需要自己去解读他的观点； 5）把问题和议题按照共通性的多寡，进行排序。 主题阅读就很有写论文的影子了，这方面对于我来说也同样薄弱，需要在今后的学习里进一步体会。 但至少有一点需要记住，在主题阅读中，面对庞杂的观点，需要你尽量保持客观，要带着作者走而不像之前的分析阅读那样，顺着作者的论点。]]></content>
      <tags>
        <tag>Reading</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何阅读《如何阅读一本书》3：不同的读物]]></title>
    <url>%2F2019%2F01%2F12%2F%E5%A6%82%E4%BD%95%E9%98%85%E8%AF%BB%E3%80%8A%E5%A6%82%E4%BD%95%E9%98%85%E8%AF%BB%E4%B8%80%E6%9C%AC%E4%B9%A6%E3%80%8B3%2F</url>
    <content type="text"><![CDATA[第三篇 阅读不同读物的方法虽然阅读不同类型书籍，规则会有所不同，但是四个问题是都要提的。 第十三章 如何阅读实用型的书实用型的书分为两类： 1）说明一中规则或者方法（例如本书）：阐述规则通常都是用命令语句而不是陈述句。 2）阐述规则形成背后的原理（例如许多经济类、政治类书籍）。 你需要判断自己是否同意作者所提出的规则，在阅读过程中可以提出以下四个问题： 1）这本书在谈些什么？ 2）找出作者的共识、主旨和论述； 3）内容是真实的吗？ 4）这本书给我什么启发？ 第十四章 如何阅读想象文学 对于小说，我们不该反对或赞成，而是喜欢和不喜欢。 想象文学不是作者在向你传递某个知识，或者说，最主要的是作者在拉你进入他所创造的世界。 1）你必须将想象文学作品分类； 2）你要能准确的说出整本书的剧情（或大意），以及明白作者想要传递的经验； 3）要能发现整本书的各个部分是如何进行架构的，即说出开头，中间，高潮，结尾； 4）要能熟悉人物和他们的性格，可以试图想象自己在这个世界中，和他们做朋友。 第十五章 阅读故事、戏剧与诗阅读过程中带着以下几个问题，并寻找他们的回答。 1）这本书的内容是在谈些什么？答：也就是说出关于一个故事、戏剧或诗的大意。 2）内容的细节是什么？如何表现出来的？答： 用自己的话复述发生在角色身上的关键事件。 3）这本书说的是真实的吗？答：这本书是否能让你欣赏，并说出你的理由。 不同类书籍阅读时各自的重点 阅读故事书在尽量短的时间内阅读完。 阅读史诗推荐《伊利亚特》、《奥德赛》、《埃涅阿斯纪》、《神曲》、《圣经》 阅读戏剧假装看到演出的实景。关于悲剧，其精髓在于故事里的英雄缺乏时间去挽回悲剧，或是他们本能做到某事却没做到。 阅读抒情诗任何人在任何处于孤独又敏感的时间里，都有可能创作出诗句。 1）不论你是否读懂，都要一口气读完全诗。去感受诗人的情感； 2）大声重读一遍。 抒情诗的问题往往是 修辞的问题。大部分好的抒情诗中，都存在冲突，比如理想和现实。 你可以在读诗过程中思考几个问题： 为什么有些词或句子特别吸引你？是节奏的原因还是押韵的原因？ 如果好几段谈的是同样的概念，那么彼此之间到底有什么关联？ 伟大的抒情诗需要再三玩味。 第十六章 如何阅读历史书阅读历史类书籍尤其不要只局限一本书，为了追求历史真相往往需要多角度来观察。可以提供一下两个要点 1）对你感兴趣的事件或时期，尽可能阅读一种以上的历史书； 2）要理解发生这件事的原因。 3）我们要知道作者是用什么方法讲故事的？时间，地区，人物？ 如何阅读自传自传具有启发性，但是对于其里面的内容（无论是自己写的还是授权于他人所写），很多时候要抱有怀疑态度。 如何阅读当前事件尤其指新闻，可以从作者的角度去思考 1）这个作者想要证明什么？ 2）他想说服谁？ 3）他具有的特殊知识是什么? 4）他使用的特殊语言是什么？ 总而言之，一定要弄清楚作者写着篇文章的动机，搞清他们的利益考虑。 如何阅读文摘因为文摘是浓缩，所以需要格外用心去阅读，必要时去翻翻原著。 第十七章 如何阅读科学与数学这里的方法多用于阅读科普类书籍，不适用于非常专业的论文等。阅读这些书只是为了了解科学而非精通某一门学科。 在阅读过程中可以时刻记住，科学家所研究的这个理论究竟是为了解决什么问题。 科学研究基本上是归纳法。少部分是用演绎法。 第十八章 如何阅读哲学书哲学家所提出的问题本身不难理解，但是要给出清晰的回答确实非常难的事情。 哲学家所探讨的问题不像科学家那样可以通过实验的方式去研究，很多时候只能通过依赖于某个他所认定的原则再展开进一步的思考。所以如果你想要阅读哲学类书籍，应该先抓住作者的原则。 第十九章 如何阅读社会科学社会科学是偏向于追求人类社会的知识。在这一领域内，具有极强的时效性，且极难找到权威类书籍。 第三篇章我总结的非常不好，其中一个很重要的原因就是，很多类别的书籍，我都只阅读过很少的一部分，对于里面提到的方法不能完全理解。 同样，我认为在今后的阅读中如果遇上了阅读障碍，再倒回来看这一部分是比较好的方式。]]></content>
      <tags>
        <tag>Reading</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何阅读《如何阅读一本书》2：分析阅读]]></title>
    <url>%2F2019%2F01%2F11%2F%E5%A6%82%E4%BD%95%E9%98%85%E8%AF%BB%E3%80%8A%E5%A6%82%E4%BD%95%E9%98%85%E8%AF%BB%E4%B8%80%E6%9C%AC%E4%B9%A6%E3%80%8B2%2F</url>
    <content type="text"><![CDATA[第二篇 阅读的第三个层次：分析阅读第六章 一本书的分类分析阅读规则一：你一定要知道自己在读的是哪一类书，这个规则尤其适用于论说性（传达知识）的书籍。 ps：有些书名有助于分类。 基本分类原则：实用性作品vs理论性作品实用性作品：告诉你应该怎么做，指南类书籍。 理论性作品：只是试图对一个事情下定义，表示这个事情是客观事实，但不会教你如何去做。 下面谈这两类作品内部的具体分类： 理论性作品的分类：一般分为历史、哲学和科学，其中历史与其他二者间的区别是比较显而易见的，往往通过书名就可以分辨，而哲学和科学存在着某些的共通性。但二者也并非不可区分，一般来说，哲学作品往往会谈及到日常的生活经验，而科学作品则往往超出你正常生活经验所能接触的范围。 第七章 一本书的架构——整本书分析阅读规则二：用一个单一的句子或是几句话来概述整本书的内容。（重点摘要） 阅读书名和前言会给你很大的帮助。 分析阅读规则三：将书中重要篇章列举出来，说明它们并按照顺序组成一个整体的框架。（框架说明） 不断细分整本书的内容，例如： 1）作者将全书分为五个部分，第一部分谈的是什么，第二部分谈的是什么，第三部分谈的是别的是，第四部分则是另外的观点，第五部分又是另一些事。 2）第一个主要的部分又分成三个段落，第一段落为X，第二段落为Y，第三段落为Z。 3）在第一部分的第一阶段，作者有四个重点，第一个而重点是A，第二个重点是B，第三个重点是C，第四个重点是D。 这种划分理论上可以划到非常详细，但实际阅读时没有很大必要太过详细，我个人认为到第三阶段就非常足够了。 很多情况下，只有阅读好了规则三，才能更好的总结出规则二。 分析阅读规则四：找出作者要问的问题，或者是作者想要解决的问题 如果你能知道每个人都会问的一些问题，你就懂得如何找出作者的问题。这个可以列出简短的公式：某件事存在吗？是什么样的事？发生的原因是什么？或是在什么样的情况下存在？或为什么会有这件事的存在？这件事的目的是什么？造成的影响是什么？特性及特征是、什么？与其他类似事件，或不相同事件的关联是什么？这件事是如何进行的？以上这些都是理论性的问题。有哪些结果可以选择？应该采取什么样的手段才能获得某种结果？要达到某个目的，应该采取哪些行动？以什么顺序？在这些条件下，什么事是对的，或怎样才会更好，而不是更糟？在什么样的条件下，这样做会比那样做好一些？以上这些都是实用的问题。 ​ ——《如何阅读一本书》 上述的四个规则都是在你阅读过程中完成的，当然，有效的检视阅读能提供非常棒的帮助。 为了方便叙述，我们把上诉内容称为分析阅读第一阶段（事实上这里的阶段并不具有时间顺序概念，无论哪个阶段，都是在阅读过程中同时发生的），这一阶段的目标是为了告诉你一本书的内容关于什么，而下一阶段则是要深入到文章具体内容中。 第八章 与作者找出共通的词义——词规则五：Step1：找出关键单词；Step2：透过它们与作者达成共识（即找到作者在使用它们时表达的语义） 找出关键单词一般来说，一段话里，你读不懂的词语就是关键单词，朝着作者想要表达的语义弄懂他们。 弄不懂的单字有两种： 1、专业单词：你在生活中几乎没见过这样的单词，比如《概率论》中的二项分布，对于这类词，可能需要你做主题阅读才能明白。 2、非专业单词：你在生活中经常见到，你也理解这个单词的某个意思，但在这个语境中，作者想要表达的意思可能和你平时理解的词义不相同。比如《国富论》中大量提到的：资本、利润、价格、价值等，就需要你进一步去理解。 这种很类似英文阅读中的一词多义，事实上中文也是，只不过我们因为已经习惯了表达而忽略这一现象。（比如双关梗）所以在要结合具体语境，和作者对这一单字的词义达成共识。 透过它们和作者达成共识（私以为即便是本书对这一部分内容的讲解也是不全面的，这涉及到语法学逻辑学的内容） 当你圈出关键字后，这些词会具有以下几种可能： 1）作者在每个用到关键字的地方都只有单一含义； 2）关键字在书中不同地方具有不同含义。 可以列出一张关键字清单，再在旁边一栏列出这写字的常用意义和文中的意义，以及对比书中不同地方的意义。 第九章 判断作者的主旨（提案）——句子书里的提案是作者在表达他对某件事的判断。 规则六：Step1：将一本书最重要的句子圈出来。Step2：找出其中的主旨。 找出关键句：从作者的观点来看，最重要的句子就是在整个论述中，阐述作者判断的部分。 要为了自己感到困惑的句子而暂停，而不只是感到有趣的句子。 找出主旨：有点像英文翻译中所要达到的“达”和“雅”的水平，需要对文法学有所了解。 这里有两个不错的方法来检测你是否理解了句子的主旨： 1）用自己的话来说； 2）举出一个例子，符合主旨所形容的经验。 不要犯 口语主义，即只会复述原文中你一眼看到就觉得很有道理的文字。要把它们转化为自己的语言。 规则七：从相关句子的关联中，设法架构出一本书的基本论述。 找出论述：就是处理一堆句子的组合。我们定义“论述”是作为一个逻辑单位，它可能只由一个句子就能说明白，也可能是一个段落，也可能是分散在不同地方的句子。 具体做法： 1）找出这些句子并按逻辑顺序做上记号，记住所有的论述都包含了一些声明，其中有些是说服你接受这个论述的理由。如果你先发现结论，就去看看理由是什么；如果你先看到理由，就去找找看这些理由能得出什么结论； 2）区别不同的论述：一种是以一个或多个例子来证明某种共同的概念，另一种是用递推出一连串的定理； 3）分辨出作者认为哪些是假设，哪些能证实以及哪些是不需要证明的。 找出解答找到作者想要解答的问题，看看作者是否引出了新的问题，作为读者你又有什么问题需要解决。 规则八：找出这些问题的解答 以上是分析阅读的第二阶段，即回答 这本书的详细内容是什么？如何叙述的？，之前我说过，第一和第二阶段是同时进行的，但是下面说的第三阶段，就是要在第一、第二阶段结束以后再进行的。 第十章 公正地评断一本书在完成分析阅读的前两个阶段之后，要对这本书进行评价。 规则九：在你做出判断之前，一定要能充分理解作者想要传达的事。 规则十：理性的表达自己的观点，而不要无理的争辩。 规则十一：对客观知识（而非个人观点）具有不同见解，你就应该提出自己的意见及理论基础。 第十一章 赞同或反对作者可以从四个方面对作者提出质疑： 1）作者知识不足，就是他缺少了与他想要解决的问题相关的知识； 2）作者的知识有错误； 3）作者的推论不符合逻辑； 4）作者的分析不完整，这个问题可能受限于作者所处的时代。 第十二章 辅助阅读作者在此处总结了几类辅助阅读的方法，因为成书时间较早，许多方法如今并不适用。就我个人而言，我觉得当今时代下，最好的辅助阅读工具就是互联网了。]]></content>
      <tags>
        <tag>Reading</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何阅读《如何阅读一本书》1：基础阅读与检视阅读]]></title>
    <url>%2F2019%2F01%2F10%2F%E5%A6%82%E4%BD%95%E9%98%85%E8%AF%BB%E3%80%8A%E5%A6%82%E4%BD%95%E9%98%85%E8%AF%BB%E4%B8%80%E6%9C%AC%E4%B9%A6%E3%80%8B1%2F</url>
    <content type="text"><![CDATA[序言之前这篇书评会加上我自己的理解，如果有谁看到了这篇书评，建议自己去阅读原著。如果你指望只看这篇文章就能明白如何阅读一本书，那你也太懒了叭。 序言这个时代里，获取资讯变的十分便捷和容易，但同时又带来了问题。换句话说，以前我们是没得选，如今却是选择太多而不知道怎么选。阅读也是一样。 最好的方式是针对不同书籍，采用不同的阅读速度 第一篇 阅读的层次阅读不是一个“轻松”的过程，如果你阅读一个东西非常容易，那么这次阅读所带给你的收获往往就越少。如同网络上那些精心包装过的言论，都是为了降低你的“思考”，让你很快就接受作者的观点，而这个过程通常相当自然以至于你可能都没察觉到。 阅读态度：主动的阅读：阅读目标：1、获得资讯：作者和读者间的理解程度一致，读者无需思考就能明白作者说了什么。 2、理解：作者的理解程度显然高出读者一个层次，读者必须通过思考才能了解作者说了什么。 阅读的艺术在阅读过程中，需要有敏锐的观察、灵敏可靠的记忆、想象的空间，再者就是训练有素的分析、省思能力。 第二章 阅读的层次（阅读时采用自己对自己“Q&amp;A”的方法我觉得可以） 阅读的层次是渐进的，每一层次都包含它的前一层次 第一层次：基础阅读，就是识字。“这个句子在说什么？” 第二层次：检视阅读，强调在一定时间内，抓住一本书的重点。“这本书的架构是什么样的”或者至少要能回答“这是哪一类书——小说、历史还是科学论文？” 如果只是为了获得资讯或消遣，只需到第二层次即可。 第三层次：分析阅读，完整的阅读，在阅读的过程中追寻理解。 第四层次：主题阅读，这一层次阅读者需要阅读很多书，并列举这些书之间的相关，提出一个所有的书都谈到的主题，甚至架构出可能在哪一本书里都没提过的主题分析。 第三章 阅读的第一个层次：基础阅读能看到这篇文章的人都具备了基础阅读的能力，一般来说大学毕业生就需要具备主题阅读的能力，然而基本上我国还是要到研究生阶段的人才具有。 第四章 阅读的第二个层次：检视阅读不具备基础阅读能力的人无法进行检视阅读，很简单的例子就是读英文文章时，如果看不懂单词，怎么理解文章大意？ 检视阅读一共有两种（其实是一题两面的事） 检视阅读一：有系统的略读或粗读1-1、需要使用检视阅读的情况： 1）当你不知道自己是否需要读这本书，或者是否需要做分析阅读 2）当你时间有限，无法阅读完整本书 1-2、如何进行检视阅读： 1）通过看书名页，序，副标题等了解书的主题，在脑海中把书归为某个特定的类型，方便主题阅读； 2）研究目录，了解本书的框架； 3）如果有索引页可以阅读一下； 4）不妨读一读出版者的介绍。 如果以上四步完成之后你扔有兴趣继续阅读，那么进入到下一步 5）从你对一本书的目录模糊的印象中，挑几个看起来和主题有关的篇章看。 6）把书随便翻几页读，尤其是开头几页和结尾几页，阅读时留意主题脉络。 以上过程一般不超过一小时，但是在这个过程中务必保持精神集中。 检视阅读二：粗浅的阅读阅读整本书，只注意你能理解的部分，先略过你不懂的地方，这些不懂的地方等到第二遍阅读时再了解。 我的意见：不能完全认同这个观点.就我个人经验来谈，有些书籍是可以参考这个方法，但对于作者的论点一定要完全理解，而对于论据之类的非重点，才能略过。而对于某些书籍（比如 高等数学）一定要了解每个地方，否则读的越多，只会不懂的越多，到时候你就略过整本书好了….. 增加阅读速度：很多内容不需要“逗留”或者“退回”，可以尝试眼睛跟着手的移动来阅读以增加阅读速度 ps：检视阅读只能做到了解一本书，如果你需要理解一本书，分析阅读是基础。 第五章 如何做一个自我要求的读者主动阅读的基础上：阅读过程中提出的四个基本问题只要超越了基础阅读的层次，阅读的艺术就是 要以适当的顺序提出适当的问题。 1）整体来说，这本书到底在谈些什么？你一定要想办法找出这本书的主题，做这如何一次发展这个主题，如何逐步从核心驻题分解出从属的关键议题来。 2）作者细部说了什么，怎么说的？你一定要想办法找出主要的想法、声明与论点。 在完成这两个问题之后，再继续下面的工作。检视阅读对前两个问题更有帮助。 3）这本书说的有道理吗？是全部有道理，还是部分有道理？ 4）这本书跟你有什么关系？如果这本书给了你一些资讯，你一定要问问这些资讯有什么意义。为什么这位作者会认为知道这件事很重要？你真的有必要去了解吗？如果这本书不只提供了资讯，还启发了你，就更有必要找出其他相关的、更深的含义或建议，以获得更多的其实。（最后的问题最重要） 做笔记的重要性1-1、做笔记的工具箱： 1）画底线——重点句； 2）画双下划线——重点句中的重点； 3）星号——建议只在最重要的地方使用，全文不要使用太多； 4）编序号； 5）在空白处记录其他的页码——强调作者在其他页码处也表达过相同观点； 6）在空白处写自己的思考； 7）阅读完整本书后，在书前的空白页列出全书的整体架构。同时在尾页写下你的个人索引。 1-2、做笔记的方法： 1）结构笔记：检视阅读 过程中回答以下几个问题：第一，这是什么样的一本书？第二，整本书在谈的是什么？第三，作者是借着怎么样的整体架构，来发展他的观点或陈述他对这个主题的理解？ 2）概念笔记：分析阅读过程中回答对这本书的立论的判断，是否有道理，以及本书的意义 3）辩证笔记：主题阅读过程中对同一个概念或观点，不同的作者之间是否存在差异或是不同的表达方式？]]></content>
      <categories>
        <category>阅读</category>
      </categories>
      <tags>
        <tag>Reading</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[github+hexo搭建个人博客]]></title>
    <url>%2F2019%2F01%2F08%2Fgithub%2Bhexo%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%2F</url>
    <content type="text"><![CDATA[本着“有好用的轮子就不要再造”这一基本原则，不再详细写出具体步骤（就是懒）。本文收录建立博客过程中参考的教程以及我所遇到的问题，如果你能看到这篇文章，就是看到了根据参考教程搭建的成果。 Step1：搭建博客1-1、从零开始到通过自定义域名访问网站：https://zhuanlan.zhihu.com/p/26625249 （这篇文章讲的挺详细的，但是会造成一个安全证书问题，在后面会给出解决方案） 1-2、部分常见问题解决方案： 1）Deploy后，页面长时间404 2）https://zhuanlan.zhihu.com/p/31427992 （这里的问题我基本没遇到过，按照1-1中的教程至少到“初识markdown语法”之前，除了前面说的访问时的安全证书，都是我自己的语法错误或是路径放错） ps：特别注意.yml文件的格式，冒号之后一定要有个空格 1-3、安全证书问题：Github支持自定义域名HTTPS Step2：对博客的配置（长期更新）2-1、我选用的是next样式：https://www.jianshu.com/p/9f0e90cc32c2 2-2、侧边栏增加知乎，豆瓣图标：https://blog.dongleizhang.com/posts/89dad1c1/ （Font Awesome中应该是包含知乎和豆瓣的，但是确实不会直接显示，没深究，按此教程加入成功） 2-3、添加标题搞怪：https://asdfv1929.github.io/2018/01/25/crash-cheat/ 2-4、集成gitalk评论系统：hexo+gitalk评论系统配置 2-5、主题透明： 1）https://zhuanlan.zhihu.com/p/30836436 2）https://ocd.ooo/2018/09/26/2018-9-26-NexT-theme-Page-transparency-color/ 2-6、next主题配置与打赏： 1）https://shirley5li.me/2017/08/10/blog-donate/ 2）自定义打赏按钮样式 2-7、百度分享功能：https://asdfv1929.github.io/2018/05/25/baidu-share/ 2-8、网页在线聊天功能：https://asdfv1929.github.io/2018/01/21/daovoice/ 2-9、图床：（用的微博图床，然后通过极简图床上传）http://jiantuku.com/#/ 七牛云绑定自定义域名需要备案，没办法。]]></content>
      <tags>
        <tag>hexo</tag>
        <tag>next</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[github pages绑定自定义域名并实现https访问]]></title>
    <url>%2F2019%2F01%2F07%2Fpages%E7%BB%91%E5%AE%9A%E8%87%AA%E5%AE%9A%E4%B9%89%E5%9F%9F%E5%90%8D%E5%B9%B6%E5%AE%9E%E7%8E%B0https%E8%AE%BF%E9%97%AE%2F</url>
    <content type="text"><![CDATA[问题： 之前是通过这篇文章绑定的从阿里云购买的自定义域名，但是绑定之后用chrome访问时总会出现证书的问题。 初步解决方案： 1、使用Cloudfare：https://blog.csdn.net/Judikator/article/details/79368172 2、Github已经为自定义域名提供HTTPS支持：启用方法 最终选用第二种解决方案：注意最初域名A记录指向的IP地址是 192.30.252.153和 192.30.252.154一定要改成几个新地址，因为原来指向的IP证书是颁发给username.github.io而不是自定义域名的]]></content>
      <tags>
        <tag>hexo</tag>
        <tag>next</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hexo+gitalk评论系统配置]]></title>
    <url>%2F2019%2F01%2F06%2Fhexo-gitalk%E8%AF%84%E8%AE%BA%E7%B3%BB%E7%BB%9F%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[主要参考以下几篇文章 Hexo NexT主题中集成gitalk评论系统：可以当教程用，但存在一些问题。 Hexo next 配置 Gitalk 总结：解决了上面那个教程存在的问题。 官方：不是配置文档但因为是官方的所以优先参考。 我参考了上面三个配置方法，整合了一下 step1：点击注册OAuth填写的内容参考参考文章2，这里放他的截图 ps：我在第二栏Homepage URL填的也是自定义域名。 注册成功后不要关闭跳转的页面，Client ID和 Client Secret 等等还会用到 Step2：配置文件这里同时参考了3个文章中的内容，我仅仅重新整合了一下，建议新手直接按步骤来。需要进一步了解的话请点击上面的三个链接。再次感谢上面的几位大佬。 gitalk.swig新建 /layout/_third-party/comments/gitalk.swig文件，添加以下内容 123456789101112131415161718192021&#123;% if not (theme.duoshuo and theme.duoshuo.shortname) and not theme.duoshuo_shortname %&#125; &#123;% if theme.gitalk.enable %&#125; &#123;% if page.comments %&#125; &lt;script src=&quot;https://unpkg.com/gitalk/dist/gitalk.min.js&quot;&gt;&lt;/script&gt; &lt;script src=&quot;https://cdn.bootcss.com/blueimp-md5/2.10.0/js/md5.js&quot;&gt;&lt;/script&gt; &lt;script type=&quot;text/javascript&quot;&gt; const gitalk = new Gitalk(&#123; clientID: &apos;&#123;&#123;theme.gitalk.ClientID&#125;&#125;&apos;, clientSecret: &apos;&#123;&#123;theme.gitalk.ClientSecret&#125;&#125;&apos;, repo: &apos;&#123;&#123;theme.gitalk.repo&#125;&#125;&apos;, owner: &apos;&#123;&#123;theme.gitalk.owner&#125;&#125;&apos;, admin: &apos;&#123;&#123;theme.gitalk.admin&#125;&#125;&apos;.split(&apos;,&apos;), pagerDirection: &apos;&#123;&#123;theme.gitalk.pagerDirection&#125;&#125;&apos;, id: md5(window.location.pathname), distractionFreeMode: false &#125;) gitalk.render(&apos;gitalk-container&apos;) &lt;/script&gt; &#123;% endif %&#125; &#123;% endif %&#125;&#123;% endif %&#125; ps: 如果这里按链接1中配置会导致 Error：Validation Failde。 错误原因：https://github.com/gitalk/gitalk/issues/102 解决方法：改成使用上面代码块中的配置方法。（详见 参考文章2） comments.swig修改/layout/_partials/comments.swig，添加以下内容（与其他的 elseif处在同一级即可）。(参考文章3) 123&#123;% elseif theme.gitalk.enable %&#125; &lt;div id=&quot;gitalk-container&quot;&gt;&lt;/div&gt; &lt;link rel=&quot;stylesheet&quot; href=&quot;https://unpkg.com/gitalk/dist/gitalk.css&quot;&gt; index.swig修改 layout/_third-party/comments/index.swig，在最后一行添加内容。(参考文章1) 1&#123;% include &apos;gitalk.swig&apos; %&#125; gitalk.styl新建 /source/css/_common/components/third-party/gitalk.styl，添加以下内容：(参考文章1) 1234.gt-header a, .gt-comments a, .gt-popup a border-bottom: none;.gt-container .gt-popup .gt-action.is--active:before top: 0.7em; third-party.styl修改/source/css/_common/components/third-party/third-party.styl，在最后一行上添加内容，引入样式。（参考文章1） 1@import &quot;gitalk&quot;; _config.yml在主题配置文件next/_config.yml中添加如下内容：（参考文章2、3） 12345678gitalk: enable: true repo: # 仓库名称，例：asdfv1929.github.io（这里注意，不是仓库地址，所以不需要加上前面的https或github.com/之类的） ClientID: # 之前注册的 OAuth 界面的提供的 ID ClientSecret: Client Secret # 同上 owner: # github 帐号 admin: # 同上，但可以添加额外的管理员用户，用逗号隔开 pagerDirection: first 填写你自己的内容（这里就需要填写之前 Client ID和 Client Secret） 其余问题1、Q：未找到相关的Issues进行评论，请联系xxx初始化创建 ​ A：登录github之后刷新页面即可 2、Q：点击评论后不变色 ​ A：1）把gitalk.swig中的 distractionFreeMode: false改为 1distractionFreeMode: &apos;&#123;&#123; theme.gitalk.distractionFreeMode &#125;&#125;&apos; ​ 2）再往主题配置文件 _config.yml中添加 distractionFreeMode: true 3、Q：点击登录github以后404 ​ A：参考Hexo next 配置 Gitalk 总结]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>hexo</tag>
        <tag>next</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《火花》观后感]]></title>
    <url>%2F2019%2F01%2F05%2F%E3%80%8A%E7%81%AB%E8%8A%B1%E3%80%8B%E8%A7%82%E5%90%8E%E6%84%9F%2F</url>
    <content type="text"><![CDATA[一部节奏缓慢，主角不惊艳，剧情不惊心动魄的剧集。它像极了人生，所以又格外好看。 我非专业人士，没法从拍摄、画面等专业角度去给出评价，但是剧中的景都选取的恰到好处，给人一种配合剧情却又不喧宾夺主的感觉。 所以想聊聊剧情。 主角之一的德永太步，sparks中负责“装傻”，虽然在现实生活中沉闷且缺乏社交能力，但是在组合中却负责写梗，可见还是有一定的喜剧天赋。在故事一开始的热海花火大会上因为表演时间被抢占，导致德永和搭档上台时观众都被烟火吸引了。 我们的声音小的可怜，只有想听得人才能听见。 而在德永人生这么一个倒霉时刻，神谷出现了，“为你报仇。”一句话就让德永的视线离不开他，然后神谷上台对着所有离开的观众大喊：“下地狱，下地狱…”甚至和搭档一起凶台下的观众。这股在台上股肆意洒脱的有些拧巴的劲如同背景中绽放的烟火，瞬间吸引了德永。演出结束后，德永拜了神谷为师。 这部剧讨论了一个矛盾：坚持自我和向现实妥协，怎么选择？ 这真的太难了，坚持自我也许很酷，但最后换来的可能是全世界的不理解 剧里的神谷象征着这一类坚持自我的人，很有喜剧天赋，表演方式不拘一格，甚至让观众笑的忘记了手上的虫子。但他也在台上讽刺打瞌睡的评委，或是用评委无法接受的方式来表演，因此他始终无法出名。 神谷先生并不在意他人的批评和流言蜚语，哪怕已经输了也会坚信自己没有失败，这样的固执、自信会让周围的人感到害怕。让人胆寒的对象必然会受人排斥，因此，世人嘲笑他，嘲笑他脱离市场的愚昧。 这太矛盾了，明明是为漫才而生的人却无法因为漫才而被大众所喜爱。也许追求极致的纯粹是神才能做到的事，而作为本质是社会属性的人类，在这条路上注定是看不到头的绝望，甚至还会伤害到周围的人。 他的才华与魅力是毋庸置疑的，但他超强的自信和咄咄逼人的话语，有时让我窒息。 比如真树小姐，比如神谷的搭档大林，他们都相信神谷，但神谷却无法回应他们。 后来就连神谷自己都迷失了，所以会模仿德永把头发染白，穿黑色衣服，希望能够被大众所接受，幸好他还有德永，让他重新做回了自己。 剧里有许多角色都代表这向现实妥协的那一类人（毕竟生活里这类人也是大多数），比如装傻的鹿谷，或是日向企划里的其他人，他们并没有做过什么大奸大恶的事，无论剧里还是书里，对他们着墨不多，不细说。唯一值得一提的是，现实生活中，比起《火花》原作者又吉直树，他的搭档绫部祐二更加重视社会关系，却在《火花》拿下芥川奖大热之后，不得不被又吉直树的光芒掩盖。 所以当你向现实妥协之后，若是仍然不能被大众所认识，又该怎么去接受。 最后来谈谈男主角德永太步，他太像一个普通人了，有一点喜剧天赋却和神谷有着近乎不可逾越的差距，心里希望能成为神谷那样不顾他人目光的漫才师，却又不敢承受他人的非议，于是尝试过向世俗的眼光让步，取得了一定的成功，却又在最紧要的关头退缩，拒绝了制片人的饭局，彻底失去了成名的机会。 我对男主的软弱是非常厌恶的（尽管自己有时候也会犯同样的错误，也许正因为如此才讨厌），既不敢像神谷一样完全不在乎他人眼光，又不敢完全的融入到世俗之中。对待感情也不敢大声说出口，只能在步美面前说“祝你幸福”，然后转身奔跑在空无一人的道路上大喊“我喜欢你”。同样如果当时没有拒绝制作人的邀请，SPARKS的节目也不至于越来越少，搭档山下也许也不需要离开。 我跟神谷先生不一样，我无法成为完完全全的另类，另一方面，也不会八面玲珑地迎逢别人。 看起来，无论如何做出选择，这部剧的主角们最终都没有如同超级英雄一般击败强敌，改变自己的命运，恰恰相反，最终神谷欠了一大笔钱，而立志成为漫才师的德永去了房地产公司成为了一名普通上班族。 我啊，是为了表演足以推翻世界常识的漫才而走上这条路的。但是，我唯一推翻的，只有“努力一定会有回报”这句话。 —德永太步 这部剧还讨论了另外一个问题：努力之后仍然无法实现理想的小人物，是否有存在的价值？ 或许这个问题还能分开来谈，首先，努力之后无法实现的理想是否是有价值？ 《火花》给出的答案是：有的。 &gt; “耗费长时间一直在做没有必要的事情很可怕吧？在仅此一次的宝贵人生中，挑战或许会完全没有结果的事情很可怕吧。排除无谓的徒劳，也就等于是在回避危险。无论是胆小、自作多情或是无药可救的笨蛋都行，总之只有敢站上充满风险的舞台，全力向倾覆常识去挑战的人，才能够成为相声师。光是能明白那点就已足够。透过这耗费漫长时光的鲁莽挑战，我认为已得到自己真正的人生。” 这部剧总共10集，每集1年，德永花了超过10年的时间，最后却告别了漫才行业，主角在事业上升时候，自己毁掉了前途，最终没能成为举世瞩目的大英雄，站在聚光灯下享受着众人的欢呼声。而是成为了千千万万个平凡的人之一。 而在结尾，SPARKS的谢幕演出中，德永用说反话的方式感动哭了所有人。我想至少这一场，他真的说出了他最想说出的那种漫才。 “你呀，信口雌黄，把观众和搭档都弄哭了。这哪儿能叫漫才呀？漫才，必须让人笑出来才对！” “这说明，一直到最后的最后，我终于完成了一个颠覆常识的漫才。” 那些小人物，是否有价值呢？ 主角说的够多了，我想从配角的角度谈谈。书里对配角的着墨几乎没有，而剧里大大加强了配角的戏份，这是我认为剧集比书要好出很多的地方，寥寥几笔，却把几个配角的性格给描绘的生动立体却又不抢戏，通过配角使得“小人物”这一形象更加丰满。 比如让德永做他托的吉他小哥，最后离开时演奏的《空に星が綺麗》；坚持修家电的老人被房东嫌弃；热爱写毛笔字的经纪公司老板；老是引用加藤鹰的同事。 总得来说，这部剧最精彩的地方就是真实，一种普普通通的真实。 它不刻意营造出某种氛围，但却又能让你能感受到剧中人物的一切。而最后的结局中，大家看似都没有往理想的方向前进，但又都过的不差。 吉他小哥有了朋友； 修家电的老人开了家电修理店； 经济公司的老板和同事们继续着他们的工作和生活； 山下回老家做了手机店员，和百合枝应该过着安稳的日子； 真树有了孩子； 步美去了伦敦深造； 神谷在德永的开导后，又想到了新的好笑的段子； 德永成为了一名房屋销售，在和神谷重逢后接着写《神谷传纪》； 街头又出现了新的无人问津的吉他小哥谈着无人问津的歌曲； 漫才新人们又满怀希望的踏入这个行业； 只要活着，就不是坏的结局。我们还在途中，今后还要继续走下去。 说点题外话 作为上过开放麦的单口喜剧爱好者，还是非常能够理解德永的纠结，也遇到过类似的问题，比如黄段子更能吸引到观众，是否要坚持不说？因为喜剧不像绘画之类的艺术，可以孤芳自赏，观众的反映几乎是评判喜剧的唯一标准，当你说出一个段子底下却毫无反映的时候，那种尴尬真的是难以言表。 这种尴尬会让你开始深深的怀疑自己，尤其是看到别人的炸场表演，像神谷那样壁立千仞的坚持真的太难了， 不过还是干了经济公司老板最后的那碗鸡汤吧]]></content>
      <categories>
        <category>电影</category>
      </categories>
      <tags>
        <tag>Reading</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[第一次读稿会和开放麦]]></title>
    <url>%2F2018%2F12%2F29%2F%E7%AC%AC%E4%B8%80%E6%AC%A1%E5%BC%80%E6%94%BE%E9%BA%A6%2F</url>
    <content type="text"><![CDATA[1、观众是个大问题，真有你说个铺垫在下面乱接梗的。2、黄段子的确更有效果，能够引发笑声。3、表演有点过于刻意了。4、段子的铺垫出梗的时间间隔不宜过久5、梗一定要短平快]]></content>
      <tags>
        <tag>Open-mic</tag>
      </tags>
  </entry>
</search>
