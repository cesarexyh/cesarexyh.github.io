<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>空博客</title>
  
  <subtitle>总不能浪费个副标题吧</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2019-05-02T06:44:46.165Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>JQK/许阳航</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>机器学习|吴恩达机器学习之PCA</title>
    <link href="http://yoursite.com/2019/05/02/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8BPCA/"/>
    <id>http://yoursite.com/2019/05/02/机器学习-吴恩达机器学习之PCA/</id>
    <published>2019-05-02T06:44:46.000Z</published>
    <updated>2019-05-02T06:44:46.165Z</updated>
    
    <content type="html"><![CDATA[<h2 id="K-means-Clustering"><a href="#K-means-Clustering" class="headerlink" title="K-means Clustering"></a>K-means Clustering</h2><p>导入模块</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> scipy.io <span class="keyword">import</span> loadmat</span><br><span class="line"><span class="keyword">from</span> skimage <span class="keyword">import</span> io</span><br></pre></td></tr></table></figure><a id="more"></a><h3 id="Implementing-K-means"><a href="#Implementing-K-means" class="headerlink" title="Implementing K-means"></a>Implementing K-means</h3><h4 id="Finding-closet-centroids"><a href="#Finding-closet-centroids" class="headerlink" title="Finding closet centroids"></a>Finding closet centroids</h4><ul><li>读入数据</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">data=loadmat(<span class="string">'ex7data2.mat'</span>)</span><br><span class="line">X=data[<span class="string">'X'</span>]</span><br></pre></td></tr></table></figure><ul><li>为每个样本找到最近的聚类中心</li></ul><p>$c^{(i)} :=j \quad$ that minimizes $\quad\left|x^{(i)}-\mu_{j}\right|^{2}$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#========================找聚类中心</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">find_closet_centroids</span><span class="params">(X,centroids)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    返回每个样本所在的cluster索引</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    idx=[]</span><br><span class="line">    max_dist=<span class="number">10000</span> <span class="comment"># 给一个距离限定</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(X)):</span><br><span class="line">        minus=X[i]-centroids <span class="comment"># minus是3x2的矩阵，每一行代表了第i个样本到一个centroids的x1,x2距离</span></span><br><span class="line">        dist=minus[:,<span class="number">0</span>]**<span class="number">2</span>+minus[:,<span class="number">1</span>]**<span class="number">2</span> <span class="comment">#求范式，即直线距离,dist是3x1的向量</span></span><br><span class="line">        <span class="keyword">if</span> dist.min()&lt;max_dist:</span><br><span class="line">            ci=np.argmin(dist) <span class="comment">#返回沿axis的最小值索引</span></span><br><span class="line">            idx.append(ci)</span><br><span class="line">    <span class="keyword">return</span> np.array(idx)</span><br></pre></td></tr></table></figure><p>可以自己初始化一组聚类中心来测试一下</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">init_centroids = np.array([[<span class="number">3</span>, <span class="number">3</span>], [<span class="number">6</span>, <span class="number">2</span>], [<span class="number">8</span>, <span class="number">5</span>]])</span><br><span class="line">idx = findClosestCentroids(X, init_centroids)</span><br><span class="line">print(idx[<span class="number">0</span>:<span class="number">3</span>])</span><br></pre></td></tr></table></figure><p>结果应该是</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[<span class="number">0</span> <span class="number">2</span> <span class="number">1</span>]</span><br></pre></td></tr></table></figure><h4 id="Computing-centroids-means"><a href="#Computing-centroids-means" class="headerlink" title="Computing  centroids means"></a>Computing  centroids means</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#=========================移动聚类中心</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_centroids</span><span class="params">(X,idx)</span>:</span></span><br><span class="line">    centroids=[]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(np.unique(idx))):</span><br><span class="line">        <span class="comment"># 布尔索引，idx==i运算返回bool value，根据值为true的下标来输出X中对应元素值</span></span><br><span class="line">        u_k=X[idx==i].mean(axis=<span class="number">0</span>)</span><br><span class="line">        </span><br><span class="line">        centroids.append(u_k)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> np.array(centroids)</span><br></pre></td></tr></table></figure><h4 id="K-means-on-example-dataset"><a href="#K-means-on-example-dataset" class="headerlink" title="K-means on example dataset"></a>K-means on example dataset</h4><ul><li>画图函数</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_data</span><span class="params">(X,centroids,idx=None)</span>:</span></span><br><span class="line"></span><br><span class="line">    colors = [<span class="string">'b'</span>,<span class="string">'g'</span>,<span class="string">'gold'</span>,<span class="string">'darkorange'</span>,<span class="string">'salmon'</span>,<span class="string">'olivedrab'</span>, </span><br><span class="line">              <span class="string">'maroon'</span>, <span class="string">'navy'</span>, <span class="string">'sienna'</span>, <span class="string">'tomato'</span>, <span class="string">'lightgray'</span>, <span class="string">'gainsboro'</span></span><br><span class="line">             <span class="string">'coral'</span>, <span class="string">'aliceblue'</span>, <span class="string">'dimgray'</span>, <span class="string">'mintcream'</span>, <span class="string">'mintcream'</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 这里centroids需要从ndarray转成list,但centroids[0]仍为ndarray.</span></span><br><span class="line">    <span class="keyword">assert</span> len(centroids[<span class="number">0</span>])&lt;=len(colors),<span class="string">'colors not enough'</span> </span><br><span class="line">    </span><br><span class="line">    subX=[]</span><br><span class="line">    <span class="keyword">if</span> idx <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(centroids[<span class="number">0</span>].shape[<span class="number">0</span>]): <span class="comment">#分成几类就循环几次</span></span><br><span class="line">            x_i=X[idx==i]</span><br><span class="line">            subX.append(x_i) <span class="comment">#把数据按cluster分成不同下标的元素，存储在subx这个list中</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        subX=[X]</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    fig=plt.figure(figsize=(<span class="number">8</span>,<span class="number">5</span>))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(subX)):</span><br><span class="line">        xx=subX[i]</span><br><span class="line">        plt.scatter(xx[:,<span class="number">0</span>],xx[:,<span class="number">1</span>],c=colors[i])    </span><br><span class="line">    plt.xlabel(<span class="string">'x'</span>)</span><br><span class="line">    plt.ylabel(<span class="string">'y'</span>)</span><br><span class="line">    plt.title(<span class="string">'Plot of X Points'</span>)</span><br><span class="line"></span><br><span class="line">    xx,yy=[],[]</span><br><span class="line">    <span class="keyword">for</span> centroid <span class="keyword">in</span> centroids: </span><br><span class="line">        xx.append(centroid[:,<span class="number">0</span>])</span><br><span class="line">        yy.append(centroid[:,<span class="number">1</span>])</span><br><span class="line">    </span><br><span class="line">    plt.plot(xx,yy,<span class="string">'rx--'</span>)</span><br><span class="line"></span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><ul><li>画出聚类中心移动过程</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#=============================聚类中心移动过程</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run_kmeans</span><span class="params">(X,centroids,max_iters)</span>:</span></span><br><span class="line"></span><br><span class="line">    centroids_all=[]</span><br><span class="line">    centroids_all.append(centroids)</span><br><span class="line">    centroid_i=centroids</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(max_iters):</span><br><span class="line">        idx=find_closet_centroids(X,centroid_i)</span><br><span class="line">        centroid_i=compute_centroids(X,idx) </span><br><span class="line">        centroids_all.append(centroid_i) <span class="comment">#每次移动后的聚类中心坐标都记录下来</span></span><br><span class="line">    <span class="keyword">return</span> idx,centroids_all</span><br></pre></td></tr></table></figure><p>结果：</p><p><img src="https://s2.ax1x.com/2019/05/02/EYOrCQ.png" alt="EYOrCQ.png"></p><h4 id="Random-initialization"><a href="#Random-initialization" class="headerlink" title="Random initialization"></a>Random initialization</h4><p>关于聚类中心的初始化，一个更好的方法是从样本集中随机选取。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#==============================随机初始化聚类中心</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">random_centroids</span><span class="params">(X,K)</span>:</span></span><br><span class="line">    m=X.shape[<span class="number">0</span>]</span><br><span class="line">    index=np.random.choice(m,K) <span class="comment">#在0~m中随机生成K个样本</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> X[index]</span><br></pre></td></tr></table></figure><p>最后再尝试一下聚类算法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">3</span>):</span><br><span class="line">        init_centroids=random_centroids(X,<span class="number">3</span>)</span><br><span class="line">        </span><br><span class="line">        idx,centroids_all=run_kmeans(X,init_centroids,<span class="number">20</span>)</span><br><span class="line">        plot_data(X,centroids_all,idx=idx) <span class="comment">#idx为每个样本所在cluster的索引</span></span><br></pre></td></tr></table></figure><p>得到最终结果：</p><p><img src="https://s2.ax1x.com/2019/05/02/EYXtG4.md.png" alt="EYXtG4.md.png"></p><h3 id="Image-compression-with-K-means"><a href="#Image-compression-with-K-means" class="headerlink" title="Image compression with K-means"></a>Image compression with K-means</h3><ul><li>导入数据</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">img=io.imread(<span class="string">'bird_small.png'</span>)</span><br><span class="line">plt.imshow(img)</span><br><span class="line">plt.show()</span><br><span class="line">print(img.shape)</span><br></pre></td></tr></table></figure><p>图像为</p><p><img src="https://s2.ax1x.com/2019/05/02/EYX5eP.png" alt="EYX5eP.png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(<span class="number">128</span>,<span class="number">128</span>,<span class="number">3</span>)</span><br></pre></td></tr></table></figure><p>可以看到图像以3维矩阵的方式存储，前面两个维度代表图像的像素点个数128x128，最后一个维度代表像素点由RGB三个通道表示。又因为一个通道占用8-bit，因此在原始的图像中一个像素点需要24-bit来储存。</p><p>在这幅图中包含了上千种种颜色，而在这个实验中，我们把颜色降为16种，也就是说，一像素点只需要4bit就足够了。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">img=img/<span class="number">255</span></span><br><span class="line">X=img.reshape(<span class="number">-1</span>,<span class="number">3</span>) <span class="comment">#转换成128x128行，3列为RGB三通道</span></span><br><span class="line">K=<span class="number">16</span> <span class="comment">#16个聚类中心，就是把所有颜色压缩为16种RGB颜色，那么每个像素值需要4bit存储即可</span></span><br><span class="line">init_centroids=random_centroids(X,K)</span><br><span class="line">idx,centroids_all=run_kmeans(X,init_centroids,<span class="number">10</span>)</span><br><span class="line">img_2=np.zeros(X.shape)</span><br><span class="line">centroids=centroids_all[<span class="number">-1</span>] <span class="comment"># 只需要记录聚类中心最后移动的位置即可</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(centroids)):</span><br><span class="line">    img_2[idx==i]=centroids[i]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">img_2=img_2.reshape((<span class="number">128</span>,<span class="number">128</span>,<span class="number">3</span>))</span><br><span class="line">fig,axes=plt.subplots(<span class="number">1</span>,<span class="number">2</span>,figsize=(<span class="number">12</span>,<span class="number">6</span>))</span><br><span class="line">axes[<span class="number">0</span>].imshow(img)</span><br><span class="line">axes[<span class="number">1</span>].imshow(img_2)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>比对一下压缩效果：</p><p><img src="https://s2.ax1x.com/2019/05/02/EYjBlj.md.png" alt="EYjBlj.md.png"></p><h2 id="Principle-Component-Analysis"><a href="#Principle-Component-Analysis" class="headerlink" title="Principle Component Analysis"></a>Principle Component Analysis</h2><ul><li>导入模块</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> scipy.io <span class="keyword">import</span> loadmat</span><br></pre></td></tr></table></figure><h3 id="Example-Dataset"><a href="#Example-Dataset" class="headerlink" title="Example Dataset"></a>Example Dataset</h3><ul><li>导入数据</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">data = loadmat(<span class="string">'ex7data1.mat'</span>)</span><br><span class="line">X = data[<span class="string">'X'</span>]</span><br><span class="line">print(X.shape)</span><br><span class="line">plt.scatter(X[:,<span class="number">0</span>], X[:,<span class="number">1</span>], facecolors=<span class="string">'none'</span>, edgecolors=<span class="string">'b'</span>)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(50,2)</span><br></pre></td></tr></table></figure><ul><li>显示图像：</li></ul><p><img src="https://s2.ax1x.com/2019/05/02/EYz5cQ.png" alt="EYz5cQ.png"></p><h3 id="Implementing-PCA"><a href="#Implementing-PCA" class="headerlink" title="Implementing PCA"></a>Implementing PCA</h3><blockquote><p>关于PCA的数学原理可以参考：<a href="https://nullblog.top/2019/05/01/%E6%B5%85%E8%B0%88SVD%E7%9A%84%E6%95%B0%E5%AD%A6%E5%8E%9F%E7%90%86%E5%8F%8A%E5%BA%94%E7%94%A8/" target="_blank" rel="noopener">浅谈SVD的数学原理及应用</a></p></blockquote><p>PCA主要分为两个计算步骤：</p><ul><li>计算数据的协方差矩阵：$\Sigma=\frac{1}{m} \sum_{i=1}^{n}\left(x^{(i)}\right)\left(x^{(i)}\right)^{T}$</li><li>用SVD函数进行奇异值分解</li></ul><p>在开始上面的步骤之前，需要先对数据进行特征缩放和归一化：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">feature_normalize</span><span class="params">(X)</span>:</span></span><br><span class="line">    means = X.mean(axis=<span class="number">0</span>)</span><br><span class="line">    stds = X.std(axis=<span class="number">0</span>, ddof=<span class="number">1</span>)</span><br><span class="line">    X_norm = (X - means) / stds</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> X_norm, means</span><br></pre></td></tr></table></figure><p>PCA：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#===================================PCA</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pca</span><span class="params">(X)</span>:</span></span><br><span class="line">    sigma = (<span class="number">1</span> / len(X)) * (X.T @ X)  <span class="comment">#求出协方差矩阵</span></span><br><span class="line">    U, S, V = np.linalg.svd(sigma)  <span class="comment">#奇异值分解</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> U, S, V</span><br></pre></td></tr></table></figure><p>可视化主成成分：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_reduce</span><span class="params">(means, U, S)</span>:</span></span><br><span class="line">    plt.plot([means[<span class="number">0</span>], means[<span class="number">0</span>] + <span class="number">1.5</span> * S[<span class="number">0</span>] * U[<span class="number">0</span>, <span class="number">0</span>]],</span><br><span class="line">             [means[<span class="number">1</span>], means[<span class="number">1</span>] + <span class="number">1.5</span> * S[<span class="number">0</span>] * U[<span class="number">0</span>, <span class="number">1</span>]],</span><br><span class="line">             c=<span class="string">'r'</span>,</span><br><span class="line">             linewidth=<span class="number">3</span>,</span><br><span class="line">             label=<span class="string">'First Principle Component'</span>)</span><br><span class="line">    plt.plot([means[<span class="number">0</span>], means[<span class="number">0</span>] + <span class="number">1.5</span> * S[<span class="number">1</span>] * U[<span class="number">1</span>, <span class="number">0</span>]],</span><br><span class="line">             [means[<span class="number">1</span>], means[<span class="number">1</span>] + <span class="number">1.5</span> * S[<span class="number">1</span>] * U[<span class="number">1</span>, <span class="number">1</span>]],</span><br><span class="line">             c=<span class="string">'g'</span>,</span><br><span class="line">             linewidth=<span class="number">3</span>,</span><br><span class="line">             label=<span class="string">'Second Principal Component'</span>)</span><br><span class="line">    plt.axis(<span class="string">'equal'</span>)</span><br><span class="line">    plt.legend()</span><br></pre></td></tr></table></figure><p>得到结果：</p><p><img src="https://s2.ax1x.com/2019/05/02/EtpMM4.md.png" alt="EtpMM4.md.png"></p><h3 id="Dimensionality-Reduction-with-PCA"><a href="#Dimensionality-Reduction-with-PCA" class="headerlink" title="Dimensionality Reduction with PCA"></a>Dimensionality Reduction with PCA</h3><h4 id="Projecting-the-data-onto-the-principal-components"><a href="#Projecting-the-data-onto-the-principal-components" class="headerlink" title="Projecting the data onto the principal components"></a>Projecting the data onto the principal components</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">返回值</span></span><br><span class="line"><span class="string">- Z：投影到主成成分后的样本点</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">project_data</span><span class="params">(X, U, K)</span>:</span></span><br><span class="line">    Z = X @ U[:, <span class="number">0</span>:K]</span><br><span class="line">    <span class="keyword">return</span> Z</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">K = <span class="number">1</span></span><br><span class="line">Z = project_data(X_norm, U, K)</span><br><span class="line">print(Z[<span class="number">0</span>])</span><br></pre></td></tr></table></figure><p>得到结果：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">array([<span class="number">1.48127391</span>])</span><br></pre></td></tr></table></figure><h4 id="Reconstructing-an-approximation-of-the-data"><a href="#Reconstructing-an-approximation-of-the-data" class="headerlink" title="Reconstructing an approximation of the data"></a>Reconstructing an approximation of the data</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#===================================重建数据</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">recover_data</span><span class="params">(Z, U, K)</span>:</span></span><br><span class="line">    X_rec = Z @ U[:, <span class="number">0</span>:K].T</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> X_rec</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># you will recover an approximation of the first example and you should see a value of</span></span><br><span class="line"><span class="comment"># about [-1.047 -1.047].</span></span><br><span class="line">X_rec = recover_data(Z, U, <span class="number">1</span>)</span><br><span class="line">X_rec[<span class="number">0</span>]</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">array([<span class="number">-1.04741883</span>, <span class="number">-1.04741883</span>])</span><br></pre></td></tr></table></figure><h4 id="Visualizing-the-projections"><a href="#Visualizing-the-projections" class="headerlink" title="Visualizing the projections"></a>Visualizing the projections</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#====================================PCA样本投影可视化</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">visualize_data</span><span class="params">(X_norm, X_rec)</span>:</span></span><br><span class="line">    fig, ax = plt.subplots(figsize=(<span class="number">8</span>, <span class="number">5</span>))</span><br><span class="line">    plt.axis(<span class="string">'equal'</span>)</span><br><span class="line">    plt.scatter(</span><br><span class="line">        X_norm[:, <span class="number">0</span>],</span><br><span class="line">        X_norm[:, <span class="number">1</span>],</span><br><span class="line">        s=<span class="number">30</span>,</span><br><span class="line">        facecolors=<span class="string">'none'</span>,</span><br><span class="line">        edgecolors=<span class="string">'blue'</span>,</span><br><span class="line">        label=<span class="string">'Original Data Points'</span>)</span><br><span class="line"></span><br><span class="line">    plt.scatter(</span><br><span class="line">        X_rec[:, <span class="number">0</span>],</span><br><span class="line">        X_rec[:, <span class="number">1</span>],</span><br><span class="line">        s=<span class="number">30</span>,</span><br><span class="line">        facecolors=<span class="string">'none'</span>,</span><br><span class="line">        edgecolors=<span class="string">'red'</span>,</span><br><span class="line">        label=<span class="string">'PCA Reduced Data Points'</span>)</span><br><span class="line"></span><br><span class="line">    plt.title(<span class="string">"Example Dataset: Reduced Dimension Points Shown"</span>,fontsize=<span class="number">14</span>)</span><br><span class="line">    plt.xlabel(<span class="string">'x1 [Feature Normalized]'</span>,fontsize=<span class="number">14</span>)</span><br><span class="line">    plt.ylabel(<span class="string">'x2 [Feature Normalized]'</span>,fontsize=<span class="number">14</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> x <span class="keyword">in</span> range(X_norm.shape[<span class="number">0</span>]):</span><br><span class="line">        plt.plot([X_norm[x,<span class="number">0</span>],X_rec[x,<span class="number">0</span>]],[X_norm[x,<span class="number">1</span>],X_rec[x,<span class="number">1</span>]],<span class="string">'k--'</span>)</span><br><span class="line">        plt.legend(loc=<span class="number">0</span>)</span><br><span class="line">        <span class="comment"># 输入第一项全是X坐标，第二项都是Y坐标</span></span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><p>结果如图：</p><p><img src="https://s2.ax1x.com/2019/05/02/Et9DcF.md.png" alt="Et9DcF.md.png"></p><h3 id="Face-Image-Dataset"><a href="#Face-Image-Dataset" class="headerlink" title="Face Image Dataset"></a>Face Image Dataset</h3><ul><li>导入数据</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">data_2=loadmat(<span class="string">'ex7faces.mat'</span>)</span><br><span class="line">X_2=data_2[<span class="string">'X'</span>]</span><br></pre></td></tr></table></figure><ul><li>数据可视化</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_face</span><span class="params">(X,row,col)</span>:</span></span><br><span class="line">    fig,ax=plt.subplots(row,col,figsize=(<span class="number">8</span>,<span class="number">8</span>))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(row):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(col):</span><br><span class="line">            ax[i][j].imshow(X[i*col+j].reshape(<span class="number">32</span>,<span class="number">32</span>).T,cmap=<span class="string">'Greys_r'</span>)</span><br><span class="line">            ax[i][j].set_xticks([])</span><br><span class="line">            ax[i][j].set_yticks([])</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><p>结果如图：</p><p><img src="https://s2.ax1x.com/2019/05/02/Et9Ije.png" alt="Et9Ije.png"></p><h4 id="PCA-on-Faces"><a href="#PCA-on-Faces" class="headerlink" title="PCA on Faces"></a>PCA on Faces</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X_2_norm,means_2=feature_normalize(X_2)</span><br><span class="line">U_2,S_2,V_2=pca(X_2_norm)</span><br></pre></td></tr></table></figure><p>显示主成成分</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plot_face(U[:,:<span class="number">36</span>].T, <span class="number">6</span>, <span class="number">6</span>)</span><br></pre></td></tr></table></figure><p>结果如下：</p><p><img src="https://s2.ax1x.com/2019/05/02/Et9O4P.png" alt="Et9O4P.png"></p><h4 id="Dimensionality-Reduction"><a href="#Dimensionality-Reduction" class="headerlink" title="Dimensionality Reduction"></a>Dimensionality Reduction</h4><p>把图片降维</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Z_2=project_data(X_2_norm,U_2,K=<span class="number">36</span>)</span><br><span class="line">X_2_rec=recover_data(Z_2,U_2,K=<span class="number">36</span>)</span><br><span class="line">plot_face(X_2_rec,<span class="number">10</span>,<span class="number">10</span>)</span><br></pre></td></tr></table></figure><p>结果：</p><p><img src="https://s2.ax1x.com/2019/05/02/Etn09H.png" alt="Etn09H.png"></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;K-means-Clustering&quot;&gt;&lt;a href=&quot;#K-means-Clustering&quot; class=&quot;headerlink&quot; title=&quot;K-means Clustering&quot;&gt;&lt;/a&gt;K-means Clustering&lt;/h2&gt;&lt;p&gt;导入模块&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; numpy &lt;span class=&quot;keyword&quot;&gt;as&lt;/span&gt; np&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; matplotlib.pyplot &lt;span class=&quot;keyword&quot;&gt;as&lt;/span&gt; plt&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;from&lt;/span&gt; scipy.io &lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; loadmat&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;from&lt;/span&gt; skimage &lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; io&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
    
      <category term="Machine Learning" scheme="http://yoursite.com/tags/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>浅谈SVD的数学原理及应用</title>
    <link href="http://yoursite.com/2019/05/01/%E6%B5%85%E8%B0%88SVD%E7%9A%84%E6%95%B0%E5%AD%A6%E5%8E%9F%E7%90%86%E5%8F%8A%E5%BA%94%E7%94%A8/"/>
    <id>http://yoursite.com/2019/05/01/浅谈SVD的数学原理及应用/</id>
    <published>2019-05-01T07:02:06.000Z</published>
    <updated>2019-05-01T11:13:56.918Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>在做PCA的实验时，遇到的SVD奇异值分解问题，这里记录自己直观的理解</p></blockquote><p>通常在<code>python</code>中我们都是直接调用<code>numpy</code>模块中的函数，那么返回的U,S,V几个值代表什么意思呢？</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">U, S, V = np.linalg.svd(X)</span><br></pre></td></tr></table></figure><p>遇事不明，手册先行：<a href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.svd.html" target="_blank" rel="noopener">官方手册</a></p><a id="more"></a><p>手册中简单说明了其含义：</p><ul><li>U：酉矩阵</li><li>S：奇异值向量（数学上是奇异值矩阵，但因为这个矩阵类似于对角阵，只有主对角线上的元素非0，程序对其进行了处理，直接提取了对角元素），且元素按降序排列。</li><li>V：酉矩阵</li></ul><p>首先给出SVD分解公式：$W=U \Sigma V^{T}$</p><p>如果你看的一脸懵逼，没关系，这只能说明你和我一样，线性代数没怎么学好。所以接下来简单解释一下其背后的数学原理，希望有帮助。</p><p>举一个具体分解的例子：</p><p>有这么一个矩阵：$W=\left[ \begin{array}{ll}{1} &amp; {1} \\ {0} &amp; {1} \\ {1} &amp; {0}\end{array}\right]$我们要如何把它SVD分解？</p><p><strong>step-1.1</strong> </p><p>计算第一个对角矩阵，$C=W^{T} W=\left[ \begin{array}{lll}{1} &amp; {0} &amp; {1} \\ {1} &amp; {1} &amp; {0}\end{array}\right] \left[ \begin{array}{ll}{1} &amp; {1} \\ {0} &amp; {1} \\ {1} &amp; {0}\end{array}\right]=\left[ \begin{array}{ll}{2} &amp; {1} \\ {1} &amp; {2}\end{array}\right]$，注意得到的是方阵。</p><p><strong>step-1.2</strong></p><p>接着对这个对角矩阵，求它的特征值$\lambda_{1}=3, \lambda_{2}=1$和特征向量$\vec{v}_{1}=\left[ \begin{array}{l}{\frac{1}{\sqrt{2}}} \\ {\frac{1}{\sqrt{2}}}\end{array}\right], \vec{v}_{2}=\left[ \begin{array}{l}{\frac{1}{\sqrt{2}}} \\ {\frac{-1}{\sqrt{2}}}\end{array}\right]$。</p><p><strong>step-2.1</strong></p><p>计算第二个对角矩阵，$B=W W^{T}=\left[ \begin{array}{ll}{1} &amp; {1} \\ {0} &amp; {1} \\ {1} &amp; {0}\end{array}\right] \left[ \begin{array}{lll}{1} &amp; {0} &amp; {1} \\ {1} &amp; {1} &amp; {0}\end{array}\right]=\left[ \begin{array}{lll}{2} &amp; {1} &amp; {1} \\ {1} &amp; {1} &amp; {0} \\ {1} &amp; {0} &amp; {1}\end{array}\right]$，和step-1.1比起来，能看到区别在于$W^{T}W$和$WW^{T}$。</p><p><strong>step-2.2</strong></p><p>同样，求它的特征值$\lambda_{1}=3, \lambda_{2}=1, \lambda_{3}=0$和特征向量$\vec{u}_{1}=\left[ \begin{array}{c}{\frac{2}{\sqrt{6} } } \\ {\frac{1}{\sqrt{6} } } \\ {\frac{1}{\sqrt{6} } }\end{array}\right], \vec{u}_{2}=\left[ \begin{array}{c}{0} \\ {\frac{-1}{\sqrt{2} } } \\ {\frac{1}{\sqrt{2} } }\end{array}\right], \vec{u}_{3}=\left[ \begin{array}{c}{\frac{-1}{\sqrt{3} } } \\ {\frac{1}{\sqrt{3} } } \\ {\frac{1}{\sqrt{3} } }\end{array}\right]$。</p><p>最终我们根据公式：$W=U \Sigma V^{T}$，就可以得到</p><p>$W=\left[ \begin{array}{ll}{1} &amp; {1} \\ {0} &amp; {1} \\ {1} &amp; {0}\end{array}\right]=\left[ \begin{array}{ccc}{\frac{2}{\sqrt{6}}} &amp; {0} &amp; {-\frac{1}{\sqrt{3}}} \\ {\frac{1}{\sqrt{6}}} &amp; {\frac{-1}{\sqrt{2}}} &amp; {\frac{1}{\sqrt{3}}} \\ {\frac{1}{\sqrt{6}}} &amp; {\frac{1}{\sqrt{2}}} &amp; {\frac{1}{\sqrt{3}}}\end{array}\right] \left[ \begin{array}{cc}{\sqrt{3}} &amp; {0} \\ {0} &amp; {1} \\ {0} &amp; {0}\end{array}\right] \left[ \begin{array}{cc}{\frac{1}{\sqrt{2}}} &amp; {\frac{1}{\sqrt{2}}} \\ {\frac{1}{\sqrt{2}}} &amp; {\frac{-1}{\sqrt{2}}}\end{array}\right]$</p><p>其中$U$就是$B$的特征向量拼成的酉矩阵，$V^{T}$就是$C$的特征向量拼成的酉矩阵。而$\Sigma$(即S)就是由<strong>特征值开根号</strong>得到的。</p><p>我们还可以把上面的矩阵写成另外的表达方式：</p><p>$W=\sqrt{3} \left[ \begin{array}{c}{\frac{2}{\sqrt{6}}} \\ {\frac{1}{\sqrt{6}}} \\ {\frac{1}{\sqrt{6}}}\end{array}\right] \left[ \begin{array}{cc}{\frac{1}{\sqrt{2}}} &amp; {\frac{1}{\sqrt{2}}}\end{array}\right]+\left[ \begin{array}{c}{0} \\ {-\frac{1}{\sqrt{2}}} \\ {\frac{1}{\sqrt{2}}}\end{array}\right] \left[ \begin{array}{cc}{\frac{1}{\sqrt{2}}} &amp; {\frac{-1}{\sqrt{2}}}\end{array}\right]$</p><p>也就是$W=\sqrt\lambda_{1}\vec{u}_{1}\left(\vec{v}_{1}\right)^{T}+\sqrt\lambda_{2}\vec{u}_{2}\left(\vec{v}_{2}\right)^{T}$。再说一次，<strong>奇异值的平方=特征值</strong>。</p><p>这样也就可以解释为什么SVD会被用于PCA，实际上我们只是举了很简单的例子，当矩阵的维数更高时，我们最后得到的式子的项数也就越多，但是有些项数的奇异值很小，舍去这些项虽然会丢失局部细节，但是能够在尽量保真的情况下节约存储空间。</p><p><strong>参考资料：</strong></p><p>[1] <a href="http://www.cnblogs.com/LeftNotEasy/archive/2011/01/19/svd-and-applications.html" target="_blank" rel="noopener">机器学习中的数学(5)-强大的矩阵奇异值分解(SVD)及其应用</a></p><p>[2] <a href="https://blog.csdn.net/u012162613/article/details/42214205" target="_blank" rel="noopener">【简化数据】奇异值分解(SVD)</a> </p><p>[3] <a href="https://www.bilibili.com/video/av15971352/?p=3" target="_blank" rel="noopener">矩阵分析之奇异值分解（SVD）</a></p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;在做PCA的实验时，遇到的SVD奇异值分解问题，这里记录自己直观的理解&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;通常在&lt;code&gt;python&lt;/code&gt;中我们都是直接调用&lt;code&gt;numpy&lt;/code&gt;模块中的函数，那么返回的U,S,V几个值代表什么意思呢？&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;U, S, V = np.linalg.svd(X)&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;遇事不明，手册先行：&lt;a href=&quot;https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.svd.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;官方手册&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="Machine Learning" scheme="http://yoursite.com/tags/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>机器学习笔记-第八周</title>
    <link href="http://yoursite.com/2019/04/29/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E7%AC%AC%E5%85%AB%E5%91%A8/"/>
    <id>http://yoursite.com/2019/04/29/机器学习笔记-第八周/</id>
    <published>2019-04-29T03:55:29.000Z</published>
    <updated>2019-04-29T04:02:11.567Z</updated>
    
    <content type="html"><![CDATA[<h2 id="无监督学习"><a href="#无监督学习" class="headerlink" title="无监督学习"></a>无监督学习</h2><h3 id="无监督学习-1"><a href="#无监督学习-1" class="headerlink" title="无监督学习"></a>无监督学习</h3><p>之间学习的都是监督学习，也就是样本都有标签。</p><p><img src="https://s2.ax1x.com/2019/04/28/EMlktI.png" alt="EMlktI.png"></p><a id="more"></a><p>而无监督学习的样本是没有标签的，也就是只有输入x，没有输出标记。</p><p><img src="https://s2.ax1x.com/2019/04/28/EMllAs.md.png" alt="EMllAs.md.png"></p><p>无监督学习就是找到隐含在这类无标签数据中的结构，这类算法称为聚类算法（<strong>clustering algorithm</strong>）。</p><h3 id="K-Means算法"><a href="#K-Means算法" class="headerlink" title="K-Means算法"></a>K-Means算法</h3><p>k均值算法，先随机生成两个数据，即聚类中心（<strong>cluster centroids</strong>）。</p><p><img src="https://s2.ax1x.com/2019/04/28/EM1lrD.md.png" alt="EM1lrD.md.png"></p><p>然后遍历所有数据，和蓝色聚类中心靠近的，就渲染为蓝色；和红色聚类中心靠近的，就渲染为红色。然后移动聚类中心到同颜色点的均值处。</p><p><img src="https://s2.ax1x.com/2019/04/28/EM1Bqg.png" alt="EM1Bqg.png"></p><p>重复上述步骤，直到聚类完成，聚类中心不再变化，样本的分类也不再变化。</p><p><img src="https://s2.ax1x.com/2019/04/28/EM14LF.png" alt="EM14LF.png"></p><p>通过图像我们可以直观的了解到，使用K均值算法聚类，输入需要样本特征$x^{(i)}$和聚类中心数K。</p><p>具体步骤如下：</p><p><img src="https://s2.ax1x.com/2019/04/28/EM8ObD.png" alt="EM8ObD.png"></p><p>循环1是根据聚类中心分类，循环2是根据平均值移动聚类中心。其中$c^{(i)}$表示的是$x^{(i)}$到不同聚类中心，距离最小的那个的聚类中心的索引值。</p><p>有时候数据的分类不那么明确，比如T-shirt的尺寸。</p><p><img src="https://s2.ax1x.com/2019/04/28/EMGLzq.png" alt="EMGLzq.png"></p><p>同样也可以用K均值算法进行聚类。将T-shirt的尺寸根据身高和体重，分类为S、M、L。</p><p><img src="https://s2.ax1x.com/2019/04/28/EMGvLT.png" alt="EMGvLT.png"></p><h3 id="优化目标"><a href="#优化目标" class="headerlink" title="优化目标"></a>优化目标</h3><p>K均值算法同样有优化目标，或者说代价函数（这里也叫畸变函数）。</p><p><img src="https://s2.ax1x.com/2019/04/28/EMNc0f.png" alt="EMNc0f.png"></p><p>代价函数就是样本到聚类中心距离的平方和，而优化目标就是找到代价函数值最小时的$c^{(i)}$和聚类中心$\mu_{k}$，从下图也可以看出两个循环就是为了得到它们。</p><p><img src="https://s2.ax1x.com/2019/04/28/EMUFAO.md.png" alt="EMUFAO.md.png"></p><h3 id="随机初始化"><a href="#随机初始化" class="headerlink" title="随机初始化"></a>随机初始化</h3><p>在实际运用中，一般来说，都是随机选取K个样本点作为聚类中心而不是像上面那样选择非样本点。当然，K的值要小于m。</p><p>通常我们都期望随机选取的聚类中心具有比较好的性质</p><p><img src="https://s2.ax1x.com/2019/04/28/EMwxIK.png" alt="EMwxIK.png"></p><p>但人生不如意之事十之八九，有时候选择的聚类中心不理想</p><p><img src="https://s2.ax1x.com/2019/04/28/EM08Zq.png" alt="EM08Zq.png"></p><p>就会在训练的过程中，导致局部最优解，如下图中右下角那样的结果。</p><p><img src="https://s2.ax1x.com/2019/04/28/EMwD58.png" alt="EMwD58.png"></p><p>解决这个问题的方法简单粗暴，就是多次随机初始化聚类中心，得到多个拟合结果，然后挑选出代价最小的分类方法    。</p><p><img src="https://s2.ax1x.com/2019/04/28/EMBGXd.png" alt="EMBGXd.png"></p><h3 id="选取聚类数量"><a href="#选取聚类数量" class="headerlink" title="选取聚类数量"></a>选取聚类数量</h3><p>关于如何确定K值，是一个很难回答的问题，甚至可以说没有确切的答案。但通常我们会采用以下的方法来帮助确定K的大小。</p><ul><li>肘部法则（<strong>Elbow method</strong>）</li></ul><p><img src="https://s2.ax1x.com/2019/04/28/EM2vX8.png" alt="EM2vX8.png"></p><p>但有些时候，得到的图像看起来像是连续的，不容易判断“肘部“位置。</p><p><img src="https://s2.ax1x.com/2019/04/28/EMRn74.png" alt="EMRn74.png"></p><p>所以这个方法没办法适用于所有情况，所以通常推荐的方法是：根据你聚类的目的来确定K的数目（感觉是废话）。</p><h2 id="降维-Dimensionality-Reduction"><a href="#降维-Dimensionality-Reduction" class="headerlink" title="降维(Dimensionality Reduction)"></a>降维(Dimensionality Reduction)</h2><p>除了聚类之外，还有另一种无监督学习算法，叫做降维。</p><h3 id="目标I-数据压缩"><a href="#目标I-数据压缩" class="headerlink" title="目标I:数据压缩"></a>目标I:数据压缩</h3><p>降维的作用是消除特征冗余，如下图所示，特征$x_{1}$代表cm而特征$x_{2}$代表inches，含义相同，在图像上呈现出线性关系，完全可以只用一个特征来表示。</p><p><img src="https://s2.ax1x.com/2019/04/28/EMfnY9.png" alt="EMfnY9.png"></p><p>更一般的，我们可以把两个冗余的特征用一个新的特征来表示，记为$z_{1}$，如下图。从另一个角度来看，也可以认为是把所有数据投影到了绿色的那条线上。</p><p><img src="https://s2.ax1x.com/2019/04/28/EMfs0S.png" alt="EMfs0S.png"></p><p>当然，很多时候冗余的特征不只两个，但原理是一样的，比如以3维为例：</p><p><img src="https://s2.ax1x.com/2019/04/28/EMfo0U.png" alt="EMfo0U.png"></p><h3 id="目标II：可视化"><a href="#目标II：可视化" class="headerlink" title="目标II：可视化"></a>目标II：可视化</h3><p>特征很多的情况下没办法直接可视化。</p><p><img src="https://s2.ax1x.com/2019/04/28/EQYCZT.png" alt="EQYCZT.png"></p><p>这时候需要用到降维。</p><p><img src="https://s2.ax1x.com/2019/04/28/EQYBFg.png" alt="EQYBFg.png"></p><p>降维之后的特征可能不具有物理意义，但方便画图。</p><p><img src="https://s2.ax1x.com/2019/04/28/EQY7lR.png" alt="EQY7lR.png"></p><h3 id="主成分分析问题规划I"><a href="#主成分分析问题规划I" class="headerlink" title="主成分分析问题规划I"></a>主成分分析问题规划I</h3><p>主成分分析问题规划（<strong>Principle Component Analysis</strong>），是目前最流行的降维算法。举个例子简单说明原理：</p><p><img src="https://s2.ax1x.com/2019/04/28/EQavDI.png" alt="EQavDI.png"></p><p>图为从2维降到1维，PCA要求样本到投影点的距离的（投影误差）平方和最小。</p><p>更一般的情况，从n-D投影到k-D，也就是找到k个向量$u^{(1)},u^{(2)},…,u^{(3)}$作为被投影的线性子空间。再用3D投影到2D为例，记住PCA关键是投影误差要最小</p><p><img src="https://s2.ax1x.com/2019/04/28/EQWAYj.png" alt="EQWAYj.png"></p><ul><li>PCA和线性回归的区别</li></ul><p>二者毫无联系，虽然有时看上去图像是一样的，但线性回归时预测输出变量，它的代价函数表示的是拟合模型的输出误差，如图左；PCA的代价函数则是投影误差，如图右。</p><p><img src="https://s2.ax1x.com/2019/04/28/EQW73n.png" alt="EQW73n.png"></p><h3 id="主成分分析问题规划II"><a href="#主成分分析问题规划II" class="headerlink" title="主成分分析问题规划II"></a>主成分分析问题规划II</h3><ul><li>特征缩放和均值归一化</li></ul><p><img src="https://s2.ax1x.com/2019/04/29/ElGKMt.png" alt="ElGKMt.png"></p><ul><li>求出$u^{(i)}$</li></ul><p>把数据从n维降到k维。</p><p>计算协方差矩阵：</p><p><img src="https://s2.ax1x.com/2019/04/29/ElGIoD.png" alt="ElGIoD.png"></p><p>其中$x^{(i)}$是nx1维，所以$\Sigma$是nxn维。写成向量形式：$\Sigma=1 / m\left(X^{T} X\right)$</p><p>再用svd对协方差矩阵进行奇异值分解：</p><p><img src="https://s2.ax1x.com/2019/04/29/ElGHWd.png" alt="ElGHWd.png"></p><p>最后我们需要的是U矩阵（nxn），取它的k个列向量构成矩阵z，再乘上$x^{(i)}$就可以得到$z^{(i)}$。</p><p><img src="https://s2.ax1x.com/2019/04/29/ElGvef.png" alt="ElGvef.png"></p><h3 id="主成分数量选择"><a href="#主成分数量选择" class="headerlink" title="主成分数量选择"></a>主成分数量选择</h3><p>之前提到过如何选择压缩后的维度k是很困难的，但在PCA中有一种很实用的方法，这一节展开说说。</p><p>通常来说，选择的k要让平均投影误差/数据总方差小于0.01，这种情况也称为“99%的方差被保留”，这句话代表了投影降维后的效果好。</p><p><img src="https://s2.ax1x.com/2019/04/29/ElwiUP.png" alt="ElwiUP.png"></p><p>从上面的公式可以看出，如果需要选择出合适的K，我们可以尝试不同的K值，直到找到能让公式小于0.01为止。</p><p><img src="https://s2.ax1x.com/2019/04/29/El07Y6.png" alt="El07Y6.png"></p><p>但这种方法显然效率不高，因此最好使用另一种方法。</p><p>之前提到的奇异值分解：$[U,S,V]=svd(Sigma)$，其中的矩阵S是一个对角矩阵，如下图所示，取主对角线中的k个值，满足$1-\frac{\sum_{i=1}^{k} S_{i i} }{\sum_{i=1}^{k} S_{ii} } \leqslant 0.01$即可。</p><p><img src="https://s2.ax1x.com/2019/04/29/El0XOH.png" alt="El0XOH.png"></p><h3 id="压缩重现"><a href="#压缩重现" class="headerlink" title="压缩重现"></a>压缩重现</h3><p>如果能从n维降至k维，同理也就能从k维升至n维。只需要把$z^{(i)}$代入$X_{appox} $=$ U_{radue} \cdot z^{(i)}$。就能求出x近似值。</p><p><img src="https://s2.ax1x.com/2019/04/29/ElDQUI.png" alt="ElDQUI.png"></p><h3 id="应用PCA的建议"><a href="#应用PCA的建议" class="headerlink" title="应用PCA的建议"></a>应用PCA的建议</h3><p>PCA也可以应用在监督学习算法中，只需要将$x^{(i)}$提取出来即可。另外必须要注意的是，把$x^{(i)}$映射到$z^{(i)}$（即求出U）的过程中只能在训练集中运行。</p><p><img src="https://s2.ax1x.com/2019/04/29/Elsvgf.png" alt="Elsvgf.png"></p><ul><li>PCA的应用</li></ul><p>（1）压缩数据，加快算法运行效率</p><p>（2）可视化：没得办法，我们只能可视化K&lt;3的数据</p><p>（3）不要用PCA来防止过拟合，因为PCA不使用标签y，这可能会导致信息的丢失。用正则化就行了，不要皮。</p><p>（4）在使用PCA之前，应该先考虑清楚是否真的有必要使用它，直接用原始数据也许就能得出想要的答案。</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;无监督学习&quot;&gt;&lt;a href=&quot;#无监督学习&quot; class=&quot;headerlink&quot; title=&quot;无监督学习&quot;&gt;&lt;/a&gt;无监督学习&lt;/h2&gt;&lt;h3 id=&quot;无监督学习-1&quot;&gt;&lt;a href=&quot;#无监督学习-1&quot; class=&quot;headerlink&quot; title=&quot;无监督学习&quot;&gt;&lt;/a&gt;无监督学习&lt;/h3&gt;&lt;p&gt;之间学习的都是监督学习，也就是样本都有标签。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://s2.ax1x.com/2019/04/28/EMlktI.png&quot; alt=&quot;EMlktI.png&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="Machine Learning" scheme="http://yoursite.com/tags/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>机器学习-吴恩达机器学习之支持向量机</title>
    <link href="http://yoursite.com/2019/04/29/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/"/>
    <id>http://yoursite.com/2019/04/29/机器学习-吴恩达机器学习之支持向量机/</id>
    <published>2019-04-28T23:46:42.000Z</published>
    <updated>2019-04-29T08:10:07.261Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Support-Vector-Machine"><a href="#Support-Vector-Machine" class="headerlink" title="Support Vector Machine"></a>Support Vector Machine</h2><p>导入库函数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> scipy.io <span class="keyword">import</span> loadmat</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> svm</span><br></pre></td></tr></table></figure><a id="more"></a><h3 id="Example-Dataset-1"><a href="#Example-Dataset-1" class="headerlink" title="Example Dataset 1"></a>Example Dataset 1</h3><p>画出含有两个特征的样本集的线性边界函数。</p><h4 id="数据处理"><a href="#数据处理" class="headerlink" title="数据处理"></a>数据处理</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">data=loadmat(<span class="string">'ex6data1.mat'</span>)</span><br><span class="line">X=data[<span class="string">'X'</span>]</span><br><span class="line">y=data[<span class="string">'y'</span>].flatten()</span><br></pre></td></tr></table></figure><p>不知道标签的话可以查看</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(data.keys()) <span class="comment"># 用于查看标签名称</span></span><br></pre></td></tr></table></figure><ul><li>将数据可视化以便观察，注意要把正负样本分开可视化。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#==================================观察数据</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_data</span><span class="params">(X,y)</span>:</span></span><br><span class="line">    positive=X[y==<span class="number">1</span>]</span><br><span class="line">    negative=X[y==<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    fig,ax=plt.subplots(figsize=(<span class="number">8</span>,<span class="number">5</span>))    </span><br><span class="line">    plt.scatter(positive[:,<span class="number">0</span>],positive[:,<span class="number">1</span>],marker=<span class="string">'+'</span>,label=<span class="string">'positive'</span>)</span><br><span class="line">    plt.scatter(negative[:,<span class="number">0</span>],negative[:,<span class="number">1</span>],color=<span class="string">'red'</span>,label=<span class="string">'negative'</span>)</span><br></pre></td></tr></table></figure><h4 id="SVM拟合"><a href="#SVM拟合" class="headerlink" title="SVM拟合"></a>SVM拟合</h4><p>可以通过改变c的大小来观察决策边界的变换。C的大小影响着模型对异常样本的反应。</p><p>因为这里是线性分类，所以使用线性核函数，参数<code>kernel=&#39;linear&#39;</code>。使用<code>sklearn</code>中的<code>svm.SVC()</code>来拟合，其结果返回一个分类器对象。最后还需要用<code>clf.fit(X,y)</code>来拟合出最终模型。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">c=<span class="number">1</span></span><br><span class="line">clf=svm.SVC(c,kernel=<span class="string">'linear'</span>) <span class="comment"># 参数 c,kernel 返回一个分类器对象</span></span><br><span class="line">clf.fit(X,y) <span class="comment"># 用训练数据拟合分类器模型</span></span><br></pre></td></tr></table></figure><h4 id="可视化决策边界"><a href="#可视化决策边界" class="headerlink" title="可视化决策边界"></a>可视化决策边界</h4><p><code>np.meshgrid()</code>生成网格点，再对每个网格点进行预测，最后画出等高线图，即决策边界。</p><p>关于<code>np.meshgrid()</code>可以参考 <a href="https://nullblog.top/2019/03/16/Numpy%E4%B8%AD%E7%9A%84Meshgrid/" target="_blank" rel="noopener">Numpy中的Meshgrid</a>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#=============================================可视化决策边界</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">visualize_boundary</span><span class="params">(clf,X)</span>:</span></span><br><span class="line">    x_min,x_max=X[:,<span class="number">0</span>].min()*<span class="number">1.2</span>,X[:,<span class="number">0</span>].max()*<span class="number">1.1</span></span><br><span class="line">    y_min,y_max=X[:,<span class="number">1</span>].min()*<span class="number">1.2</span>,X[:,<span class="number">1</span>].max()*<span class="number">1.1</span></span><br><span class="line">    xx,yy=np.meshgrid(np.arange(x_min,x_max,<span class="number">0.02</span>),np.arange(y_min,y_max,<span class="number">0.02</span>)) <span class="comment"># 画网格点</span></span><br><span class="line">    Z=clf.predict(np.c_[xx.ravel(),yy.ravel()]) <span class="comment"># 用训练好的分类器对网格点进行预测</span></span><br><span class="line"></span><br><span class="line">    Z=Z.reshape(xx.shape) <span class="comment"># 转换成对应的网格点</span></span><br><span class="line">    plt.contour(xx,yy,Z,level=[<span class="number">0</span>],colors=<span class="string">'black'</span>) <span class="comment"># 等高线图，画出0/1分界线</span></span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><p>像前面提到的，不同的C值对决策边界会产生不同影响</p><p>c=1时：</p><p><img src="https://s2.ax1x.com/2019/04/27/EKmvo6.md.png" alt="EKmvo6.md.png"></p><p>c=1000时：</p><p><img src="https://s2.ax1x.com/2019/04/27/EKno7t.md.png" alt="EKno7t.md.png"></p><p>注意左上角的正样本，c较大时，决策边界会过于追求将数据正确分类，而失去<strong>大间距</strong>的特点。</p><h3 id="SVM-with-Gaussion-Kernels"><a href="#SVM-with-Gaussion-Kernels" class="headerlink" title="SVM with Gaussion Kernels"></a>SVM with Gaussion Kernels</h3><h4 id="Gaussion-Kernel"><a href="#Gaussion-Kernel" class="headerlink" title="Gaussion Kernel"></a>Gaussion Kernel</h4><p>用SVM做分线性分类，我们需要用到高斯核函数。</p><ul><li>公式：</li></ul><p><img src="https://s2.ax1x.com/2019/04/27/EKKnMQ.md.png" alt="EKKnMQ.md.png"></p><ul><li>代码：</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#=============================================高斯核函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gaussKernel</span><span class="params">(x1,x2,sigma)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> np.exp(-((x1-x2)**<span class="number">2</span>).sum()/(<span class="number">2</span>*sigma**<span class="number">2</span>))</span><br></pre></td></tr></table></figure><h4 id="Example-Dataset-2"><a href="#Example-Dataset-2" class="headerlink" title="Example Dataset 2"></a>Example Dataset 2</h4><ul><li>数据处理及可视化</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">data2=loadmat(<span class="string">'ex6data2.mat'</span>)</span><br><span class="line">X2=data2[<span class="string">'X'</span>]</span><br><span class="line">y2=data2[<span class="string">'y'</span>].flatten()</span><br></pre></td></tr></table></figure><p><img src="https://s2.ax1x.com/2019/04/27/EKKoJf.md.png" alt="EKKoJf.md.png"></p><ul><li>用高斯核函数拟合模型</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sigma=<span class="number">0.1</span></span><br><span class="line">gamma=np.power(sigma,<span class="number">-2</span>)/<span class="number">2</span></span><br><span class="line">clf=svm.SVC(c,kernel=<span class="string">'rbf'</span>,gamma=gamma) <span class="comment"># 注意这里的参数gamma是整个分母，且要写成乘法形式</span></span><br><span class="line">clf.fit(X2,y2)</span><br></pre></td></tr></table></figure><p><code>kernel=&#39;rbf&#39;</code>代表使用高斯核函数，其中<code>gamma</code>值就是公式中的整个分母项，即$\frac{1}{2\sigma^{2} }$。</p><ul><li>决策边界</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">plot_data(X2,y2)</span><br><span class="line">visualize_boundary(clf,X2)</span><br></pre></td></tr></table></figure><p><img src="https://s2.ax1x.com/2019/04/27/EKJ52q.md.png" alt="EKJ52q.md.png"></p><h4 id="Example-Dataset-3"><a href="#Example-Dataset-3" class="headerlink" title="Example Dataset 3"></a>Example Dataset 3</h4><ul><li>数据处理即可视化</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">data3=loadmat(<span class="string">'ex6data3.mat'</span>)</span><br><span class="line">X3=data3[<span class="string">'X'</span>]</span><br><span class="line">y3=data3[<span class="string">'y'</span>].flatten()</span><br></pre></td></tr></table></figure><p><img src="https://s2.ax1x.com/2019/04/27/EKGYkj.md.png" alt="EKGYkj.md.png"></p><p>可以发现，有个别样本存在比较大的差异。</p><ul><li>使用带高斯核函数的SVM进行训练</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">clf=svm.SVC(c,kernel=<span class="string">'rbf'</span>,gamma=gamma)</span><br><span class="line">clf.fit(X3,y3)</span><br></pre></td></tr></table></figure><ul><li>决策边界</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">plot_data(X3,y3)</span><br><span class="line">visualize_boundary(clf,X3)</span><br></pre></td></tr></table></figure><p><img src="https://s2.ax1x.com/2019/04/27/EKGHNd.md.png" alt="EKGHNd.md.png"></p><h2 id="Spam-Classification"><a href="#Spam-Classification" class="headerlink" title="Spam Classification"></a>Spam Classification</h2><p>建立一个垃圾邮件分类器，下面这个例子将会告诉你如何通过一封邮件来建立特征向量。</p><p>导入库函数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> scipy.io <span class="keyword">import</span> loadmat</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> svm</span><br><span class="line"><span class="keyword">import</span> re <span class="comment"># 电子邮件处理的正则表达式</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 一个英文分词算法(Poter stemmer)</span></span><br><span class="line"><span class="keyword">import</span> nltk, nltk.stem.porter</span><br></pre></td></tr></table></figure><h3 id="Preprocesssing-Email"><a href="#Preprocesssing-Email" class="headerlink" title="Preprocesssing Email"></a>Preprocesssing Email</h3><ul><li>读取数据</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> open(<span class="string">'emailSample1.txt'</span>,<span class="string">'r'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    email=f.read()</span><br></pre></td></tr></table></figure><p>打印邮件内容为</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&gt; Anyone knows how much it costs to host a web portal ?</span><br><span class="line">&gt;</span><br><span class="line">Well, it depends on how many visitors you&apos;re expecting.</span><br><span class="line">This can be anywhere from less than 10 bucks a month to a couple of $100. </span><br><span class="line">You should checkout http://www.rackspace.com/ or perhaps Amazon EC2 </span><br><span class="line">if youre running something big..</span><br><span class="line"></span><br><span class="line">To unsubscribe yourself from this mailing list, send an email to:</span><br><span class="line">groupname-unsubscribe@egroups.com</span><br></pre></td></tr></table></figure><p>一般邮件都具有一些相似的内容，比如数字、URL、其它邮件地址。因此我们会采取一些”标准化“的方法来处理邮件，这些方法会提高垃圾邮件分类的性能。</p><p>例如：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">1. Lower-casing: 把整封邮件转化为小写。</span><br><span class="line">2. Stripping HTML: 移除所有HTML标签，只保留内容。</span><br><span class="line">3. Normalizing URLs: 将所有的URL替换为字符串 “httpaddr”.</span><br><span class="line">4. Normalizing Email Addresses: 所有的地址替换为 “emailaddr”</span><br><span class="line">5. Normalizing Dollars: 所有dollar符号($)替换为“dollar”.</span><br><span class="line">6. Normalizing Numbers: 所有数字替换为“number”</span><br><span class="line">7. Word Stemming(词干提取): 将所有单词还原为词源。例如，“discount”, “discounts”, “discounted” and “discounting”都替换为“discount”。</span><br><span class="line">8. Removal of non-words: 移除所有非文字类型，所有的空格(tabs, newlines, spaces)调整为一个空格.</span><br></pre></td></tr></table></figure><ul><li>邮件内容处理：</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">process_email</span><span class="params">(email)</span>:</span></span><br><span class="line">    <span class="string">"""做除了Word Stemming和Removal of non-words的所有处理"""</span></span><br><span class="line">    email = email.lower()</span><br><span class="line">    email = re.sub(<span class="string">'&lt;[^&lt;&gt;]&gt;'</span>, <span class="string">' '</span>, email)  </span><br><span class="line">    <span class="comment"># 匹配&lt;开头，然后所有不是&lt; ,&gt; 的内容，知道&gt;结尾，相当于匹配&lt;...&gt;</span></span><br><span class="line">    email = re.sub(<span class="string">'(http|https)://[^\s]*'</span>, <span class="string">'httpaddr'</span>, email )  </span><br><span class="line">    <span class="comment"># 匹配//后面不是空白字符的内容，遇到空白字符则停止</span></span><br><span class="line">    email = re.sub(<span class="string">'[^\s]+@[^\s]+'</span>, <span class="string">'emailaddr'</span>, email)</span><br><span class="line">    email = re.sub(<span class="string">'[\$]+'</span>, <span class="string">'dollar'</span>, email)</span><br><span class="line">    email = re.sub(<span class="string">'[\d]+'</span>, <span class="string">'number'</span>, email) </span><br><span class="line">    <span class="keyword">return</span> email</span><br></pre></td></tr></table></figure><p>再接下来提取词干，去除非字符内容，并返回一个单词列表。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">email2TokenList</span><span class="params">(email)</span>:</span></span><br><span class="line">    <span class="string">"""预处理数据，返回一个干净的单词列表"""</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># I'll use the NLTK stemmer because it more accurately duplicates the</span></span><br><span class="line">    <span class="comment"># performance of the OCTAVE implementation in the assignment</span></span><br><span class="line">    stemmer = nltk.stem.porter.PorterStemmer()</span><br><span class="line">    </span><br><span class="line">    email = process_email(email)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将邮件分割为单个单词，re.split() 可以设置多种分隔符</span></span><br><span class="line">    tokens = re.split(<span class="string">'[ \@\$\/\#\.\-\:\&amp;\*\+\=\[\]\?\!\(\)\&#123;\&#125;\,\'\"\&gt;\_\&lt;\;\%]'</span>, email)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 遍历每个分割出来的内容</span></span><br><span class="line">    tokenlist = []</span><br><span class="line">    <span class="keyword">for</span> token <span class="keyword">in</span> tokens:</span><br><span class="line">        <span class="comment"># 删除任何非字母数字的字符</span></span><br><span class="line">        token = re.sub(<span class="string">'[^a-zA-Z0-9]'</span>, <span class="string">''</span>, token);</span><br><span class="line">        <span class="comment"># Use the Porter stemmer to 提取词根</span></span><br><span class="line">        stemmed = stemmer.stem(token)</span><br><span class="line">        <span class="comment"># 去除空字符串‘’，里面不含任何字符</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> len(token): <span class="keyword">continue</span></span><br><span class="line">        tokenlist.append(stemmed)</span><br><span class="line">            </span><br><span class="line">    <span class="keyword">return</span> tokenlist</span><br></pre></td></tr></table></figure><h4 id="Vocabulary-List"><a href="#Vocabulary-List" class="headerlink" title="Vocabulary List"></a>Vocabulary List</h4><p>我们得到了邮件的单词列表，接下来需要结合记录实际中经常使用到的单词的词汇表<code>vocab.txt</code>。函数返回邮件单词在词汇表中的索引值。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">email2VocabIndices</span><span class="params">(email, vocab)</span>:</span></span><br><span class="line">    <span class="string">"""提取存在单词的索引"""</span></span><br><span class="line">    token = email2TokenList(email)</span><br><span class="line">    index = [i <span class="keyword">for</span> i <span class="keyword">in</span> range(len(vocab)) <span class="keyword">if</span> vocab[i] <span class="keyword">in</span> token]</span><br><span class="line">    <span class="keyword">return</span> index</span><br></pre></td></tr></table></figure><p>得到的索引值如下：</p><p><img src="https://s2.ax1x.com/2019/04/27/EK0An1.png" alt="EK0An1.png"></p><h4 id="Extracting-Feature-from-Emails"><a href="#Extracting-Feature-from-Emails" class="headerlink" title="Extracting Feature from Emails"></a>Extracting Feature from Emails</h4><p>如果邮件中的单词出现在词汇表的第i个位置，则把特征向量的第i个索引值置为1,；如果没出现，置为0。也就是说，建立一个和词汇表同维度的向量feature，再把上面得到的索引位置的值改写为1，其余为0。可以得到：</p><p><img src="https://s2.ax1x.com/2019/04/27/EKcP5q.png" alt="EKcP5q.png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">email_feature_vector</span><span class="params">(email)</span>:</span></span><br><span class="line">    <span class="string">'''将email的单词转换为特征向量0/1'''</span></span><br><span class="line">    df = pd.read_table(<span class="string">'vocab.txt'</span>, names=[<span class="string">'words'</span>])</span><br><span class="line">    vocab = df.values <span class="comment"># Datafram转换为ndarray</span></span><br><span class="line">    vector = np.zeros(len(vocab))</span><br><span class="line">    vecab_indices = email2VocabIndices(email, vocab)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> vecab_indices:</span><br><span class="line">        vector[i] = <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> vector</span><br></pre></td></tr></table></figure><p>该向量长度为1899，其中有45个索引值为1。</p><h3 id="Training-SVM-for-Spam-Classification"><a href="#Training-SVM-for-Spam-Classification" class="headerlink" title="Training SVM for Spam Classification"></a>Training SVM for Spam Classification</h3><p>用已经预处理过的训练集和测试集来拟合模型，并计算准确度。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">clf = svm.SVC(C=<span class="number">0.1</span>, kernel=<span class="string">'linear'</span>)</span><br><span class="line">clf.fit(X, y)</span><br><span class="line">print(clf.score(X,y),clf.score(Xtest,ytest))</span><br></pre></td></tr></table></figure><p>打印结果：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">0.99825 0.989</span><br></pre></td></tr></table></figure><h3 id="Top-predictors-for-Spam"><a href="#Top-predictors-for-Spam" class="headerlink" title="Top predictors for Spam"></a>Top predictors for Spam</h3><p>返回权重最大的15个单词，这些单词出现频率高的邮件就是垃圾邮件。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_vocab_list</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">'''以字典形式获得词汇表'''</span></span><br><span class="line">    vocab_dict = &#123;&#125;</span><br><span class="line">    <span class="keyword">with</span> open(<span class="string">'vocab.txt'</span>) <span class="keyword">as</span> f:  <span class="comment">#打开txt格式的词汇表</span></span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> f:</span><br><span class="line">            (val, key) = line.split()  <span class="comment">#读取每一行的键和值</span></span><br><span class="line">            vocab_dict[int(val)] = key  <span class="comment">#存放到字典中</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> vocab_dict</span><br></pre></td></tr></table></figure><p>获取词汇表</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">vocab_list = get_vocab_list()  <span class="comment">#得到词汇表 存在字典中</span></span><br><span class="line">indices = np.argsort(clf.coef_).flatten()[::<span class="number">-1</span>]  <span class="comment">#对权重序号进行从大到小排序 并返回</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">15</span>):  <span class="comment">#打印权重最大的前15个词 及其对应的权重</span></span><br><span class="line">    print(<span class="string">'&#123;&#125; (&#123;:0.6f&#125;)'</span>.format(vocab_list[indices[i]],</span><br><span class="line">                                clf.coef_.flatten()[indices[i]]))</span><br></pre></td></tr></table></figure><p>最终打印结果:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">otherwis (0.500614)</span><br><span class="line">clearli (0.465916)</span><br><span class="line">remot (0.422869)</span><br><span class="line">gt (0.383622)</span><br><span class="line">visa (0.367710)</span><br><span class="line">base (0.345064)</span><br><span class="line">doesn (0.323632)</span><br><span class="line">wife (0.269724)</span><br><span class="line">previous (0.267298)</span><br><span class="line">player (0.261169)</span><br><span class="line">mortgag (0.257298)</span><br><span class="line">natur (0.253941)</span><br><span class="line">ll (0.253467)</span><br><span class="line">futur (0.248297)</span><br><span class="line">hot (0.246404)</span><br></pre></td></tr></table></figure><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p>[1] <a href="https://blog.csdn.net/sdu_hao/article/details/84189266" target="_blank" rel="noopener">机器学习 | 吴恩达机器学习第七周编程作业(Python版)</a></p><p>[2] <a href="https://blog.csdn.net/Cowry5/article/details/80465922" target="_blank" rel="noopener">吴恩达机器学习作业Python实现(六)：SVM支持向量机</a></p><blockquote><p>这次的代码几乎完全<strong>照抄</strong>两位大神的，实在是正则化的内容不懂，而因为用了sklearn库使得拟合变得又很简单。记录一下自己的naive，还得努力。</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Support-Vector-Machine&quot;&gt;&lt;a href=&quot;#Support-Vector-Machine&quot; class=&quot;headerlink&quot; title=&quot;Support Vector Machine&quot;&gt;&lt;/a&gt;Support Vector Machine&lt;/h2&gt;&lt;p&gt;导入库函数&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; numpy &lt;span class=&quot;keyword&quot;&gt;as&lt;/span&gt; np&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; pandas &lt;span class=&quot;keyword&quot;&gt;as&lt;/span&gt; pd&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; matplotlib.pyplot &lt;span class=&quot;keyword&quot;&gt;as&lt;/span&gt; plt&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;from&lt;/span&gt; scipy.io &lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; loadmat&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;from&lt;/span&gt; sklearn &lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; svm&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
    
      <category term="Machine Learning" scheme="http://yoursite.com/tags/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>numpy中向量的表示方法</title>
    <link href="http://yoursite.com/2019/04/27/numpy%E4%B8%AD%E5%90%91%E9%87%8F%E7%9A%84%E8%A1%A8%E7%A4%BA%E6%96%B9%E6%B3%95/"/>
    <id>http://yoursite.com/2019/04/27/numpy中向量的表示方法/</id>
    <published>2019-04-27T11:17:01.000Z</published>
    <updated>2019-04-27T11:27:46.982Z</updated>
    
    <content type="html"><![CDATA[<h3 id="行向量"><a href="#行向量" class="headerlink" title="行向量"></a>行向量</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">np.array([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>]])</span><br></pre></td></tr></table></figure><p>结果:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>]]</span><br></pre></td></tr></table></figure><p><code>.shape</code>为（1,3）。是一个一行三列的<strong>行向量</strong>。</p><a id="more"></a><h3 id="数组"><a href="#数组" class="headerlink" title="数组"></a>数组</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">np.array([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>])</span><br></pre></td></tr></table></figure><p>结果：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[1,2,3]</span><br></pre></td></tr></table></figure><p><code>.shape</code>为（3，）。是一个ndarray<strong>数组</strong>，严格来说并不是向量，注意和行向量区分。</p><h3 id="列向量"><a href="#列向量" class="headerlink" title="列向量"></a>列向量</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">np.array([[<span class="number">1</span>],[<span class="number">2</span>],[<span class="number">3</span>]])</span><br></pre></td></tr></table></figure><p>结果：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[[<span class="number">1</span>],</span><br><span class="line"> [<span class="number">2</span>],</span><br><span class="line"> [<span class="number">3</span>]]</span><br></pre></td></tr></table></figure><p><code>.shape</code>为（3,1）。是一个三行一列的<strong>列向量</strong>。</p>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;行向量&quot;&gt;&lt;a href=&quot;#行向量&quot; class=&quot;headerlink&quot; title=&quot;行向量&quot;&gt;&lt;/a&gt;行向量&lt;/h3&gt;&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;np.array([[&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;,&lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;,&lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;]])&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;结果:&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;[[&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;,&lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;,&lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;]]&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;&lt;code&gt;.shape&lt;/code&gt;为（1,3）。是一个一行三列的&lt;strong&gt;行向量&lt;/strong&gt;。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Python" scheme="http://yoursite.com/tags/Python/"/>
    
      <category term="Numpy" scheme="http://yoursite.com/tags/Numpy/"/>
    
  </entry>
  
  <entry>
    <title>机器学习笔记|第七周</title>
    <link href="http://yoursite.com/2019/04/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E7%AC%AC%E4%B8%83%E5%91%A8/"/>
    <id>http://yoursite.com/2019/04/26/机器学习笔记-第七周/</id>
    <published>2019-04-26T03:16:22.000Z</published>
    <updated>2019-04-26T03:19:51.909Z</updated>
    
    <content type="html"><![CDATA[<h2 id="支持向量机"><a href="#支持向量机" class="headerlink" title="支持向量机"></a>支持向量机</h2><h3 id="优化目标"><a href="#优化目标" class="headerlink" title="优化目标"></a>优化目标</h3><p>在监督学习中，重要的不是你选择的算法，而是应用这些算法时，选择的特征、正则化参数等等诸如此类。其中有一种非常强大的分类工具，称为支持向量机。（<strong>Support Vector Machine，SVM</strong>）</p><p>下面展示如何通过修改逻辑回归，来得到SVM。</p><p>首先回顾一下逻辑回归假设函数，注意<code>y=1</code>和<code>y=0</code>的情况。</p><p><img src="https://s2.ax1x.com/2019/04/24/EVoj9P.md.png" alt="EVoj9P.md.png"></p><a id="more"></a><p>进一步写出代价函数</p><p><img src="https://s2.ax1x.com/2019/04/24/EVTC7j.md.png" alt="EVTC7j.md.png"></p><p>当<code>y=1</code>时，此时代价函数剩左边一项$-y \log \frac{1}{1+e^{-\theta^{T} x} }$，图像为</p><p><img src="https://s2.ax1x.com/2019/04/24/EVTFNn.png" alt="EVTFNn.png"></p><p>其中的曲线为逻辑回归代价函数，两段直线为SVM的代价函数，记为$cost_{1}(z)$</p><p>同理，当<code>y=0</code>时，代价函数剩右边一项$(1-y) \log \left(1-\frac{1}{1+e^{-\theta^{T} x} }\right)$，图像为</p><p><img src="https://s2.ax1x.com/2019/04/24/EVTfEj.png" alt="EVTfEj.png"></p><p>其中的SVM代价函数，记为$cost_{0}(z)$。</p><p>完整的逻辑回归代价函数写成：$\min _{\theta} \frac{1}{m}\left[\sum_{i=1}^{m} y^{(i)}\left(-\log h_{\theta}\left(x^{(i)}\right)\right)+\left(1-y^{(i)}\right)\left(\left(-\log \left(1-h_{\theta}\left(x^{(i)}\right)\right)\right)\right]+\frac{\lambda}{2 m} \sum_{j=1}^{n} \theta_{j}^{2}\right.$</p><p>可以看出由代价项和正则化项两部分组成，基本形式为$A+\lambda B$，$\lambda$可以看做是控制两部分的<em>权重</em>。</p><p>要写出SVM的代价函数，首先我们把逻辑回归代价函数的$\frac{1}{m}$去掉，不影响$\theta$值。然后我们把正则化项的系数$\lambda$去掉，在代价项前面添上一个系数C，效果等同于$\frac{1}{\lambda}$。</p><p>因此，最终SVM的代价函数为：$\min _{\theta} C \sum_{i=1}^{m}\left[y^{(i)} \cos t_{1}\left(\theta^{T} x^{(i)}\right)+\left(1-y^{(i)}\right) \operatorname{cost}_{0}\left(\theta^{T} x^{(i)}\right)\right]+\frac{1}{2} \sum_{i=1}^{n} \theta_{j}^{2}$</p><p>假设函数为：</p><p><img src="https://s2.ax1x.com/2019/04/24/EVHUkd.png" alt="EVHUkd.png"></p><p>注意，和逻辑回归假设函数计算出的是概率不同，SVM假设函数直接计算结果(1/0)。</p><h3 id="直观理解大间距"><a href="#直观理解大间距" class="headerlink" title="直观理解大间距"></a>直观理解大间距</h3><p>SVM又被称为大间距分类器，以下直观讲解为什么。</p><p><img src="https://s2.ax1x.com/2019/04/25/EZ0zzq.md.png" alt="EZ0zzq.md.png"></p><p>从图中可以看出：</p><p><code>y=1</code>时，需要$\theta^{T}x\ge1$，区别于逻辑回归中$\ge0$，此时$cost_{1}(z)=0$，即SVM在正确分类的基础上还构建了一个安全距离。</p><p><code>y=0</code>时，需要$\theta^{T}x\le-1$，区别于逻辑回归中$\le0$，此时$cost_{0}(z)=0$，即SVM在正确分类的基础上还构建了一个安全距离。</p><p>接下来，如果我们把代价函数中的常熟C设置成一个非常大的值，例如C=100000，此时为了使代价最小，就必须让最优化函数<img src="https://s2.ax1x.com/2019/04/25/EZBHpR.md.png" alt="EZBHpR.md.png"></p><p>中蓝色框部分等于0，又根据<code>y=1</code>或<code>y=0</code>时$\theta^{T}x$的取值，可以把优化问题和约束条件改为：</p><p><img src="https://s2.ax1x.com/2019/04/25/EZBv7D.png" alt="EZBv7D.png"></p><p>当你把它最小化时，可以得到决策边界。</p><p><img src="https://s2.ax1x.com/2019/04/25/EZDphd.png" alt="EZDphd.png"></p><p>其中黑线为SVM的决策边界，比起粉红线和绿线，显然黑线具有更好的鲁棒性（robustness，又叫<strong>健壮性</strong>，在计算机中代表运行过程中处理错误，或是算法、输入异常时仍然正确运行的能力），距离正负样本的最小距离最大。也因此我们把SVM称为最大间距分类。</p><p>如果把C设置的非常大时，可能会出现下面的情况</p><p><img src="https://s2.ax1x.com/2019/04/25/EZDK9s.png" alt="EZDK9s.png"></p><p>由于左下角出现了一个异常点，决策边界会从黑线变为粉红线。相当于$\lambda$过小导致过拟合。如果把C值设置的小一点，则会忽略异常点的影响。因此对C的讨论，其实也是过拟合和欠拟合的问题。</p><p><img src="https://s2.ax1x.com/2019/04/25/EZDcUe.png" alt="EZDcUe.png"></p><h3 id="大间隔分类器数学原理"><a href="#大间隔分类器数学原理" class="headerlink" title="大间隔分类器数学原理"></a>大间隔分类器数学原理</h3><p>本节解释了为什么SVM能够大间隔分类</p><ul><li>向量内积</li></ul><p><img src="https://s2.ax1x.com/2019/04/25/EZfmL9.md.png" alt="EZfmL9.md.png"></p><p>简单来说，就是内积等于$u^{T}v$，也等于$p\lVert u \rVert$，其中p为向量v对向量u的投影，$\lVert u \rVert$为范数（norm，意思是具有“长度”概念的函数，这里可以简单理解为向量的长度）。内积具有正负，当向量夹角大于90度，内积为负；小于90度，内积为正。</p><p>那么回到优化问题上</p><p><img src="https://s2.ax1x.com/2019/04/25/EZ5mLQ.md.png" alt="EZ5mLQ.md.png"></p><p>我们假设n=2，$\theta_{0}=0$，然后通过之前提到的内容，可以改写优化问题：$\min _{\theta} \frac{1}{2} \sum_{j=1}^{n} \theta_{j}^{2}=\frac{1}{2}|\theta|^{2}$</p><p>也可以改写约束条件，即把$\theta^{T} x^{(i)}$看成是向量相乘，最终能改写为下面的式子：</p><p><img src="https://s2.ax1x.com/2019/04/25/EZ5Tl8.png" alt="EZ5Tl8.png"></p><p>那么为什么SVM不会选择下图中的决策边界呢？</p><p><img src="https://s2.ax1x.com/2019/04/25/EZIJht.png" alt="EZIJht.png"></p><p>其原因是，由下图可以看出，位于一、四象限的样本$x^{(1)}$，因为和决策边界接近，当它投影到$\theta$上时得到的$p^{(1)}$非常小，因此如果要满足$p^{(i)} \cdot|\theta| \geq 1$，那么$|\theta|$就必须非常大，这跟我们的优化问题$\min _{\theta} \frac{1}{2} \sum_{j=1}^{n} \theta_{j}^{2}=\frac{1}{2}|\theta|^{2}$矛盾。</p><p>位于二、三象限样本同理。</p><p><img src="https://s2.ax1x.com/2019/04/25/EZIB7j.png" alt="EZIB7j.png"></p><p>正确的样本边界如下图</p><p><img src="https://s2.ax1x.com/2019/04/25/EeKFDx.png" alt="EeKFDx.png"></p><p>额外说明一下，这里的$\theta_{0}=0$意味着决策边界一定会经过原点，如果令其不等于0，结论也是同样成立的。</p><h3 id="核函数1"><a href="#核函数1" class="headerlink" title="核函数1"></a>核函数1</h3><p>本质上说，核函数跟SVM没有必然联系，但是在用于求分线性分类器时具有很好的效果。</p><p>考虑下图中的数据集：</p><p><img src="https://s2.ax1x.com/2019/04/25/EeYNJP.md.png" alt="EeYNJP.md.png"></p><p>这种非线性的情况，显然用原始输入的两个特征是无法表示的，因此我们需要增加多项式特征。</p><p><img src="https://s2.ax1x.com/2019/04/25/EecHzQ.png" alt="EecHzQ.png"></p><p>这里我们可以将特征重新编号为$f_{i}$：</p><p><img src="https://s2.ax1x.com/2019/04/25/EecLss.md.png" alt="EecLss.md.png"></p><p>但问题在于，如何去选取新的特征呢？之前在逻辑回归时，我们是列举出了多个不同组合方式，通过交叉验证集进行验证，挑选出最合适的。而在SVM中，我们可以通过核函数对原始输入特征进行映射进而得到新的特征。</p><p>举例说明：</p><p>假设此时原始输入x有两个特征$x_{1}$和$x_{2}$（不考虑$x_{0}$），需要得到新的特征$f_{1}$、$f_{2}$、$f_{3}$。</p><p>首先我们选取了3个点$l_{1}$、$l_{2}$、$l_{3}$，称为标记（<strong>landmark</strong>）。</p><p><img src="https://s2.ax1x.com/2019/04/25/Eegrmn.md.png" alt="Eegrmn.md.png"></p><p>从图中可以看出，$f_{i}$为x和$l^{(i)}$相似度，而这个求相似度的函数就被称为核函数（<strong>kernel</strong>），这里用到的是高斯核函数（<strong>Gaussion Kernels</strong>），${ {f}_{1} }=similarity(x,{ {l}^{ (1) } } )=e(-\frac{ { {\left| x-{ {l}^{(1)} } \right|}^{2} } }{2{ {\sigma }^{2} } } )$。</p><p>那么右边的式子究竟是什么含义？</p><p><img src="https://s2.ax1x.com/2019/04/25/Ee2KA0.md.png" alt="Ee2KA0.md.png"></p><p>图中可以很明显的看出，$f_{i}$的取值在0~1之间。</p><p>绘制出核函数的图像如下</p><p><img src="https://s2.ax1x.com/2019/04/25/Ee2f4f.md.png" alt="Ee2f4f.md.png"></p><p>图中水平面坐标代表$x_{1}$和$x_{2}$，垂直的坐标代表$f$，可以看出，只有当x和$l$重合时，$f$才具有最大值。</p><p>同时也可以看出$\sigma$对$f$改变速率的影响。$\sigma$较小时，中间凸起的部分较窄，当x远离$l$时，$f$下降的较快；当$\sigma$较大时则刚好相反。</p><p>得到新的特征值$f$之后，就可以写出假设函数$h(\theta)=\theta_{0}+\theta_{1} f_{1}+\theta_{2} f_{2}+\theta_{3} f_{3}$，进而在图像上画出决策边界如下：</p><p><img src="https://s2.ax1x.com/2019/04/25/EeWpJf.png" alt="EeWpJf.png"></p><p>可以看出，当样本位于粉红色点的位置时，x靠近$l^{(1)}$，y=1；位于绿色点时，靠近$l^{(2)}$，y=1；位于蓝色点时，靠近$l^{(3)}$，y=0。</p><h3 id="核函数2"><a href="#核函数2" class="headerlink" title="核函数2"></a>核函数2</h3><p>在上一节中，我们提到一个重要问题，那就是如何选取标记？</p><p>通常根据训练集的样本数量来选择对应地标，即如果训练集中有m个样本，就选取m个标记。并且令：$l^{(1)}=x^{(1)}, l^{(2)}=x^{(2)}, \ldots, l^{(m)}=x^{(m)}$，这样做的好处在于我们得到的新标记是基于原有特征与其他样本特征的距离。</p><p><img src="https://s2.ax1x.com/2019/04/26/EmE7Q0.md.png" alt="EmE7Q0.md.png"></p><p>由上图我们可以得出结论，对于一个样本$x^{(i)}$，我们可以将它的特征映射为$f^{(i)}$，从维度上看，我们也将原始输入特征从n+1维，映射为m+1维（+1为偏置项，m为标记个数也是样本个数）</p><p><img src="https://s2.ax1x.com/2019/04/26/EmExY9.png" alt="EmExY9.png"></p><p>下面将核函数运用到SVM中，首先我们将原始输入特征映射为新特征$f$，然后根据代价函数求出最优化的$\theta$</p><p>$min C\sum\limits_{i=1}^{m}{ [ { {y}^{ (i)} }cos { {t}_{1} } }( { {\theta }^{T} }{ {f}^{(i)} })+(1-{ {y}^{(i)} })cos { {t}_{0} }( { {\theta }^{T} }{ {f}^{(i)} })]+\frac{1}{2}\sum\limits_{j=1}^{m}{\theta _{j}^{2} }$</p><p>注意这里的最后的正则化项，$j$是从1-m而不是1-n，因为我们新特征$f \in \mathbb{R}^{m+1}$。而在实际运用中，我们还得对这一项的计算做一些修改，通常我们对正则化项的计算是用矩阵相乘$\sum_{j=1}^{n=m} \theta_{j}^{2}=\theta^{T} \theta$，但在SVM中，我们用$θ^TMθ$代替$θ^Tθ$，其中M根据我们选择的核函数来确定，这样可以简化运算。</p><p>最后再代入新的假设函数中。</p><p>使用SVM时还需要选择几个参数：</p><p><img src="https://s2.ax1x.com/2019/04/26/Emn4xA.md.png" alt="Emn4xA.md.png"></p><h3 id="使用SVM"><a href="#使用SVM" class="headerlink" title="使用SVM"></a>使用SVM</h3><p>使用成熟的软件包来计算参数</p><p><img src="https://s2.ax1x.com/2019/04/26/EmMZHx.md.png" alt="EmMZHx.md.png"></p><p>不过还是有些问题需要你自己解决</p><p><img src="https://s2.ax1x.com/2019/04/26/EmMQ8e.png" alt="EmMQ8e.png"></p><p>关于内核函数的选择</p><p>1.可以使用线性内核函数，即不用内核函数，这种情况适用于n较大，m较小。因为如果样本个数少，特征多，去拟合复杂的非线性函数，容易导致过拟合。</p><p><img src="https://s2.ax1x.com/2019/04/26/EmMOPO.md.png" alt="EmMOPO.md.png"></p><p>2.也可以使用核函数（高斯核函数），适用于n较小，m较大。</p><p><img src="https://s2.ax1x.com/2019/04/26/Em1gat.md.png" alt="Em1gat.md.png"></p><p>以一个高斯核函数为例，输入两个特征向量，输出一个实数<img src="https://s2.ax1x.com/2019/04/26/Em3gOJ.png" alt="Em3gOJ.png"></p><p>注意在使用高斯核函数时，一定要对原始输入特征进行缩放：</p><p><img src="https://s2.ax1x.com/2019/04/26/Em35Y6.md.png" alt="Em35Y6.md.png"></p><p>否则会如图中那样，不同范围的特征在生成新特征时会产生不同的影响。</p><p>除此之外还可以选择其他的核函数，但是都必须满足莫塞尔定理，不过除了线性核函数和高斯核函数，其他的一般也不太常用。</p><p><img src="https://s2.ax1x.com/2019/04/26/Em8ehV.md.png" alt="Em8ehV.md.png"></p><h4 id="多类分类问题"><a href="#多类分类问题" class="headerlink" title="多类分类问题"></a>多类分类问题</h4><p><img src="https://s2.ax1x.com/2019/04/26/EmJnL4.png" alt="EmJnL4.png"></p><p>解决多分类问题的方法</p><p>1.可以使用封装好的模块</p><p>2.和之前逻辑回归中多分类问题类似，也可以使用把多分类问题转换成多个二分类问题，最终对新样本进行预测时，只要看它属于哪个正类的假设函数值最大，就属于哪个类别。</p><h4 id="关于逻辑回归、核函数SVM、不含核函数SVM的选择"><a href="#关于逻辑回归、核函数SVM、不含核函数SVM的选择" class="headerlink" title="关于逻辑回归、核函数SVM、不含核函数SVM的选择"></a>关于逻辑回归、核函数SVM、不含核函数SVM的选择</h4><p><img src="https://s2.ax1x.com/2019/04/26/EmJH6U.md.png" alt="EmJH6U.md.png"></p><p>SVM的效果可能不如神经网络，但训练速度快，且是一个凸优化问题（即一定能得到全局最优解）。</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;支持向量机&quot;&gt;&lt;a href=&quot;#支持向量机&quot; class=&quot;headerlink&quot; title=&quot;支持向量机&quot;&gt;&lt;/a&gt;支持向量机&lt;/h2&gt;&lt;h3 id=&quot;优化目标&quot;&gt;&lt;a href=&quot;#优化目标&quot; class=&quot;headerlink&quot; title=&quot;优化目标&quot;&gt;&lt;/a&gt;优化目标&lt;/h3&gt;&lt;p&gt;在监督学习中，重要的不是你选择的算法，而是应用这些算法时，选择的特征、正则化参数等等诸如此类。其中有一种非常强大的分类工具，称为支持向量机。（&lt;strong&gt;Support Vector Machine，SVM&lt;/strong&gt;）&lt;/p&gt;
&lt;p&gt;下面展示如何通过修改逻辑回归，来得到SVM。&lt;/p&gt;
&lt;p&gt;首先回顾一下逻辑回归假设函数，注意&lt;code&gt;y=1&lt;/code&gt;和&lt;code&gt;y=0&lt;/code&gt;的情况。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://s2.ax1x.com/2019/04/24/EVoj9P.md.png&quot; alt=&quot;EVoj9P.md.png&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="Machine Learning" scheme="http://yoursite.com/tags/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>机器学习笔记|第六周</title>
    <link href="http://yoursite.com/2019/04/24/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E7%AC%AC%E5%85%AD%E5%91%A8/"/>
    <id>http://yoursite.com/2019/04/24/机器学习笔记-第六周/</id>
    <published>2019-04-24T07:37:37.000Z</published>
    <updated>2019-05-02T10:21:39.332Z</updated>
    
    <content type="html"><![CDATA[<h2 id="运用机器学习建议"><a href="#运用机器学习建议" class="headerlink" title="运用机器学习建议"></a>运用机器学习建议</h2><h3 id="决定下一步做什么？"><a href="#决定下一步做什么？" class="headerlink" title="决定下一步做什么？"></a>决定下一步做什么？</h3><p>当你的模型运用于新的样本时，如果产生巨大的误差该怎么办？</p><p>一般来说，有以下几种处理方式：</p><ul><li>获得更多的训练样本——通常是有效的，但代价较大，下面的方法也可能有效，可考虑先采用下面的几种方法。</li><li>尝试减少特征的数量</li><li>尝试获得更多的特征</li><li>尝试增加多项式特征</li><li>尝试减少正则化程度$\lambda$</li><li>尝试增加正则化程度$\lambda$</li></ul><p>当然我们不可能随机去一个个方法尝试，所以需要一点手段来预测。</p><a id="more"></a><h3 id="评估假设函数"><a href="#评估假设函数" class="headerlink" title="评估假设函数"></a>评估假设函数</h3><p>首先，如果你计算出的误差（代价函数值）非常大，那么选取的假设函数就可能存在问题；即便误差小，也有可能引起过拟合。</p><p>那么如何来判断过拟合问题？</p><p>可以将数据集按比例划分成，训练集（Training set）：测试集（Test set）=70：30。</p><p><img src="https://s2.ax1x.com/2019/04/13/AqXOCq.md.png" alt="AqXOCq.md.png"></p><p>然后将训练集得到的模型运用于测试集，用来计算误差：</p><p>1.对于线性回归模型，我们直接计算<img src="https://s2.ax1x.com/2019/04/13/ALnxt1.png" alt="ALnxt1.png"></p><p>2.对于逻辑回归模型，除了计算代价函数<img src="https://s2.ax1x.com/2019/04/13/AL18fA.png" alt="AL18fA.png"></p><p>更常见的方法是计算误分类的比例（0/1错误分类度量）<img src="https://s2.ax1x.com/2019/04/13/AqjGxf.png" alt="AqjGxf.png"></p><h3 id="模型选择和交叉验证集"><a href="#模型选择和交叉验证集" class="headerlink" title="模型选择和交叉验证集"></a>模型选择和交叉验证集</h3><p>当模型确定的时候可以使用上一节的方法来验证，但是如何确定一个模型呢？首先需要知道，正则化惩罚项系数$\lambda$的选择；增加多项式特征时，多项式的次数等这类问题，称为模型选择问题。</p><p>假设我们的输入特征只有一个时，拟合效果是非常不理想的。因此，我们通常会增加特征项，那么问题又来了，多项式特征的次数应该怎么选取，即到底选取什么样的模型（假设）？</p><p>我们可以罗列出多种情况</p><p><img src="https://s2.ax1x.com/2019/04/13/AqvWX8.md.png" alt="AqvWX8.md.png"></p><p>这时再将数据集划分成训练集和测试集，对这些假设分别在训练集上训练，通过最小化训练集的代价，求出最优参数$\Theta_{1}$~$\Theta_{10}$。将其代入测试集，计算每个模型的误差，选择误差最小的那组作为假设，并把这组的误差值作为泛化误差。</p><p>然而其中存在一个问题：通过测试集来选取模型，又用测试集来求泛化误差，显然是不是坠吼滴。</p><p>因此，我们重新划分数据集的比例，训练集：<strong>交叉验证集(Cross Validation set)</strong>：测试集=60:20:20</p><p><img src="https://s2.ax1x.com/2019/04/13/Aqzdde.md.png" alt="Aqzdde.md.png"></p><p>然后计算选择出模型：</p><p>1.用训练集训练出$\Theta_{1}$~$\Theta_{10}$;</p><p>2.用交叉验证集计算出最小误差，选择误差最小的模型;</p><p>3.用第2步中选择的模型计算测试集得出泛化误差。</p><p><strong><em>Train/validation/test error</em></strong></p><p><strong>Training error:</strong></p><p>​            <img src="https://s2.ax1x.com/2019/04/24/EVNxVs.png" alt="EVNxVs.png"></p><p><strong>Cross Validation error:</strong></p><p>​            <img src="https://s2.ax1x.com/2019/04/24/EVUS5q.png" alt="EVUS5q.png"></p><p><strong>Test error:</strong></p><p>​            <img src="https://s2.ax1x.com/2019/04/24/EVUCGV.png" alt="EVUCGV.png"></p><blockquote><p>上面说到的，是关于如何改变特征来减小误差，而接下来的内容则和正则化$\lambda$有关</p></blockquote><h3 id="诊断偏差和方差"><a href="#诊断偏差和方差" class="headerlink" title="诊断偏差和方差"></a>诊断偏差和方差</h3><p>当运行结果不理想时，多半有两种情况：<strong>过拟合</strong>或者<strong>欠拟合</strong>。而这两种情况，哪种和高偏差有关？哪种和高方差有关？还是都有关系？</p><p><img src="https://s2.ax1x.com/2019/04/13/ALkcV0.md.png" alt="ALkcV0.md.png"></p><p>从图中可以看出，欠拟合时高偏差，过拟合时高方差。那么在没法画图的情况下（基本都是这种情况）如何来确定是高偏差还是高方差呢？</p><p>方法如下：</p><p><img src="https://s2.ax1x.com/2019/04/13/ALAvkV.png" alt="ALAvkV.png"></p><p>将训练集误差和验证集误差绘制在图中，其中横坐标为多项式次数。</p><p><strong>Training error:</strong>                               <img src="https://s2.ax1x.com/2019/04/24/EVUP2T.png" alt="EVUP2T.png"></p><p><strong>Cross Validation error:</strong>                <img src="https://s2.ax1x.com/2019/04/24/EVUkMF.png" alt="EVUkMF.png"></p><p>可以明显看出</p><ul><li>对于训练集，d越大，误差越小</li><li>对于验证集，随着d增长，误差会先减小后增大，其中的最低点就是开始过拟合的情况。</li></ul><p>那么当我们在没有图像的情况下，得出验证集误差较大时，只需要根据训练集误差的大小就能得出是高偏差还是高方差了。</p><p><img src="https://s2.ax1x.com/2019/04/13/ALE10I.md.png" alt="ALE10I.md.png"></p><p>具体来说，就是：</p><ul><li>训练集误差和交叉验证集误差近似时：偏差/欠拟合</li><li>交叉验证集误差远大于训练集误差时：方差/过拟合</li></ul><h3 id="正则化和偏差、方差"><a href="#正则化和偏差、方差" class="headerlink" title="正则化和偏差、方差"></a>正则化和偏差、方差</h3><p>正则化常常用来解决过拟合问题，但是对于正则化项参数$\lambda$，如下图所示不同的选取可能会导致不同的偏差问题，那么应该如何选择合适的值呢？</p><p><img src="https://s2.ax1x.com/2019/04/13/AL8954.md.png" alt="AL8954.md.png"></p><p>我们先选择一系列想要测试的$\lambda$，通常是0~12之间呈现2倍的关系，例如：$0,0.01,0.02,0.04,0.08,0.15,0.32,0.64,1.28,2.56,5.12,10$（共12个）。</p><p>再像之前那样，把数据集划分为训练集、验证集和测试集。但是因为之前我们都是不带正则化，所以$J(\theta)$和$J_{train}(\theta)$是一样的，但是这里就要如下图中的那样分开。</p><p><img src="https://s2.ax1x.com/2019/04/13/AL8MPH.md.png" alt="AL8MPH.md.png"></p><p>先在训练集上用最小化代价函数，求得最优的一组参数。然后再用该参数在训练/验证/测试集上根据公式计算相应误差。</p><p>（这里存疑？关于$J(\theta)$和$J_{train}(\theta)$什么时候使用）</p><p><img src="https://s2.ax1x.com/2019/04/13/ALJn9H.md.png" alt="ALJn9H.md.png"></p><p>具体步骤如下：</p><p>1.先对每个$\lambda$值，求对应最小化代价函数的参数</p><p><img src="https://s2.ax1x.com/2019/04/13/ALJ0uq.png" alt="ALJ0uq.png"></p><p>2.将上述求得的参数，代入交叉验证集计算代价函数$J_{cv}(\theta)$，选择对应最小代价函数的$\theta$作为最优模型参数。</p><p>3.代入测试集$J_{test}(\theta)$计算泛化误差。</p><p>还可以把验证集误差和训练集误差绘制成图，横坐标为$\lambda$。</p><p><img src="https://s2.ax1x.com/2019/04/13/ALw9I0.md.png" alt="ALw9I0.md.png"></p><p>可以看出：</p><ul><li>当$\lambda$较小时，$J_{train}(\theta)$值较小，$J_{cv}(\theta)$值较大，过拟合</li><li>当$\lambda$较大时，$J_{train}(\theta)$值较大，$J_{cv}(\theta)$值较大，欠拟合</li></ul><h3 id="学习曲线"><a href="#学习曲线" class="headerlink" title="学习曲线"></a>学习曲线</h3><p>学习曲线是一个非常好的工具，用于判断模型偏差、方差问题。它是一个以<strong>样本个数</strong>为横坐标，以<strong>误差</strong>为纵坐标绘制的图像。</p><h4 id="1-如何利用学习曲线识别高偏差？"><a href="#1-如何利用学习曲线识别高偏差？" class="headerlink" title="1.如何利用学习曲线识别高偏差？"></a>1.如何利用学习曲线识别高偏差？</h4><p><img src="https://s2.ax1x.com/2019/04/13/AL2J9s.md.png" alt="AL2J9s.md.png"></p><p>从图中可以看出，对于欠拟合而言，增加样本个数没有意义，因为模型拟合能力差，没有能力去关注”细节“。</p><p><img src="https://s2.ax1x.com/2019/04/13/AL2wHU.png" alt="AL2wHU.png"></p><p>从这张图中也可以看出，当样本到达一定数量时，训练集合测试集的误差会非常接近且不再变化，但高于我们期望的误差。</p><h4 id="如何利用学习曲线识别高方差？"><a href="#如何利用学习曲线识别高方差？" class="headerlink" title="如何利用学习曲线识别高方差？"></a>如何利用学习曲线识别高方差？</h4><p><img src="https://s2.ax1x.com/2019/04/13/ALfbvR.md.png" alt="ALfbvR.md.png"></p><p>可以看出在过拟合情况下，增加数据，可能可以提高算法效果。</p><p><img src="https://s2.ax1x.com/2019/04/13/ALh3q0.png" alt="ALh3q0.png"></p><p>增加样本个数会小幅度提高训练集误差，但是始终维持在一个相对较低的水平。而验证集个数增加为增进模型对数据的了解，因此会验证集误差会减小</p><blockquote><p>上面的test error和cross validation error用哪个效果都一样。</p></blockquote><h4 id="决定下一步做什么？-1"><a href="#决定下一步做什么？-1" class="headerlink" title="决定下一步做什么？"></a>决定下一步做什么？</h4><p>如何通过这些诊断法来帮助我们选择改进模型的方法呢？回到最初的问题上</p><ol><li>获得更多的训练样本——解决高方差</li><li>尝试减少特征的数量——解决高方差</li><li>尝试获得更多的特征——解决高偏差</li><li>尝试增加多项式特征——解决高偏差</li><li>尝试减少正则化程度λ——解决高偏差</li><li>尝试增加正则化程度λ——解决高方差</li></ol><p><strong>神经网络</strong></p><p><img src="https://s2.ax1x.com/2019/04/14/AOn2b4.md.png" alt="AOn2b4.md.png"></p><p>使用神经元较少的神经网络（左图）跟参数较少时情况类似，容易导致高偏差和欠拟合；同理，神经元较多（右图）这容易导致高方差，可以使用正则化手段来解决。</p><p>一般使用神经元较多的情况比较好处理。当然你也可以同样把数据集划分为训练集、交叉验证集和测试集。然后从一层隐藏层开始逐一尝试。找到验证集误差最小的作为模型。</p><h2 id="机器学习系统设计"><a href="#机器学习系统设计" class="headerlink" title="机器学习系统设计"></a>机器学习系统设计</h2><h3 id="确定执行的优先级"><a href="#确定执行的优先级" class="headerlink" title="确定执行的优先级"></a>确定执行的优先级</h3><p>以建立一个垃圾邮件分类器为例</p><h5 id="step1-用向量表示邮件"><a href="#step1-用向量表示邮件" class="headerlink" title="step1.用向量表示邮件"></a>step1.用向量表示邮件</h5><p>输入变量x为邮件的特征；y表示邮件的标签，1代表垃圾邮件，0代表不是。</p><p>我们可以人工选择100个单词作为词典，然后比对邮件中单词是否出现，出现则记为1，未出现记为0（注意不是记录出现个数），因此能够得到下图中的100维向量，这个向量作为输入。</p><p><img src="https://s2.ax1x.com/2019/04/24/EV3bgH.md.png" alt="EV3bgH.md.png"></p><blockquote><p>实际上，我们不会人工选择单词构成字典，而是在训练集中自动选择出现频率最高的n(10000-50000)个单词构成字典,然后用一个n维的特征向量来表示邮件。</p></blockquote><h5 id="step2-想办法降低分类的错误率"><a href="#step2-想办法降低分类的错误率" class="headerlink" title="step2.想办法降低分类的错误率"></a>step2.想办法降低分类的错误率</h5><p>1.收集更多的训练数据，但这种要视情况而定。</p><p>2.为每个邮件设计更复杂的特征，比如把邮件正文标题也考虑进去</p><p>3.为邮件的正文设计更复杂的特征，比如单词的单复数，discount和discounts是否应该看成一个单词；首字母大小、后缀，deal和Dealer是否应该看作一个单词；是否应该考虑标点符号，可能垃圾邮件中叹号会比较多。</p><p>4.构建更复杂的算法来检测邮件中的错误拼写，比如垃圾邮件发送者经常把一些容易被检测到的单词写错，如m0rtgage，med1cine，w4tches等，从而避免被检测到。</p><p>当然上述方法如何选择，同样也是个问题。</p><h3 id="误差分析"><a href="#误差分析" class="headerlink" title="误差分析"></a>误差分析</h3><p>当你准备好构建一个机器学习系统时，最好的方式是先用简单的方法快速实现。具体来说：</p><p>1.从一个简单的能快速实现的算法开始，实现该算法并用交叉验证集数据测试。</p><p>2.绘制学习曲线，判断是高偏差或是高方差，决定是增加更多的数据还是添加更多的特征。</p><p>3.<strong>误差分析：</strong>人工观察交叉验证集，看看被分错的样本是否具有某些规律。</p><ul><li>举个误差分析的例子：</li></ul><p>假设验证集有500个样本，$m_{cv}=500$，分类器分错了100个样本，此时可以人工检查这100个错误：首先分析这100个样本的错分类型和数量。如下所示：</p><p><img src="https://s2.ax1x.com/2019/04/24/EVYC8O.png" alt="EVYC8O.png"></p><p>卖药的邮件12封，卖假货的邮件4封，钓鱼的邮件53封，其他的31封；那么我们就可以把关注点放在钓鱼邮件上。</p><p>然会继续对钓鱼邮件分析，是否可以发现一些新的特征。能够提高分类器的的性能，比如这些钓鱼邮件存在以下几个问题：</p><p><img src="https://s2.ax1x.com/2019/04/24/EVYArd.png" alt="EVYArd.png"></p><p>我们又可以把关注点放在“不常用的标点”这一项上，毕竟有32封邮件都存在该问题，进而设计一些包含标点的更复杂的特征，以此提高性能。</p><ul><li>数值评估：</li></ul><p>在做垃圾邮件分类时，可能会遇到这种问题：</p><p>应不应该把discount,discounts,discounted,discounting看作是同一个单词,即是否看成一个特征。</p><p>在自然语言处理中会使用词干提取，在这种情况下，上述单词会被看作是同一个。但是词干提取有利有弊，这种方法通常指关注前几个字母，比如它可能会把universe/university看作一个单词，这显然不合理。</p><p>那么我们到底要不要使用词干提取？这时我们可以使用数值评估的方法：</p><p>首先使用词干提取，训练一个分类器，在验证集上计算它的错误率；然后不使用词干提取，训练一个分类器，在验证集上计算它的错误率。二者进行比较，哪个错误率低，就使用哪种方法。</p><p>类似的这种问题，还包括是否应该区分单词大小写，如mom和Mom是否应该看作一个单词，都可以使用上述数值评估的方法，选择一个合理的做法。</p><h3 id="不对称分类的误差评估"><a href="#不对称分类的误差评估" class="headerlink" title="不对称分类的误差评估"></a>不对称分类的误差评估</h3><p>我们通常用错误率/正确率来评估一个算法，但有时这种方法是不合适的，比如偏斜类问题。</p><p><img src="https://s2.ax1x.com/2019/04/24/EEHJrF.md.png" alt="EEHJrF.md.png"></p><p>以这个判断肿瘤恶性/良性的分类器为例，如果分类器错误率为1%，而实际只有0.5%的病人肿瘤为恶性。这种情况下，尽管错误率看起来非常低，也可能造成严重后果。此时哪怕直接把所有病人都认为良性，也只有0.5%的错误率，高于分类器。</p><p>这就是<strong>偏斜类</strong>问题，情况表现为训练集中同一种类样本非常多，而其他类样本样本比较少。</p><blockquote><p>那么如何解决偏斜类问题？换句话说，如何知道算法把所有病人都认为良性而没有做出真正的分类？</p></blockquote><p>可以使用查准率(<strong>Precision</strong>)和召回率(<strong>Recall</strong>)。</p><blockquote><p>召回率又可以叫做<strong>查全率</strong>，事实上叫查全率显然更符合其含义</p></blockquote><p>我的理解，查准率指预测正确的样本个数占样本总数的多少；查全率指预测正确的样本个数占实际正确的样本个数。</p><p>还是以上面肿瘤的例子</p><p><img src="https://s2.ax1x.com/2019/04/24/EELpE4.png" alt="EELpE4.png"></p><p>准确率等于True positive除以一行；</p><p>召回率等于True positive除以一列；</p><p>由此可以看出，当使用<code>y=0</code>（把所有病人判断为良性）的方法预测时，召回率为0，由此可以排除。</p><h3 id="查准率和召回率的均衡"><a href="#查准率和召回率的均衡" class="headerlink" title="查准率和召回率的均衡"></a>查准率和召回率的均衡</h3><p>在肿瘤例子中，我们一般情况下设置分类的阈值为0.5。</p><p><img src="https://s2.ax1x.com/2019/04/24/EEzys1.png" alt="EEzys1.png"></p><ul><li>我们可能需要在非常准确的情况下，才去预测肿瘤为恶性，否则让良性肿瘤的病人去接受治疗，那肯定要被骂，因此这时候我们需要高查准率，因此可以调高阈值为0.7或0.9，可以更准确的预测出恶性肿瘤。但此时明显，召回率就被降低了</li><li>但召回率的降低可能导致更多恶性肿瘤的情况被归类为良性肿瘤，这种情况也是我们不愿见到的，因此我们也可能想调高召回率。</li></ul><p>由此可以画出查准率和召回率的图像：</p><p><img src="https://s2.ax1x.com/2019/04/24/EVPm4K.png" alt="EVPm4K.png"></p><p>综上，我们需要向办法设置合适的阈值，以达到查准率和召回率的平衡。</p><p>通常使用$F_{1}score$来权衡，计算公式为${ {F}_{1} }Score:2\frac{PR}{P+R}$，$F_{1}score$值越高，模型的性能越好。</p><h3 id="机器学习数据"><a href="#机器学习数据" class="headerlink" title="机器学习数据"></a>机器学习数据</h3><p>之前曾经说过，不要盲目的收集大量训练数据。</p><p>但是有些时候，收集大量数据会得到一个性能良好的学习算法。</p><p>即当你的算法有很多参数，数据特征充足（当一个人类专家拿到输入x时，能做出良好的判断，证明特征充足）时，更多的训练数据会带来更好的性能。</p><p><strong>参考资料</strong></p><p>[1] <a href="https://blog.csdn.net/sdu_hao/article/details/84026798#7.%E5%86%B3%E5%AE%9A%E6%8E%A5%E4%B8%8B%E6%9D%A5%E5%81%9A%E4%BB%80%E4%B9%88" target="_blank" rel="noopener">机器学习 | 吴恩达机器学习第六周学习笔记</a></p><p>[2] <a href="https://zhuanlan.zhihu.com/p/28409180" target="_blank" rel="noopener">机器学习笔记（3）—— 优化，偏差和方差，偏斜数据 - 关右的文章 - 知乎</a></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;运用机器学习建议&quot;&gt;&lt;a href=&quot;#运用机器学习建议&quot; class=&quot;headerlink&quot; title=&quot;运用机器学习建议&quot;&gt;&lt;/a&gt;运用机器学习建议&lt;/h2&gt;&lt;h3 id=&quot;决定下一步做什么？&quot;&gt;&lt;a href=&quot;#决定下一步做什么？&quot; class=&quot;headerlink&quot; title=&quot;决定下一步做什么？&quot;&gt;&lt;/a&gt;决定下一步做什么？&lt;/h3&gt;&lt;p&gt;当你的模型运用于新的样本时，如果产生巨大的误差该怎么办？&lt;/p&gt;
&lt;p&gt;一般来说，有以下几种处理方式：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;获得更多的训练样本——通常是有效的，但代价较大，下面的方法也可能有效，可考虑先采用下面的几种方法。&lt;/li&gt;
&lt;li&gt;尝试减少特征的数量&lt;/li&gt;
&lt;li&gt;尝试获得更多的特征&lt;/li&gt;
&lt;li&gt;尝试增加多项式特征&lt;/li&gt;
&lt;li&gt;尝试减少正则化程度$\lambda$&lt;/li&gt;
&lt;li&gt;尝试增加正则化程度$\lambda$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;当然我们不可能随机去一个个方法尝试，所以需要一点手段来预测。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Machine Learning" scheme="http://yoursite.com/tags/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>matplotlib坐标原点不重合</title>
    <link href="http://yoursite.com/2019/04/15/matplotlib%E5%9D%90%E6%A0%87%E5%8E%9F%E7%82%B9%E4%B8%8D%E9%87%8D%E5%90%88/"/>
    <id>http://yoursite.com/2019/04/15/matplotlib坐标原点不重合/</id>
    <published>2019-04-15T02:22:16.000Z</published>
    <updated>2019-04-24T07:37:43.663Z</updated>
    
    <content type="html"><![CDATA[<p>用matplotlib画图时会遇到原点不重合在左下角的情况</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">fig,ax=plt.subplots(figsize=(<span class="number">8</span>,<span class="number">5</span>))</span><br><span class="line">    ax.plot(range(<span class="number">1</span>,<span class="number">13</span>),error_train,label=<span class="string">"Train"</span>)</span><br><span class="line">    ax.plot(range(<span class="number">1</span>,<span class="number">13</span>),error_cv,label=<span class="string">"Cross Validation"</span>,color=<span class="string">"green"</span>)</span><br><span class="line">    ax.legend()</span><br><span class="line">    plt.xlabel(<span class="string">"Number of training examples"</span>)</span><br><span class="line">    plt.ylabel(<span class="string">'error'</span>)</span><br><span class="line">    plt.title(<span class="string">'Learning curve of linear regression'</span>)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><a id="more"></a><p><img src="https://s2.ax1x.com/2019/04/15/AXb0a9.md.png" alt="AXb0a9.md.png"></p><p>只需要添加两行代码即可</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">plt.ylim(bottom=<span class="number">0</span>)</span><br><span class="line">plt.xlim(left=<span class="number">0</span>)</span><br></pre></td></tr></table></figure><p><img src="https://s2.ax1x.com/2019/04/15/AXqoTJ.md.png" alt="AXqoTJ.md.png"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;用matplotlib画图时会遇到原点不重合在左下角的情况&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;fig,ax=plt.subplots(figsize=(&lt;span class=&quot;number&quot;&gt;8&lt;/span&gt;,&lt;span class=&quot;number&quot;&gt;5&lt;/span&gt;))&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    ax.plot(range(&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;,&lt;span class=&quot;number&quot;&gt;13&lt;/span&gt;),error_train,label=&lt;span class=&quot;string&quot;&gt;&quot;Train&quot;&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    ax.plot(range(&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;,&lt;span class=&quot;number&quot;&gt;13&lt;/span&gt;),error_cv,label=&lt;span class=&quot;string&quot;&gt;&quot;Cross Validation&quot;&lt;/span&gt;,color=&lt;span class=&quot;string&quot;&gt;&quot;green&quot;&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    ax.legend()&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    plt.xlabel(&lt;span class=&quot;string&quot;&gt;&quot;Number of training examples&quot;&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    plt.ylabel(&lt;span class=&quot;string&quot;&gt;&#39;error&#39;&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    plt.title(&lt;span class=&quot;string&quot;&gt;&#39;Learning curve of linear regression&#39;&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    plt.show()&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
    
      <category term="matplotlib" scheme="http://yoursite.com/tags/matplotlib/"/>
    
  </entry>
  
  <entry>
    <title>机器学习|吴恩达机器学习之神经网络反向传播</title>
    <link href="http://yoursite.com/2019/04/12/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/"/>
    <id>http://yoursite.com/2019/04/12/机器学习-吴恩达机器学习之神经网络反向传播/</id>
    <published>2019-04-12T09:01:04.000Z</published>
    <updated>2019-04-12T14:25:47.805Z</updated>
    
    <content type="html"><![CDATA[<h2 id="正向传播"><a href="#正向传播" class="headerlink" title="正向传播"></a>正向传播</h2><p>和<a href="https://nullblog.top/2019/04/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" target="_blank" rel="noopener">机器学习|吴恩达机器学习之神经网络</a>中的内容差不多，都是在给出$\Theta_{(1)}$和$\Theta_{(2)}$的情况下通过正向传播求个代价值或是验证一下准确率。</p><a id="more"></a><h3 id="导入数据"><a href="#导入数据" class="headerlink" title="导入数据"></a>导入数据</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_mat</span><span class="params">(path)</span>:</span></span><br><span class="line">    <span class="string">'''读取数据'''</span></span><br><span class="line">    data = loadmat(<span class="string">'ex4data1.mat'</span>)  <span class="comment"># return a dict</span></span><br><span class="line">    X = data[<span class="string">'X'</span>]</span><br><span class="line">    y = data[<span class="string">'y'</span>].flatten()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> X, y  </span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_weight</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">'''读取权重'''</span></span><br><span class="line">    weight = loadmat(<span class="string">'ex4weights.mat'</span>)</span><br><span class="line">    theta1 = weight[<span class="string">'Theta1'</span>]</span><br><span class="line">    theta2 = weight[<span class="string">'Theta2'</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> theta1, theta2</span><br></pre></td></tr></table></figure><p>这里需要对y向量做一个处理：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">expand_y</span><span class="params">(y)</span>:</span></span><br><span class="line">    result = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> y:</span><br><span class="line">        y_array = np.zeros(<span class="number">10</span>)</span><br><span class="line">        y_array[i - <span class="number">1</span>] = <span class="number">1</span></span><br><span class="line">        result.append(y_array)</span><br><span class="line">    <span class="keyword">return</span> np.array(result)</span><br></pre></td></tr></table></figure><p>原来的y是用数字表示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[<span class="number">10</span> <span class="number">10</span> <span class="number">10</span> ...  <span class="number">9</span>  <span class="number">9</span>  <span class="number">9</span>]</span><br></pre></td></tr></table></figure><p>转换为矩阵，用1的位置来表示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[[<span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> ... <span class="number">0.</span> <span class="number">0.</span> <span class="number">1.</span>]</span><br><span class="line"> [<span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> ... <span class="number">0.</span> <span class="number">0.</span> <span class="number">1.</span>]</span><br><span class="line"> [<span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> ... <span class="number">0.</span> <span class="number">0.</span> <span class="number">1.</span>]</span><br><span class="line"> ...</span><br><span class="line"> [<span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> ... <span class="number">0.</span> <span class="number">1.</span> <span class="number">0.</span>]</span><br><span class="line"> [<span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> ... <span class="number">0.</span> <span class="number">1.</span> <span class="number">0.</span>]</span><br><span class="line"> [<span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> ... <span class="number">0.</span> <span class="number">1.</span> <span class="number">0.</span>]]</span><br></pre></td></tr></table></figure><h4 id="可视化数据"><a href="#可视化数据" class="headerlink" title="可视化数据"></a>可视化数据</h4><p>随机选100张图，可视化观察一波</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_data</span><span class="params">(X)</span>:</span></span><br><span class="line">    <span class="string">'''随机画100个数字'''</span></span><br><span class="line">    index = np.random.choice(range(<span class="number">5000</span>),</span><br><span class="line">                             <span class="number">100</span>)  <span class="comment"># np.random.choice(arrange,size),返回ndarray</span></span><br><span class="line">    images = X[index]  <span class="comment"># 随机选择100个样本</span></span><br><span class="line">    fig, ax_array = plt.subplots(</span><br><span class="line">        <span class="number">10</span>, <span class="number">10</span>, sharex=<span class="keyword">True</span>, sharey=<span class="keyword">True</span>, figsize=(<span class="number">8</span>, <span class="number">8</span>))  <span class="comment"># ax_array为Axes对象</span></span><br><span class="line">    <span class="keyword">for</span> r <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">        <span class="keyword">for</span> c <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">            ax_array[r, c].matshow(</span><br><span class="line">                images[r * <span class="number">10</span> + c].reshape(<span class="number">20</span>, <span class="number">20</span>), cmap=<span class="string">'gray_r'</span></span><br><span class="line">            )  <span class="comment"># matshow() 第一个参数为要显示的矩阵（Display an array as a matrix in a new  figure window）</span></span><br><span class="line">    plt.yticks([])</span><br><span class="line">    plt.xticks([])</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><p>关于<code>plt.subplots</code>展开，参考<a href="https://nullblog.top/2019/04/07/subplots%E7%94%BB%E5%9B%BE/#more" target="_blank" rel="noopener">subplots画图</a>。</p><h3 id="计算"><a href="#计算" class="headerlink" title="计算"></a>计算</h3><p>因为反向传播和正向传播的公式，代价函数，正则化都是一样的，在后面反向传播再展开讲。</p><h4 id="正向传播-1"><a href="#正向传播-1" class="headerlink" title="正向传播"></a>正向传播</h4><p>1.公式：</p><p><img src="https://s2.ax1x.com/2019/04/11/A7xwiq.png" alt="A7xwiq.png"></p><p>2.代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">feed_forward</span><span class="params">(theta1, theta2, X)</span>:</span></span><br><span class="line">    z2 = X @ theta1.T</span><br><span class="line">    a2 = sg.sigmoid(z2)  <span class="comment">#(5000,25)</span></span><br><span class="line">    a2 = np.insert(a2, <span class="number">0</span>, <span class="number">1</span>, axis=<span class="number">1</span>)  <span class="comment">#(5000,26)</span></span><br><span class="line">    z3 = a2 @ theta2.T</span><br><span class="line">    a3 = sg.sigmoid(z3)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> z2, a2, z3, a3</span><br></pre></td></tr></table></figure><h4 id="代价函数"><a href="#代价函数" class="headerlink" title="代价函数"></a>代价函数</h4><p>1.公式：$J(\theta)=\frac{1}{m} \sum_{i=1}^{m} \sum_{k=1}^{K}\left[-y_{k}^{(i)} \log \left(\left(h_{\theta}\left(x^{(i)}\right)\right)_{k}\right)-\left(1-y_{k}^{(i)}\right) \log \left(1-\left(h_{\theta}\left(x^{(i)}\right)\right)_{k}\right)\right.$</p><p>2.代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cost</span><span class="params">(theta1, theta2, X, y)</span>:</span></span><br><span class="line">    z2, a2, z3, h = feed_forward(theta1, theta2, X)</span><br><span class="line">    <span class="comment"># 这里的y是矩阵而不是向量了</span></span><br><span class="line">    first = -y * np.log(h)</span><br><span class="line">    second = (<span class="number">1</span> - y) * np.log(<span class="number">1</span> - h)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> (np.sum(first - second)) / len(X)  <span class="comment"># 这里不能用np.mean()，否则会相差10倍</span></span><br></pre></td></tr></table></figure><h4 id="代价函数正则化"><a href="#代价函数正则化" class="headerlink" title="代价函数正则化"></a>代价函数正则化</h4><p>1.公式：</p><p>$\begin{aligned} J(\theta)=&amp; \frac{1}{m} \sum_{i=1}^{m} \sum_{k=1}^{K}\left[-y_{k}^{(i)} \log \left(\left(h_{\theta}\left(x^{(i)}\right)\right)_{k}\right)-\left(1-y_{k}^{(i)}\right) \log \left(1-\left(h_{\theta}\left(x^{(i)}\right)\right)_{k}\right)\right]+\\ &amp; \frac{\lambda}{2 m}\left[\sum_{j=1}^{25} \sum_{k=1}^{400}\left(\Theta_{j, k}^{(1)}\right)^{2}+\sum_{j=1}^{10} \sum_{k=1}^{25}\left(\Theta_{j, k}^{(2)}\right)^{2}\right] \end{aligned}$</p><p>2.代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cost_reg</span><span class="params">(theta1, theta2, X, y, lmd)</span>:</span></span><br><span class="line">    c = cost(theta1, theta2, X, y)</span><br><span class="line">    reg = (lmd / (<span class="number">2</span> * len(X))) * (</span><br><span class="line">        np.sum(theta1[:, <span class="number">1</span>:]**<span class="number">2</span>) + np.sum(theta2[:, <span class="number">1</span>:]**<span class="number">2</span>))</span><br><span class="line">    <span class="keyword">return</span> reg + c</span><br></pre></td></tr></table></figure><h4 id="主函数"><a href="#主函数" class="headerlink" title="主函数"></a>主函数</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line">    path = <span class="string">'ex4data1.mat'</span></span><br><span class="line">    X, y = load_mat(path)</span><br><span class="line">    X = np.insert(X, <span class="number">0</span>, <span class="number">1</span>, axis=<span class="number">1</span>)</span><br><span class="line">    y = expand_y(y)</span><br><span class="line">    theta1, theta2 = load_weight()</span><br><span class="line">    print(cost_reg(theta1, theta2, X, y, <span class="number">1</span>))  <span class="comment">#0.38376985909092354</span></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure><h2 id="反向传播"><a href="#反向传播" class="headerlink" title="反向传播"></a>反向传播</h2><h4 id="随机初始化"><a href="#随机初始化" class="headerlink" title="随机初始化"></a>随机初始化</h4><p>之前我们是把$\theta​$初始化为0向量，但是在神经网络中如果把$\theta_{1}​$初始化为0，那么$S_{2}​$中的激活单元都为相同值。同理，只要初始化为相同的数，那么结果都一样。</p><p>因此我们通常随机初始化，即在（-$\varepsilon$，$\varepsilon$）之间随机取值，为了保证效率，需要取值足够小，所以选择$\varepsilon=0.12$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">random_init</span><span class="params">(size)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> np.random.uniform(<span class="number">-0.12</span>, <span class="number">0.12</span>, size)</span><br></pre></td></tr></table></figure><h4 id="处理参数"><a href="#处理参数" class="headerlink" title="处理参数"></a>处理参数</h4><p>使用优化参数<code>opt.minimize()</code>，需要把参数展开。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">serialize</span><span class="params">(a, b)</span>:</span></span><br><span class="line">    <span class="string">'''展开参数'''</span></span><br><span class="line">    <span class="keyword">return</span> np.r_[a.flatten(), b.flatten()]  <span class="comment"># 按行拼接</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">deserialize</span><span class="params">(seq)</span>:</span></span><br><span class="line">    <span class="string">'''提取参数'''</span></span><br><span class="line">    <span class="keyword">return</span> seq[:<span class="number">25</span> * <span class="number">401</span>].reshape(<span class="number">25</span>, <span class="number">401</span>), seq[<span class="number">25</span> * <span class="number">401</span>:].reshape(<span class="number">10</span>, <span class="number">26</span>)</span><br></pre></td></tr></table></figure><h4 id="代价函数（带正则项）"><a href="#代价函数（带正则项）" class="headerlink" title="代价函数（带正则项）"></a>代价函数（带正则项）</h4><p>以多分类为例</p><p>1.公式：<img src="https://s2.ax1x.com/2019/04/06/AW7Z40.md.png" alt="AW7Z40.md.png"></p><p>其中</p><ul><li>L：神经网络的层数</li><li>$s_{l}​$：$l​$层中的神经元个数（不包括bias unit）</li><li>K：输出层中的神经元个数</li><li>m：样本个数</li></ul><p><img src="https://s2.ax1x.com/2019/04/06/AW7hrQ.png" alt="AW7hrQ.png"></p><p>累加项中表示从第一项累加到第k项（why？）</p><p>正则项表示神经网络中所有权重的平方和。</p><h4 id="梯度项"><a href="#梯度项" class="headerlink" title="梯度项"></a>梯度项</h4><p><img src="https://s2.ax1x.com/2019/04/08/A4jwvR.png" alt="A4jwvR.png"></p><p>一般情况下，我们只知道Input Layer和Output Layer两层的神经元个数，中间的Hidden Layer很难确定，不过对于初学者而言，都是参考别人算法里的，所以这里也直接给出了Hidden Layer的层数（1层）以及$\theta_{1}$（25，401）和$\theta_{2}$（10，26）的维度，一个神经元为一列。</p><h5 id="计算前馈-feedforward"><a href="#计算前馈-feedforward" class="headerlink" title="计算前馈(feedforward)"></a>计算前馈(feedforward)</h5><p>参数含义及传递过程如下</p><p>1.参数含义：</p><ul><li>$\Theta^{i}​$第$i​$层的参数矩阵</li><li>$z^{(l)}$第$l$层的输入</li><li>$a^{(l)}$第$l$层的输出</li></ul><p>2.传递过程：</p><ul><li>$a^{(1)}=x​$</li><li>$z^{(2)}=\Theta^{(1)} a^{(1)}​$</li><li>$a^{(2)}=g\left(z^{(2)}\right)\left(a d d \ bias \ \ a_{0}^{(2)}\right)​$</li><li>$z^{(3)}=\Theta^{(2)} a^{(2)}​$</li><li>$h=a^{(3)}=g\left(z^{(3)}\right)​$</li></ul><p>3.前馈代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">feed_forward</span><span class="params">(theta1, theta2, X)</span>:</span></span><br><span class="line">    z2 = X @ theta1.T</span><br><span class="line">    a2 = sg.sigmoid(z2)  <span class="comment">#(5000,25)</span></span><br><span class="line">    a2 = np.insert(a2, <span class="number">0</span>, <span class="number">1</span>, axis=<span class="number">1</span>)  <span class="comment">#(5000,26)</span></span><br><span class="line">    z3 = a2 @ theta2.T</span><br><span class="line">    a3 = sg.sigmoid(z3) </span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> z2, a2, z3, a3</span><br></pre></td></tr></table></figure><h5 id="计算梯度项"><a href="#计算梯度项" class="headerlink" title="计算梯度项"></a>计算梯度项</h5><p>计算梯度项，也就是代价函数的偏导数$\frac{\partial}{\partial\Theta^{(l)}_{ij}}J\left(\Theta\right)​$。通过前馈的计算我们得到了$h​$，接下来计算“误差”，这里之所以用引号，是因为误差的实质是$\delta^{(l)}=\frac{\partial J}{\partial z^{(l)}}​$</p><ul><li>$\delta^{(3)}=h-y​$ ……(1)</li><li>$\delta^{(2)}=(\Theta^{(2)})^{T}\delta^{(3)}g^{\prime}\left(z^{(2)}\right)$……(2)</li></ul><p>第一层没有误差，接下去计算每层参数矩阵的<strong>梯度值</strong>，用$\Delta^{(l)}$表示</p><ul><li>$\Delta^{(2)}=a^{(2)} \delta^{(3)}$……(3)</li><li>$\Delta^{(1)}=a^{(1)} \delta^{(2)}​$……(4)</li></ul><p>最后网络的总梯度为：</p><p>（这里并不是真的相加，而是将$\Delta^{(1)}$和$\Delta^{(2)}$合成为一个向量，方便后面计算）</p><p>$D=\frac{1}{m}\left(\Delta^{(1)}+\Delta^{(2)}\right)$</p><p>求梯度项代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradient</span><span class="params">(theta1,theta2,X,y)</span>:</span></span><br><span class="line">    z2,a2,z3,h=feed_forward(theta1,theta2,X)</span><br><span class="line">    d3=h-y <span class="comment"># (5000,10) </span></span><br><span class="line">    d2=d3@theta2[:,<span class="number">1</span>:]*sg.sigmoid_gradient(z2) <span class="comment"># (5000,25)</span></span><br><span class="line">    D2=d3.T@a2 <span class="comment"># (10,26)</span></span><br><span class="line">    D1=d2.T@X <span class="comment"># (25,401)</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 这里合并成1-D array是为了方便后面用优化函数处理</span></span><br><span class="line">    D=(<span class="number">1</span>/len(X))*serialize(D1,D2) <span class="comment">#(10285,)</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> D</span><br></pre></td></tr></table></figure><p><strong>正则化</strong></p><p>1.原理</p><p><img src="https://s2.ax1x.com/2019/04/09/AI71Yj.png" alt="AI71Yj.png"></p><p>2.代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">regularized_gradient</span><span class="params">(theta,X,y,lmd=<span class="number">1</span>)</span>:</span></span><br><span class="line">    theta1,theta2=deserialize(theta)</span><br><span class="line">    D1,D2=deserialize(gradient(theta1,theta2,X,y))</span><br><span class="line">    theta1[:,<span class="number">0</span>]=<span class="number">0</span></span><br><span class="line">    theta2[:,<span class="number">0</span>]=<span class="number">0</span></span><br><span class="line">    reg_D1=D1+(lmd/len(X))*theta1</span><br><span class="line">    reg_D2=D2+(lmd/len(X))*theta2</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> serialize(reg_D1,reg_D2)</span><br></pre></td></tr></table></figure><h5 id="推导-delta-和-Delta"><a href="#推导-delta-和-Delta" class="headerlink" title="*推导$\delta$和$\Delta$"></a>*推导$\delta$和$\Delta$</h5><p>从代价函数入手，假设我们只有一个输入样本，那么代价函数为：$J(\theta)=-y \operatorname{logh}(x)-(1-y) \log (1-h)$，所谓梯度项，就是将代价函数对参数求导，即$\frac{\partial}{\partial \Theta^{(2)}} J(\theta), \frac{\partial}{\partial \Theta^{(1)}} J(\theta)$。而由传递过程函数：</p><ul><li>$a^{(1)}=x$</li><li>$z^{(2)}=\Theta^{(1)} a^{(1)}$</li><li>$a^{(2)}=g\left(z^{(2)}\right)\left(a d d \ bias \ \ a_{0}^{(2)}\right)$</li><li>$z^{(3)}=\Theta^{(2)} a^{(2)}$</li><li>$h=a^{(3)}=g\left(z^{(3)}\right)​$</li></ul><p>我们可以使用链式求导法则，因此有$\frac{\partial J}{\partial \Theta^{(2)}}=\frac{\partial J}{\partial a^{(3)}} \frac{\partial a^{(3)}}{\partial z^{(3)}} \frac{\partial z^{(3)}}{\partial \Theta^{(2)}}=(h-y)a^{(2)}​$</p><p>其中令$\delta^{(3)}=h-y​$得到公式(1)；令$\Delta^{(2)}=\frac{\partial J}{\partial \Theta^{(2)}}​$则得到公式(3)。</p><p>接着求$\frac{\partial J}{\partial \Theta^{(1)}}​$=$\frac{\partial J}{\partial a^{(3)}} \frac{\partial a^{(3)}}{\partial z^{(3)}} \frac{\partial z^{(3)}}{\partial a^{(2)}} \frac{\partial a^{(2)}}{\partial z^{(2)}} \frac{\partial z^{(2)}}{\partial \Theta^{(1)}}​$</p><p>​            =$\delta^{(3)} \Theta^{(2)} g^{\prime}\left(Z^{(2)}\right) a^{(1)}​$</p><p>​            =$\delta^{(2)} a^{(1)}​$</p><p>同样可以看出，令$\delta^{(3)}=\frac{\partial J}{\partial a^{(3)}} \frac{\partial a^{(3)}}{\partial z^{(3)}}$ ，$\delta^{(2)}=\delta^{(3)} \Theta^{(2)} g^{\prime}\left(Z^{(2)}\right)$则得到公式(2)。令$\frac{\partial J}{\partial \Theta^{(1)}}=\Delta^{(1)}$得到公式(4)。</p><h4 id="梯度检验"><a href="#梯度检验" class="headerlink" title="梯度检验"></a>梯度检验</h4><p>目的：在反向传播的过程中，因为需要计算的参数很多，因此容易导致误差，使得最终的结果并非最优解。因此为了确定反向传播计算的梯度是否正确，需要用到梯度检验(<strong>gradient check</strong>)。</p><p>原理：通过计算$\frac{\partial}{\partial \Theta}J(\Theta)=\lim _{\varepsilon \rightarrow 0} \frac{J(\theta+\varepsilon)-J(\theta-\varepsilon)}{2 \varepsilon}$，估计出$J(\theta)$在$\theta$的值，和反向传播计算的梯度值$\Delta$进行对比。具体来说，对于某个特定的 $\theta$，我们计算出在 $\theta$-$\varepsilon $ 处和 $\theta$+$\varepsilon $ 的代价值（$\varepsilon $是一个非常小的值，通常选取 0.001），然后求两个代价的平均，用以估计在 $\theta$ 处的代价值。</p><p><img src="https://s2.ax1x.com/2019/04/09/A5HaS1.png" alt="A5HaS1.png"></p><p>具体做法：先将$\Theta_{(1)}$和$\Theta_{(2)}$展开成一个向量$\theta$，维度（10285，）。然后循环，对每个$\theta_{j}$求$\frac{\partial}{\partial \theta_{j}}J(\theta)$，以$\theta_{1}$为例，$\frac{\partial}{\partial \theta_{1}}=\frac{J\left(\theta_{1}+\varepsilon_{1}, \theta_{2}, \theta_{3} \dots \theta_{n}\right)-J\left(\theta_{1}-\varepsilon_{1}, \theta_{2}, \theta_{3} \ldots \theta_{n}\right)}{2 \varepsilon}$，注意这里虽然只有$\theta_{j}$每次都要把整个$J(\theta)$带进去，因此需要每次都复制整个$\theta$。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradient_check</span><span class="params">(theta1,theta2,X,y,e)</span>:</span></span><br><span class="line">    theta_temp=serialize(theta1,theta2) <span class="comment"># (10285,)</span></span><br><span class="line">    numeric_grad=[]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(theta_temp)):</span><br><span class="line">        plus=copy.copy(theta_temp) <span class="comment"># 复制theta</span></span><br><span class="line">        minus=copy.copy(theta_temp) <span class="comment"># 复制theta</span></span><br><span class="line">        plus[i]+=e <span class="comment"># 只改变当前的一个值</span></span><br><span class="line">        minus[i]-=e <span class="comment"># 只改变当前的一个值</span></span><br><span class="line">        </span><br><span class="line">        grad_i=(cost_reg(plus,X,y,<span class="number">1</span>)-cost_reg(minus,X,y,<span class="number">1</span>))/(e*<span class="number">2</span>)</span><br><span class="line">        numeric_grad.append(grad_i)</span><br></pre></td></tr></table></figure><p>再和$\Theta_{(1)}$和$\Theta_{(2)}$比较求得准确度。</p><p>这里简单介绍一下数值梯度(<strong>numerical gradient</strong>)和解析梯度(<strong>analytic gradient</strong>)，数值梯度的优点是编程可以直接实现，不要求函数可微，缺点是运行速度非常慢，也就是上面中<code>numeric_grad</code>，且只能求出近似解；解析梯度能求出近似解，也是我们通常使用的方法，即<code>analytic_grad</code>。</p><p>得到数值梯度和解析梯度之后，要求他们的进行相似性度量(<strong>Similarity Measurement</strong>)，这里用标准化欧氏距离（存疑？）$diff=\frac{ | \text { numeric_grad }-\text {analytic_grad}\left|_{2}\right.}{ | \text { numeric_grad }\left|_{2}+\right| \text { analytic_grad }\left|_{2}\right.}$，当距离(diff)小于<code>10e-9</code>时为计算正确。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">reg_D1,reg_D2=regularized_gradient(theta_temp,X,y)</span><br><span class="line">analytic_grad=serialize(reg_D1,reg_D2)</span><br><span class="line">   </span><br><span class="line">   diff = np.linalg.norm(numeric_grad - analytic_grad) / np.linalg.norm(numeric_grad + analytic_grad)</span><br><span class="line"></span><br><span class="line">   print(<span class="string">'If your backpropagation implementation is correct,\nthe relative difference will be smaller than 10e-9 (assume epsilon=0.0001).\nRelative Difference:  &#123;&#125;\n'</span>.format(diff))</span><br></pre></td></tr></table></figure><p><code>np.linalg.norm()</code>：linalg=linear(线性)+algebra(代数)，norm()表示范数$^{[1]}$。</p><h4 id="优化参数"><a href="#优化参数" class="headerlink" title="优化参数"></a>优化参数</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">nn_trainning</span><span class="params">(X,y)</span>:</span></span><br><span class="line"></span><br><span class="line">    init_theta = random_init(<span class="number">10285</span>) <span class="comment">#随机初始化theta1和theta2</span></span><br><span class="line">    res = opt.minimize(</span><br><span class="line">        fun=cost_reg,</span><br><span class="line">        x0=init_theta,</span><br><span class="line">        args=(X, y, <span class="number">1</span>),</span><br><span class="line">        method=<span class="string">'TNC'</span>,</span><br><span class="line">        jac=regularized_gradient,</span><br><span class="line">        options=&#123;<span class="string">'maxiter'</span>: <span class="number">400</span>&#125;)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure><h4 id="可视化隐藏层"><a href="#可视化隐藏层" class="headerlink" title="可视化隐藏层"></a>可视化隐藏层</h4><p>一种明白神经网络是如何学习的方法就是将隐藏层捕获的内容可视化，通俗来说就是输入一个x，激活这个隐藏层。在我们的这个训练样本中，$\theta_{1}$（25,401）有401个参数，去掉偏置单元，将剩下的400个参数reshape为(20,20)。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_hidden</span><span class="params">(theta)</span>:</span></span><br><span class="line">    t1,_=deserialize(theta)</span><br><span class="line">    t1=t1[:,<span class="number">1</span>:]</span><br><span class="line">    fig,ax_array=plt.subplots(<span class="number">5</span>,<span class="number">5</span>,sharex=<span class="keyword">True</span>,sharey=<span class="keyword">True</span>,figsize=(<span class="number">6</span>,<span class="number">6</span>))</span><br><span class="line">    <span class="keyword">for</span> r <span class="keyword">in</span> range(<span class="number">5</span>):</span><br><span class="line">        <span class="keyword">for</span> c <span class="keyword">in</span> range(<span class="number">5</span>):</span><br><span class="line">            ax_array[r,c].matshow(t1[r*<span class="number">5</span>+c].reshape(<span class="number">20</span>,<span class="number">20</span>),cmap=<span class="string">'gray_r'</span>)</span><br><span class="line">    plt.xticks([])</span><br><span class="line">    plt.yticks([])</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><h4 id="计算精确度"><a href="#计算精确度" class="headerlink" title="计算精确度"></a>计算精确度</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">accuracy</span><span class="params">(theta, X, y)</span>:</span></span><br><span class="line">    theta1, theta2 = deserialize(theta)</span><br><span class="line">    _, _, _, h = feed_forward(theta1, theta2, X)</span><br><span class="line">    y_pred = np.argmax(h, axis=<span class="number">1</span>) + <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    print(classification_report(y, y_pred))</span><br></pre></td></tr></table></figure><h4 id="主函数-1"><a href="#主函数-1" class="headerlink" title="主函数"></a>主函数</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line">    path = <span class="string">'ex4data1.mat'</span></span><br><span class="line">    X, raw_y = ff.load_mat(path)</span><br><span class="line">    X = np.insert(X, <span class="number">0</span>, <span class="number">1</span>, axis=<span class="number">1</span>)</span><br><span class="line">    y = ff.expand_y(raw_y)  <span class="comment"># y的一行表示一个样本</span></span><br><span class="line">    <span class="comment"># theta1, theta2 = ff.load_weight()</span></span><br><span class="line">    <span class="comment"># gradient_check(theta1,theta2,X,y,0.0001)</span></span><br><span class="line">    theta_unroll = nn_training(X, y)</span><br><span class="line">    accuracy(theta_unroll.x, X, raw_y)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p>[1] <a href="https://blog.csdn.net/hqh131360239/article/details/79061535" target="_blank" rel="noopener">np.linalg.norm(求范数)</a></p><p>[2] <a href="https://blog.csdn.net/Cowry5/article/details/80399350" target="_blank" rel="noopener">吴恩达机器学习作业Python实现(四)：神经网络(反向传播)</a></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;正向传播&quot;&gt;&lt;a href=&quot;#正向传播&quot; class=&quot;headerlink&quot; title=&quot;正向传播&quot;&gt;&lt;/a&gt;正向传播&lt;/h2&gt;&lt;p&gt;和&lt;a href=&quot;https://nullblog.top/2019/04/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;机器学习|吴恩达机器学习之神经网络&lt;/a&gt;中的内容差不多，都是在给出$\Theta_{(1)}$和$\Theta_{(2)}$的情况下通过正向传播求个代价值或是验证一下准确率。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Machine Learning" scheme="http://yoursite.com/tags/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>subplots画图</title>
    <link href="http://yoursite.com/2019/04/07/subplots%E7%94%BB%E5%9B%BE/"/>
    <id>http://yoursite.com/2019/04/07/subplots画图/</id>
    <published>2019-04-07T10:32:59.000Z</published>
    <updated>2019-04-08T02:06:46.741Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>matplotlib是python领域中很常见的绘图模块</p></blockquote><p>几乎只在需要用到时查手册，这里记录一些比较常用的函数，有机会再来系统学习一下</p><a id="more"></a><p>以一段画图代码为例</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_data</span><span class="params">(X)</span>:</span></span><br><span class="line">    <span class="string">'''随机画100个数字'''</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># np.random.choice(arrange,size),返回ndarray</span></span><br><span class="line">    index=np.random.choice(range(<span class="number">5000</span>),<span class="number">100</span>) </span><br><span class="line">    images=X[index] <span class="comment"># 随机选择100个样本</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># ax_array为Axes对象</span></span><br><span class="line">    fig,ax_array=plt.subplots(<span class="number">10</span>,<span class="number">10</span>,sharex=<span class="keyword">True</span>,sharey=<span class="keyword">True</span>,figsize=(<span class="number">8</span>,<span class="number">8</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> r <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">        <span class="keyword">for</span> c <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">            <span class="comment"># matshow() 第一个参数为要显示的矩阵</span></span><br><span class="line">            <span class="comment">#Display an array as a matrix in a new figure window</span></span><br><span class="line">            ax_array[r,c].matshow(images[r*<span class="number">10</span>+c].reshape(<span class="number">20</span>,<span class="number">20</span>),cmap=<span class="string">'gray_r'</span>)</span><br><span class="line">    plt.yticks([])</span><br><span class="line">    plt.xticks([])</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><p>1.<code>matplotlib.pyplot.subplots()</code>：创建一个figure和一组subplots</p><p>参数：</p><ul><li>nrows，ncols：<code>axes</code>的数量，这里是10*10</li><li>sharex，sharey：共享所有<code>axes</code>X轴和y轴的属性，设置<code>True</code>开启</li></ul><p>返回值：</p><ul><li>figure</li><li>ax：一个或多个<code>axes</code>对象</li></ul><p><strong>*</strong>  <code>axes</code>和<code>subplot</code>的区别：简单来说，如果把<code>figure</code>看做是电脑桌面，那么<code>axes</code>就是可自由移动的图标，<code>subplot</code>则是不可自由移动的图标。</p><p>2.<code>matplotlib.pyplot.matshow()</code>：在窗口用矩阵显示一个数组</p><p>参数：</p><ul><li>array-like(M,N)：要显示的(M,N)<strong>矩阵</strong></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;matplotlib是python领域中很常见的绘图模块&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;几乎只在需要用到时查手册，这里记录一些比较常用的函数，有机会再来系统学习一下&lt;/p&gt;
    
    </summary>
    
    
      <category term="python" scheme="http://yoursite.com/tags/python/"/>
    
      <category term="matplotlib" scheme="http://yoursite.com/tags/matplotlib/"/>
    
  </entry>
  
  <entry>
    <title>机器学习|吴恩达机器学习之神经网络</title>
    <link href="http://yoursite.com/2019/04/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    <id>http://yoursite.com/2019/04/06/机器学习-吴恩达机器学习之神经网络/</id>
    <published>2019-04-06T07:43:14.000Z</published>
    <updated>2019-04-06T08:27:28.290Z</updated>
    
    <content type="html"><![CDATA[<h2 id="多类分类问题"><a href="#多类分类问题" class="headerlink" title="多类分类问题"></a>多类分类问题</h2><p>就是把多个类别细分为多个0-1类别来分析</p><p><img src="https://s2.ax1x.com/2019/04/06/AW3v7T.png" alt="AW3v7T.png"></p><a id="more"></a><h3 id="代码："><a href="#代码：" class="headerlink" title="代码："></a>代码：</h3><h4 id="导入与处理数据"><a href="#导入与处理数据" class="headerlink" title="导入与处理数据"></a>导入与处理数据</h4><p>1.导入模块</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> scipy.io <span class="keyword">import</span> loadmat</span><br><span class="line"><span class="keyword">from</span> scipy.optimize <span class="keyword">import</span> minimize</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> Sigmoid</span><br></pre></td></tr></table></figure><p>2.导入数据</p><p>因为图像的灰度值是用<code>.mat</code>存储的，所以用<code>loadmat</code>来导入到<code>python</code>中</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 5000个训练样本，每个样本20*20的灰度值，展开为400维向量。输出为0~9的数字</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_data</span><span class="params">(path)</span>:</span></span><br><span class="line">    data=loadmat(path)</span><br><span class="line">    X=data[<span class="string">'X'</span>]</span><br><span class="line">    y=data[<span class="string">'y'</span>]</span><br><span class="line">    <span class="keyword">return</span> X,y</span><br></pre></td></tr></table></figure><p>3.可以随机显示一张图片来观察</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_an_image</span><span class="params">(X,y)</span>:</span></span><br><span class="line">    pick_one=np.random.randint(<span class="number">0</span>,<span class="number">5000</span>)</span><br><span class="line">    image=X[pick_one,:]</span><br><span class="line">    fig,ax=plt.subplots(figsize=(<span class="number">1</span>,<span class="number">1</span>))</span><br><span class="line">    ax.matshow(image.reshape(<span class="number">20</span>,<span class="number">20</span>),cmap=<span class="string">'gray_r'</span>)</span><br><span class="line">    plt.xticks([])</span><br><span class="line">    plt.yticks([])</span><br><span class="line">    plt.show()</span><br><span class="line">    print(<span class="string">'this should be &#123;&#125;'</span>.format(y[pick_one]))</span><br></pre></td></tr></table></figure><p>结果如图：</p><p><img src="https://s2.ax1x.com/2019/04/06/AWG3RJ.png" alt="AWG3RJ.png"></p><h4 id="代价函数"><a href="#代价函数" class="headerlink" title="代价函数"></a>代价函数</h4><p>正则化项逻辑回归，和之前的没区别</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cost_reg</span><span class="params">(theta,X,y,lmd)</span>:</span></span><br><span class="line">    theta_reg=theta[<span class="number">1</span>:]</span><br><span class="line">    first=y*np.log(Sigmoid.sigmoid(X@theta))+(<span class="number">1</span>-y)*np.log(<span class="number">1</span>-Sigmoid.sigmoid(X@theta))</span><br><span class="line">    reg=(lmd/(<span class="number">2</span>*len(X)))*(theta_reg@theta_reg)<span class="comment"># 惩罚项不从第一项开始</span></span><br><span class="line">    <span class="keyword">return</span> -np.mean(first)+reg</span><br></pre></td></tr></table></figure><h4 id="梯度函数"><a href="#梯度函数" class="headerlink" title="梯度函数"></a>梯度函数</h4><p>和之前的没区别，只不过这里用了<code>np.concatenate()</code>来拼接。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradient_reg</span><span class="params">(theta,X,y,lmd)</span>:</span></span><br><span class="line">    theta_reg=theta[<span class="number">1</span>:]</span><br><span class="line">    first=(<span class="number">1</span>/len(X))*(X.T@(Sigmoid.sigmoid(X@theta)-y))</span><br><span class="line">    reg=np.concatenate([np.array([<span class="number">0</span>]),(lmd/len(X))*theta_reg])</span><br><span class="line">    <span class="keyword">return</span> first+reg</span><br></pre></td></tr></table></figure><h4 id="one-vs-all"><a href="#one-vs-all" class="headerlink" title="one vs all"></a>one vs all</h4><p>每次循环都求出0~9中任一数字的$\theta$值（将该数字和其他数字二分类），最后拼接成一个矩阵。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">one_vs_all</span><span class="params">(X,y,lmd,K)</span>:</span></span><br><span class="line">    all_theta=np.zeros((K,X.shape[<span class="number">1</span>]))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,K+<span class="number">1</span>):</span><br><span class="line">        theta=np.zeros(X.shape[<span class="number">1</span>])</span><br><span class="line">        y_i=np.array([<span class="number">1</span> <span class="keyword">if</span> label==i <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">for</span> label <span class="keyword">in</span> y])</span><br><span class="line">    </span><br><span class="line">        ret=minimize(fun=cost_reg,x0=theta,args=(X,y_i,lmd),method=<span class="string">'TNC'</span>,</span><br><span class="line">                        jac=gradient_reg,options=&#123;<span class="string">'disp'</span>:<span class="keyword">True</span>&#125;)</span><br><span class="line">        </span><br><span class="line">        all_theta[i<span class="number">-1</span>,:]=ret.x</span><br><span class="line">   </span><br><span class="line">    <span class="keyword">return</span> all_theta</span><br></pre></td></tr></table></figure><h4 id="计算精确度"><a href="#计算精确度" class="headerlink" title="计算精确度"></a>计算精确度</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">计算精确度</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">返回值</span></span><br><span class="line"><span class="string">h_argmax是一个存放预测数字的一维ndarray</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict_all</span><span class="params">(X, all_theta)</span>:</span></span><br><span class="line">    h = Sigmoid.sigmoid(X @ all_theta.T)</span><br><span class="line">    <span class="comment"># 返回指定方向上的最大值的索引 axis=0:按列索引，axis=1：按行索引</span></span><br><span class="line">    h_argmax = np.argmax(h, axis=<span class="number">1</span>)</span><br><span class="line">    h_argmax = h_argmax + <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> h_argmax</span><br></pre></td></tr></table></figure><h4 id="main函数"><a href="#main函数" class="headerlink" title="main函数"></a>main函数</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line">    X, y = load_data(<span class="string">'ex3data1.mat'</span>)</span><br><span class="line">    plot_an_image(X,y)</span><br><span class="line">    X = np.insert(X, <span class="number">0</span>, <span class="number">1</span>, axis=<span class="number">1</span>)</span><br><span class="line">    y = y.flatten()</span><br><span class="line">    all_theta = one_vs_all(X, y, <span class="number">1</span>, <span class="number">10</span>)</span><br><span class="line">    y_pred = predict_all(X, all_theta)</span><br><span class="line">    accuracy = np.mean(y_pred == y)  <span class="comment"># 精确度94.46%</span></span><br></pre></td></tr></table></figure><h2 id="神经网络"><a href="#神经网络" class="headerlink" title="神经网络"></a>神经网络</h2><h3 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h3><p>模仿人类大脑的神经元：</p><p><img src="https://s2.ax1x.com/2019/04/06/AW01J0.md.png" alt="AW01J0.md.png"></p><p>进一步设计出神经网络：</p><p><img src="https://s2.ax1x.com/2019/04/06/AW0JQU.md.png" alt="AW0JQU.md.png"></p><h3 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h3><p>这个实验是神经网络的正向传播过程，不涉及如何训练。</p><h4 id="导入模块"><a href="#导入模块" class="headerlink" title="导入模块"></a>导入模块</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scipy.io <span class="keyword">import</span> loadmat</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> Sigmoid</span><br><span class="line"><span class="keyword">import</span> multiClassClassification <span class="keyword">as</span> mc</span><br></pre></td></tr></table></figure><h4 id="导入数据"><a href="#导入数据" class="headerlink" title="导入数据"></a>导入数据</h4><p>这里的权重已经给出，导入即可</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_weigth</span><span class="params">(path)</span>:</span></span><br><span class="line">    data=loadmat(path)</span><br><span class="line">    <span class="comment"># Theta1是输入层和隐藏层之间的参数；Theta2是隐藏层和输出层之间的参数</span></span><br><span class="line">    <span class="keyword">return</span> data[<span class="string">'Theta1'</span>],data[<span class="string">'Theta2'</span>]</span><br></pre></td></tr></table></figure><h4 id="main函数-1"><a href="#main函数-1" class="headerlink" title="main函数"></a>main函数</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line">    theta1, theta2 = load_weigth(<span class="string">'ex3weights.mat'</span>)</span><br><span class="line">    X, y = mc.load_data(<span class="string">'ex3data1.mat'</span>)</span><br><span class="line">    y = y.flatten()</span><br><span class="line">    X = np.insert(X, <span class="number">0</span>, values=np.ones(X.shape[<span class="number">0</span>]), axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 输入层到隐藏层</span></span><br><span class="line">    z2 = X @ theta1.T</span><br><span class="line">    z2 = np.insert(z2, <span class="number">0</span>, <span class="number">1</span>, axis=<span class="number">1</span>)</span><br><span class="line">    <span class="comment"># 隐藏层到输出层</span></span><br><span class="line">    a2 = Sigmoid.sigmoid(z2)</span><br><span class="line">    </span><br><span class="line">    z3 = a2 @ theta2.T</span><br><span class="line">    a3 = Sigmoid.sigmoid(z3)</span><br><span class="line"></span><br><span class="line">    y_pred = np.argmax(a3, axis=<span class="number">1</span>) + <span class="number">1</span></span><br><span class="line">    accurcy = np.mean(y_pred == y)  <span class="comment"># 精确度97.52%</span></span><br></pre></td></tr></table></figure><h3 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h3><p><a href="https://nullblog.top/2019/03/23/%E6%AD%A3%E5%90%91%E4%BC%A0%E6%92%AD%E7%9A%84%E5%90%91%E9%87%8F%E5%8C%96%E5%AE%9E%E7%8E%B0/#more" target="_blank" rel="noopener">正向传播的向量化实现</a></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;多类分类问题&quot;&gt;&lt;a href=&quot;#多类分类问题&quot; class=&quot;headerlink&quot; title=&quot;多类分类问题&quot;&gt;&lt;/a&gt;多类分类问题&lt;/h2&gt;&lt;p&gt;就是把多个类别细分为多个0-1类别来分析&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://s2.ax1x.com/2019/04/06/AW3v7T.png&quot; alt=&quot;AW3v7T.png&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="Machine Learning" scheme="http://yoursite.com/tags/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>机器学习|吴恩达机器学习之线性回归</title>
    <link href="http://yoursite.com/2019/04/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/"/>
    <id>http://yoursite.com/2019/04/06/机器学习-吴恩达机器学习之线性回归/</id>
    <published>2019-04-06T07:31:33.000Z</published>
    <updated>2019-04-06T07:31:33.892Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>对《机器学习》这门课程的回顾，系列文章的目的是希望能够把<strong>原理</strong>和<strong>代码</strong>实现统一起来，增进理解，所以对一些我认为简单的知识，可能会一笔带过。</p></blockquote><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>这里对第一周的内容做一些简单的回顾：</p><p><img src="https://s2.ax1x.com/2019/03/29/A0dT2V.png" alt=""></p><a id="more"></a><h2 id="线性回归-单变量-one-variable"><a href="#线性回归-单变量-one-variable" class="headerlink" title="线性回归-单变量(one variable)"></a>线性回归-单变量(one variable)</h2><h3 id="基本内容"><a href="#基本内容" class="headerlink" title="基本内容"></a>基本内容</h3><h5 id="假设函数"><a href="#假设函数" class="headerlink" title="假设函数"></a>假设函数</h5><p>（1）公式：$h_{\theta}(x)=\theta_{0}+\theta_{1}x​$</p><p>（2）原理：输入一个单变量x，输出预测值</p><h5 id="代价函数"><a href="#代价函数" class="headerlink" title="代价函数"></a>代价函数</h5><p>（1）公式：$J(\theta_{0},\theta_{1})=\frac{1}{2m}\sum_{i=1}^{m}(h_{\theta}(x_{i})-y_{i})^{2}$，又称“平方误差函数”，这里的1/2是方便后面求导时约掉平方。</p><p>（2）原理：最小二乘法判断误差。</p><h5 id="梯度下降法"><a href="#梯度下降法" class="headerlink" title="梯度下降法"></a>梯度下降法</h5><p>（1）公式：</p><p><img src="https://s2.ax1x.com/2019/03/29/A0BCHe.png" alt=""></p><p>重复直到收敛。</p><h3 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h3><p><strong>在实际代码中，数据都是用矩阵的方式来表示。</strong></p><p>1.导入几个常用库</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt  <span class="comment">#画图函数</span></span><br><span class="line"><span class="keyword">from</span> mpl_toolkits.mplot3d <span class="keyword">import</span> Axes3D</span><br><span class="line"><span class="keyword">from</span> matplotlib.colors <span class="keyword">import</span> LogNorm</span><br></pre></td></tr></table></figure><h4 id="数据处理"><a href="#数据处理" class="headerlink" title="数据处理"></a>数据处理</h4><p>1.导入<code>csv</code>文件，用<code>DataFrame</code>结构存储。</p><p>（1）关于<code>csv</code>文件：简单来说就是纯文本，以行为一条记录，每条记录被分隔符分隔。<a href="https://baike.baidu.com/item/CSV/10739" target="_blank" rel="noopener">CSV文件</a></p><p>（2）pandas中的<code>DataFrame</code>：<a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html" target="_blank" rel="noopener">官方手册</a></p><p>（3）关于<code>.describe()</code>返回值中的分位数：<a href="https://nullblog.top/2019/03/12/%E6%B5%85%E8%B0%88%E5%88%86%E4%BD%8D%E6%95%B0/" target="_blank" rel="noopener">浅谈分为数</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'''导入数据'''</span></span><br><span class="line">path = <span class="string">'Data\ex1data1.txt'</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">参数：</span></span><br><span class="line"><span class="string">path:路径</span></span><br><span class="line"><span class="string">header:列名，等于None，是因为接下去列名会显示传递</span></span><br><span class="line"><span class="string">names:需要传递的列名</span></span><br><span class="line"><span class="string">返回值：</span></span><br><span class="line"><span class="string">DataFrame形式数据结构</span></span><br><span class="line"><span class="string">'''</span> </span><br><span class="line">data = pd.read_csv(</span><br><span class="line">    path, header=<span class="keyword">None</span>,</span><br><span class="line">    names=[<span class="string">'Population'</span>,</span><br><span class="line">           <span class="string">'Profit'</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 导入数据以后可视化观察一下</span></span><br><span class="line">data.head() <span class="comment"># 查看记录，默认返回前5条</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看数据集的描述信息 </span></span><br><span class="line"><span class="comment"># count:样本个数  mean：均值  std:标准差  min:最小值  25%:四分之一位数  50%:中位数  75%:四分之# 三位数  max:最大值</span></span><br><span class="line">data.describe()</span><br></pre></td></tr></table></figure><p>2.对得到的数据进行加工，方便计算</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">从样本集中分离出X和y</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"></span><br><span class="line">data.insert(<span class="number">0</span>, <span class="string">'one'</span>, <span class="number">1</span>)<span class="comment"># 插入第1列：X0=1</span></span><br><span class="line"></span><br><span class="line">cols = data.shape[<span class="number">1</span>]  <span class="comment"># .shape返回一个元组，[0]为行数，[1]为列数</span></span><br><span class="line"><span class="comment"># 提取X，y的值</span></span><br><span class="line">X = data.iloc[:, <span class="number">0</span>:cols - <span class="number">1</span>]</span><br><span class="line">y = data.iloc[:, cols - <span class="number">1</span>:cols]</span><br></pre></td></tr></table></figure><p>初始化数据矩阵，这里转化成了<code>np.matrix</code>，但建议使用<code>np.ndarray</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">数据处理</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="comment"># 将dataframe结构转化成np的matrix</span></span><br><span class="line"><span class="comment"># 当Theta取0时计算平均误差</span></span><br><span class="line">X = np.matrix(X.values)</span><br><span class="line">y = np.matrix(y.values)</span><br><span class="line">theta = np.matrix([<span class="number">0</span>, <span class="number">0</span>])</span><br><span class="line"><span class="comment"># 初始化学习速率和迭代次数</span></span><br><span class="line">alpha = <span class="number">0.01</span></span><br><span class="line">epoch = <span class="number">1000</span></span><br></pre></td></tr></table></figure><h4 id="计算函数"><a href="#计算函数" class="headerlink" title="计算函数"></a>计算函数</h4><p>1.代价函数</p><p>公式：$J(\theta_{0},\theta_{1})=\frac{1}{2m}\sum_{i=1}^{m}(h_{\theta}(x_{i})-y_{i})^{2}$，其中主要矩阵化：假设函数$h_{\theta}(x_{i})=\theta^{T}X$和$y_{(i)}$</p><p>（1）向量化：计算过程中注意矩阵乘法或是向量乘法的<strong>合法性</strong>。</p><p>（2）数据可以用<code>np.matrix</code>存储，但更建议使用<code>np.ndarray</code></p><p>（3）<code>np.matrix</code>和<code>np.ndarray</code>在乘法上有所区别:<a href="https://nullblog.top/2019/03/19/Numpy%E4%B8%AD%E7%9A%84%E7%9F%A9%E9%98%B5%E4%B9%98%E6%B3%95/" target="_blank" rel="noopener">Numpy中的矩阵乘法</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">函数名：</span></span><br><span class="line"><span class="string">代价函数</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">参数：</span></span><br><span class="line"><span class="string">X：矩阵 </span></span><br><span class="line"><span class="string">-可用.shape查看维度(97,2)</span></span><br><span class="line"><span class="string">y：向量，用numpy.matrix的结构存储</span></span><br><span class="line"><span class="string">    -可用.shape查看维度(97,1)</span></span><br><span class="line"><span class="string">theta:向量，np.matrix的结构存储</span></span><br><span class="line"><span class="string">-维度(1,2)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">返回值：</span></span><br><span class="line"><span class="string">代价函数值</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">costFunciton</span><span class="params">(X, y, theta)</span>:</span></span><br><span class="line">    inner = np.power(((X * theta.T) - y), <span class="number">2</span>)</span><br><span class="line">    <span class="keyword">return</span> np.sum(inner) / (<span class="number">2</span> * len(X))</span><br></pre></td></tr></table></figure><p>2.批量梯度下降：</p><p>（1）公式：</p><p><img src="https://s2.ax1x.com/2019/03/29/A0BCHe.png" alt=""></p><p>（2）作用：通过迭代的方式来寻找代价函数最小时的参数（$\theta_{j}$）</p><p>（3）学习速率：如果过大，会导致无法到达代价值最小点（函数发散或震荡）；如果过小，则会使得迭代时间过长。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">函数名：</span></span><br><span class="line"><span class="string">梯度下降法</span></span><br><span class="line"><span class="string">参数：</span></span><br><span class="line"><span class="string">X:矩阵</span></span><br><span class="line"><span class="string">y:向量</span></span><br><span class="line"><span class="string">theta:向量</span></span><br><span class="line"><span class="string">alpha:学习速率</span></span><br><span class="line"><span class="string">epoch：迭代次数</span></span><br><span class="line"><span class="string">返回值：</span></span><br><span class="line"><span class="string">theta:最后得到的两个参数theta_0,theta_1</span></span><br><span class="line"><span class="string">cost:最后得到的误差</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradientDescent</span><span class="params">(X, y, theta, alpha, epoch)</span>:</span></span><br><span class="line"></span><br><span class="line">    temp = np.matrix(np.zeros(theta.shape))</span><br><span class="line"></span><br><span class="line">    cost = np.zeros(epoch)  <span class="comment"># epoch为迭代次数</span></span><br><span class="line">    m = X.shape[<span class="number">0</span>]  <span class="comment"># 样本数量</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(epoch):</span><br><span class="line">        temp = theta - (alpha / m) * (X.dot(theta.T) - y).T.dot(X)</span><br><span class="line">        theta = temp</span><br><span class="line">        <span class="comment"># 记录一下每次更新后的误差</span></span><br><span class="line">        cost[i] = costFunciton(X, y, theta)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> theta, cost</span><br></pre></td></tr></table></figure><p>2*.正规方程法(Normal Equation)</p><p>（1）同样也可以用于寻找代价函数最小时候的参数取值，与梯度下降法(Gradient Descent)比较</p><p><img src="https://s2.ax1x.com/2019/04/02/Aydhpd.md.jpg" alt="Aydhpd.md.jpg"></p><p>（2）公式：$\theta=(X^{T}X)^{-1}X^{T}y$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'''特征方程'''</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">norEquation</span><span class="params">(X,y)</span>:</span></span><br><span class="line">    theta=np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> theta</span><br></pre></td></tr></table></figure><h4 id="画图函数"><a href="#画图函数" class="headerlink" title="画图函数"></a>画图函数</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">x = np.linspace(</span><br><span class="line">    data.Population.min(), data.Population.max(),</span><br><span class="line">    <span class="number">100</span>)  <span class="comment"># 横坐标:linspace(start,end, num)从start开始到end结束，平均分成num份，返回一个数组</span></span><br><span class="line">f = final_theta[<span class="number">0</span>, <span class="number">0</span>] + (final_theta[<span class="number">0</span>, <span class="number">1</span>] * x)  <span class="comment"># 假设函数</span></span><br><span class="line"></span><br><span class="line"><span class="string">'''函数和散点图'''</span></span><br><span class="line">plt.figure(figsize=(<span class="number">8</span>, <span class="number">5</span>))</span><br><span class="line">plt.plot(x, f, <span class="string">'r'</span>, label=<span class="string">'Prediction'</span>)</span><br><span class="line">plt.scatter(data[<span class="string">'Population'</span>], data.Profit, label=<span class="string">'Training Data'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'Population'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Profit'</span>)</span><br><span class="line">plt.title(<span class="string">'Predicted Profit vs. Population Size'</span>)</span><br></pre></td></tr></table></figure><p>结果图：</p><p><img src="https://s2.ax1x.com/2019/04/02/AydTnP.md.png" alt="AydTnP.md.png"></p><p>可以看出拟合效果还可以。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">绘制代价函数与迭代次数的图像</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line">plt.figure(figsize=(<span class="number">8</span>, <span class="number">5</span>))</span><br><span class="line">plt.plot(np.arange(epoch), cost, <span class="string">'r'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'iteration'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'cost'</span>)</span><br></pre></td></tr></table></figure><p>结果图：</p><p><img src="https://s2.ax1x.com/2019/04/02/AydqAS.md.png" alt="AydqAS.md.png"></p><p>随着迭代次数的增加，代价函数值单调递减。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">绘制代价函数3D图像</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line">fig = plt.figure(figsize=(<span class="number">8</span>, <span class="number">5</span>))</span><br><span class="line">ax = Axes3D(fig)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制网格</span></span><br><span class="line"><span class="comment"># X,Y value</span></span><br><span class="line">theta0 = np.linspace(<span class="number">-10</span>, <span class="number">10</span>, <span class="number">100</span>)  <span class="comment"># 网格theta0范围</span></span><br><span class="line">theta1 = np.linspace(<span class="number">-1</span>, <span class="number">4</span>, <span class="number">100</span>)  <span class="comment"># 网格theta1范围</span></span><br><span class="line">x1, y1 = np.meshgrid(theta0, theta1)  <span class="comment"># 画网格</span></span><br><span class="line"><span class="comment"># height value</span></span><br><span class="line">z = np.zeros(x1.shape)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, theta0.size):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">0</span>, theta1.size):</span><br><span class="line">        t = np.matrix([theta0[i], theta1[j]])</span><br><span class="line">        z[i][j] = costFunciton(X, y, t)</span><br><span class="line"><span class="comment"># 由循环可以看出，这里是先取x=-10时，y的所有取值，然后计算代价函数传入z的第一行</span></span><br><span class="line"><span class="comment"># 因此在绘图过程中，需要把行和列转置过来</span></span><br><span class="line">z = z.T</span><br><span class="line">ax.set_xlabel(<span class="string">r'$\theta_0$'</span>)</span><br><span class="line">ax.set_ylabel(<span class="string">r'$\theta_1$'</span>)</span><br><span class="line"><span class="comment"># 绘制函数图像</span></span><br><span class="line">ax.plot_surface(x1, y1, z, rstride=<span class="number">1</span>, cstride=<span class="number">1</span>)</span><br></pre></td></tr></table></figure><p>结果图：</p><p><img src="https://s2.ax1x.com/2019/04/02/AydLtg.md.png" alt="AydLtg.md.png"></p><p>关于3D图中的<a href="https://nullblog.top/2019/03/16/Numpy%E4%B8%AD%E7%9A%84Meshgrid/" target="_blank" rel="noopener">Numpy中的Meshgrid</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">绘制等高线图</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line">plt.figure(figsize=(<span class="number">8</span>, <span class="number">5</span>))</span><br><span class="line">lvls = np.logspace(<span class="number">-2</span>, <span class="number">3</span>, <span class="number">20</span>)</span><br><span class="line">plt.contour(x1, y1, z, levels=lvls, norm=LogNorm())  <span class="comment"># 画出等高线</span></span><br><span class="line">plt.plot(final_theta[<span class="number">0</span>, <span class="number">0</span>], final_theta[<span class="number">0</span>, <span class="number">1</span>], <span class="string">'r'</span>, marker=<span class="string">'x'</span>)  <span class="comment"># 标出代价函数最小值点</span></span><br></pre></td></tr></table></figure><p>结果图：</p><p><img src="https://s2.ax1x.com/2019/04/02/AydOhQ.md.png" alt="AydOhQ.md.png"></p><h2 id="线性回归-多变量-multiple-variables"><a href="#线性回归-多变量-multiple-variables" class="headerlink" title="线性回归-多变量(multiple variables)"></a>线性回归-多变量(multiple variables)</h2><h3 id="特征缩放-Feature-scaling"><a href="#特征缩放-Feature-scaling" class="headerlink" title="特征缩放(Feature scaling)"></a>特征缩放(Feature scaling)</h3><p>也叫均值归一化</p><p>（1）公式：$x=\frac{x-\mu}{s_{1}}$，其中$\mu$为$x$的均值，$s_{1}$为$x$的最大值减去最小值，或者使用标准差。</p><p>（2）作用：在多个特征值情况下，如果某个特征值$x_{i}$的取值范围和另一个特征值$x_{j}$的取值范围相差太大，会减慢梯度下降的速度，因此需要用特征缩放，将不同特征值的取值限定在差不多的范围内。</p><p>e.g. $x_{1}$为房子的面积，取值范围0~2000；$x_{2}$为卧室的数量，取值0-5；那么对二者使用特征缩放，可得：</p><p><img src="https://s2.ax1x.com/2019/03/31/ADOqIS.md.png" alt="ADOqIS.md.png"></p><p>（3）代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">data = (data - data.mean()) / data.std()  </span><br><span class="line"><span class="comment"># 除数可以用标准差也可以用max-min，因为pandas方便，所以使用标准差</span></span><br></pre></td></tr></table></figure><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p>[1] <a href="https://blog.csdn.net/sdu_hao/article/details/82973212" target="_blank" rel="noopener">机器学习 | 吴恩达机器学习第一周学习笔记</a></p><p>[2] <a href="https://blog.csdn.net/sdu_hao/article/details/83932480" target="_blank" rel="noopener">机器学习 | 吴恩达机器学习第二周编程作业(Python版）</a></p><p>[3] <a href="https://blog.csdn.net/Cowry5/article/details/80174130" target="_blank" rel="noopener">吴恩达机器学习作业Python实现(一)：线性回归</a></p><h2 id="代码-1"><a href="#代码-1" class="headerlink" title="代码"></a>代码</h2><p><a href="https://github.com/scp-1024/Coursera-ML-Ng/tree/master/Exercise%201-Linear%20Regression" target="_blank" rel="noopener">scp-1024/Coursera-ML-Ng</a></p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;对《机器学习》这门课程的回顾，系列文章的目的是希望能够把&lt;strong&gt;原理&lt;/strong&gt;和&lt;strong&gt;代码&lt;/strong&gt;实现统一起来，增进理解，所以对一些我认为简单的知识，可能会一笔带过。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;这里对第一周的内容做一些简单的回顾：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://s2.ax1x.com/2019/03/29/A0dT2V.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="Machine Learning" scheme="http://yoursite.com/tags/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>机器学习|吴恩达机器学习之逻辑回归</title>
    <link href="http://yoursite.com/2019/04/03/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/"/>
    <id>http://yoursite.com/2019/04/03/机器学习-吴恩达机器学习之逻辑回归/</id>
    <published>2019-04-03T04:27:42.000Z</published>
    <updated>2019-04-13T05:44:50.048Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>一个名为回归确用于解决分类的算法，吴恩达coursera第三周</p></blockquote><a id="more"></a><h2 id="逻辑回归-Logic-Regression"><a href="#逻辑回归-Logic-Regression" class="headerlink" title="逻辑回归(Logic Regression)"></a>逻辑回归(Logic Regression)</h2><h3 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h3><h4 id="数据处理"><a href="#数据处理" class="headerlink" title="数据处理"></a>数据处理</h4><p>（1）常规读取数据的方法，值得注意的是<code>X = data.iloc[:, :-1].values</code>中<code>.values</code>作用是将<code>DataFrame</code>转化为<code>ndarray</code>。根据手册，更推荐使用<code>.to_numpy()</code>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">path = <span class="string">'ex2data1.txt'</span></span><br><span class="line">data = pd.read_csv(path, names=(<span class="string">'exam1'</span>, <span class="string">'exam2'</span>, <span class="string">'admitted'</span>))</span><br><span class="line">data_copy = pd.read_csv(path, names=(<span class="string">'exam1'</span>, <span class="string">'exam2'</span>, <span class="string">'admitted'</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 进一步准备数据，对结构初始化</span></span><br><span class="line">data.insert(<span class="number">0</span>, <span class="string">'One'</span>, <span class="number">1</span>)</span><br><span class="line">X = data.iloc[:, :<span class="number">-1</span>].values </span><br><span class="line">y = data.iloc[:, <span class="number">-1</span>].values</span><br><span class="line">theta = np.zeros(</span><br><span class="line">    X.shape[<span class="number">1</span>])  <span class="comment"># 注意这里theta创建的是一维的数组，对于ndarray一定要注意一维时它的shape（和matrix有很大区别）</span></span><br></pre></td></tr></table></figure><h4 id="数据可视化"><a href="#数据可视化" class="headerlink" title="数据可视化"></a>数据可视化</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">数据可视化</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_data</span><span class="params">(data)</span>:</span></span><br><span class="line">    cols = data.shape[<span class="number">1</span>]</span><br><span class="line">    feature = data.iloc[:, <span class="number">0</span>:cols - <span class="number">1</span>]</span><br><span class="line">    label = data.iloc[:, cols - <span class="number">1</span>]</span><br><span class="line">    <span class="comment"># iloc 根据列的位置索引来切片</span></span><br><span class="line">    postive = feature[label == <span class="number">1</span>]</span><br><span class="line">    negtive = feature[label == <span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># plt.figure(figsize=(8, 5))</span></span><br><span class="line">    plt.scatter(postive.iloc[:, <span class="number">0</span>], postive.iloc[:, <span class="number">1</span>])</span><br><span class="line">    plt.scatter(negtive.iloc[:, <span class="number">0</span>], negtive.iloc[:, <span class="number">1</span>], c=<span class="string">'r'</span>, marker=<span class="string">'x'</span>)</span><br><span class="line">    plt.legend([<span class="string">'Admitted'</span>, <span class="string">'Not admitted'</span>], loc=<span class="number">1</span>)</span><br><span class="line">    plt.xlabel(<span class="string">'Exam1 score'</span>)</span><br><span class="line">    plt.ylabel(<span class="string">'Exam2 score'</span>)</span><br></pre></td></tr></table></figure><h4 id="假设函数"><a href="#假设函数" class="headerlink" title="假设函数"></a>假设函数</h4><h5 id="假设函数："><a href="#假设函数：" class="headerlink" title="假设函数："></a>假设函数：</h5><p>（1）公式：$h_{\theta}(x)=\frac{1}{1+e^{-\theta^{T}x}}$，又名Sigmoid function，函数图像如下图所示，从图中可以看出$h_{\theta}$的取值范围0~1</p><p><img src="https://s2.ax1x.com/2019/04/03/Ac02QJ.png" alt=""></p><p>（2）$h_{\theta}$的含义是：<strong>特征x的情况下，y=1的概率</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span><span class="params">(z)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span> / (<span class="number">1</span> + np.exp(-z))</span><br></pre></td></tr></table></figure><h4 id="代价函数："><a href="#代价函数：" class="headerlink" title="代价函数："></a>代价函数：</h4><p>如果这时再使用线性回归的$J(\theta_{0},\theta_{1})=\frac{1}{2m}\sum_{i=1}^{m}(h_{\theta}(x_{i})-y_{i})^{2}$将会导致函数图像不光滑。这样使用最小值法时容易导致无法找到全局最优解。因此需要使用新的代价函数。</p><p><img src="https://s2.ax1x.com/2019/04/03/AcBilQ.png" alt="AcBilQ.png"></p><p>（1）公式：<img src="https://s2.ax1x.com/2019/04/03/Ac0jeI.png" alt="Ac0jeI.png"></p><p>具体解释如下：</p><p><img src="https://s2.ax1x.com/2019/04/03/AcBVwq.md.png" alt="AcBVwq.md.png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">cost function可以用矩阵实现也可以用ndarray实现，更建议使用后者</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cost</span><span class="params">(theta, X, y)</span>:</span></span><br><span class="line">    first = (-y) * np.log(Sigmoid.sigmoid(X @ theta))  <span class="comment"># 这里*号是对应位置相乘而不是矩阵运算</span></span><br><span class="line">    second = (<span class="number">1</span> - y) * np.log(<span class="number">1</span> - Sigmoid.sigmoid(X @ theta))</span><br><span class="line">    <span class="keyword">return</span> np.mean(first - second)</span><br></pre></td></tr></table></figure><h4 id="高级优化法："><a href="#高级优化法：" class="headerlink" title="高级优化法："></a>高级优化法：</h4><p>除了梯度下降法之外，还有其他几种优化方法，比起gradient descent，这些方法更适合处理大型数据且不需要你设置学习速率。</p><p><img src="https://s2.ax1x.com/2019/04/03/AcBl6J.md.png" alt="AcBl6J.md.png"></p><p>这些方法的具体原理非常复杂，但是，python的模块往往是非常强大的，因此往往只需要你计算一下到导数项即可</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradient</span><span class="params">(theta, X, y)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> (<span class="number">1</span> / len(X)) * (X.T @ (Sigmoid.sigmoid(X @ theta) - y))</span><br></pre></td></tr></table></figure><p>再使用<code>import scipy.optimize as opt</code>中的<code>.fmin_tnc</code>来迭代,得到最终的$\theta$值。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 这里不使用梯度下降法，换成其他优化算法来迭代</span></span><br><span class="line">result = opt.fmin_tnc(func=cost, x0=theta, fprime=gradient, args=(X, y))</span><br><span class="line">final_theta = result[<span class="number">0</span>]</span><br></pre></td></tr></table></figure><h4 id="检测准确率"><a href="#检测准确率" class="headerlink" title="检测准确率"></a>检测准确率</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">预测标签</span></span><br><span class="line"><span class="string">参数：</span></span><br><span class="line"><span class="string">-参数：theta</span></span><br><span class="line"><span class="string">-样本：X</span></span><br><span class="line"><span class="string">返回值：</span></span><br><span class="line"><span class="string">-预测标签</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(theta, X)</span>:</span></span><br><span class="line">    probability = Sigmoid.sigmoid(X @ theta)</span><br><span class="line">    <span class="keyword">return</span> [<span class="number">1</span> <span class="keyword">if</span> x &gt;= <span class="number">0.5</span> <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">for</span> x <span class="keyword">in</span> probability]</span><br></pre></td></tr></table></figure><p>将得到的预测标签同数据原有的标签进行比对，得到准确率</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">predictions = predict(final_theta, X)</span><br><span class="line">correct = [<span class="number">1</span> <span class="keyword">if</span> a == b <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">for</span> (a, b) <span class="keyword">in</span> zip(predictions, y)]</span><br><span class="line">accurcy = np.sum(correct) / len(X)  <span class="comment"># 准确率89%</span></span><br></pre></td></tr></table></figure><p>也可以自定义一个样本预测一发</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 输入一个数据进行预测</span></span><br><span class="line">test = np.array([<span class="number">1</span>, <span class="number">45</span>, <span class="number">85</span>]).reshape(<span class="number">1</span>, <span class="number">3</span>)</span><br><span class="line">predict_result = predict(final_theta, test)  <span class="comment"># 预测值y=1，概率为0.776</span></span><br></pre></td></tr></table></figure><h4 id="决策边界"><a href="#决策边界" class="headerlink" title="决策边界"></a>决策边界</h4><p>$h_{\theta}=0.5$时的直线，由假设函数图像可以看出，当$h_{\theta}=0.5$时，$z=\theta^{T}x=0$，这里图中是以$x2$作为纵坐标。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 决策边界</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_decision_boundary</span><span class="params">(theta, X)</span>:</span></span><br><span class="line">    plot_x = np.linspace(<span class="number">20</span>, <span class="number">110</span>)</span><br><span class="line">    plot_y = -(theta[<span class="number">0</span>] + plot_x * theta[<span class="number">1</span>]) / theta[<span class="number">2</span>]</span><br><span class="line">    plt.plot(plot_x, plot_y, c=<span class="string">'y'</span>)</span><br></pre></td></tr></table></figure><h4 id="画图函数"><a href="#画图函数" class="headerlink" title="画图函数"></a>画图函数</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">plt.figure(figsize=(<span class="number">8</span>, <span class="number">5</span>))</span><br><span class="line">plot_data(data_copy)</span><br><span class="line">plot_decision_boundary(final_theta, X)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>结果图为：</p><p><img src="https://s2.ax1x.com/2019/04/03/AcBvjJ.md.png" alt="AcBvjJ.md.png"></p><h2 id="带正则化项的逻辑回归函数"><a href="#带正则化项的逻辑回归函数" class="headerlink" title="带正则化项的逻辑回归函数"></a>带正则化项的逻辑回归函数</h2><h4 id="读入数据："><a href="#读入数据：" class="headerlink" title="读入数据："></a>读入数据：</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">path = <span class="string">'ex2data2.txt'</span></span><br><span class="line">data = pd.read_csv(</span><br><span class="line">    path, names=(<span class="string">'Microchip Test1'</span>, <span class="string">'Microchip Test2'</span>, <span class="string">'Accept'</span>))</span><br><span class="line">x1 = data.iloc[:, <span class="number">0</span>].values</span><br><span class="line">x2 = data.iloc[:, <span class="number">1</span>].values</span><br></pre></td></tr></table></figure><h4 id="可视化数据"><a href="#可视化数据" class="headerlink" title="可视化数据"></a>可视化数据</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_data</span><span class="params">(data)</span>:</span></span><br><span class="line">    feature = data.iloc[:, <span class="number">0</span>:<span class="number">2</span>]</span><br><span class="line">    label = data.iloc[:, <span class="number">2</span>]</span><br><span class="line">    positive = feature[label == <span class="number">1</span>]</span><br><span class="line">    negative = feature[label == <span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    plt.scatter(positive.iloc[:, <span class="number">0</span>].values, positive.iloc[:, <span class="number">1</span>].values)</span><br><span class="line">    plt.scatter(</span><br><span class="line">        negative.iloc[:, <span class="number">0</span>].values,</span><br><span class="line">        negative.iloc[:, <span class="number">1</span>].values,</span><br><span class="line">        c=<span class="string">'r'</span>,</span><br><span class="line">        marker=<span class="string">'x'</span>)</span><br></pre></td></tr></table></figure><h4 id="特征映射"><a href="#特征映射" class="headerlink" title="特征映射"></a>特征映射</h4><p>由可视化数据可知，如果单独用两个特征，是无法表示出决策边界的（欠拟合underfit）。因此需要映射多个特征。</p><p><img src="https://s2.ax1x.com/2019/04/03/AcDVjH.png" alt="AcDVjH.png"></p><p>这里将$x1$和$x2$映射为6个特征值</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">map_feature</span><span class="params">(x1, x2)</span>:</span></span><br><span class="line">    degree = <span class="number">6</span></span><br><span class="line"></span><br><span class="line">    x1 = x1.reshape((x1.size, <span class="number">1</span>))  <span class="comment"># ndarray.size：数组中元素的个数</span></span><br><span class="line">    x2 = x2.reshape((x2.size, <span class="number">1</span>))</span><br><span class="line">    result = np.ones(x1.shape[<span class="number">0</span>])  <span class="comment"># 初始化一个值为1的数组(列向量)</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, degree + <span class="number">1</span>):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">0</span>, i + <span class="number">1</span>):</span><br><span class="line">            result = np.c_[result, (x1**(i - j) * (x2**j))]  <span class="comment"># np.c_：列拼接</span></span><br><span class="line">    <span class="keyword">return</span> result  <span class="comment"># 返回值即为特征值X</span></span><br></pre></td></tr></table></figure><p>但这种映射可能导致过拟合（overfit），泛化能力差，因此还需要正则化（regularization）。</p><h4 id="正则化"><a href="#正则化" class="headerlink" title="*正则化"></a>*正则化</h4><p>（1）原理：</p><p>因为过拟合是由于特征项过多引起的，减少特征的数量固然可以，还有一种方法就是正则化：<strong>减小$\theta_{j}$的值。</strong></p><p><img src="https://s2.ax1x.com/2019/04/03/AcDRV1.md.png" alt="AcDRV1.md.png"></p><p>由图中可以看出，线性回归算法中添加两个正则化项——也叫惩罚项，1000$\theta_{3}$和1000$\theta_{4}$。上图可以看出，在利用优化算法求解参数时，要想让代价函数值变小，会使得$\theta_{3}$和$\theta_{4}$变得非常小，也就导致了$\theta_{3}x^{3}$和$\theta_{4}x^{4}$非常小，那么图中右边的假设函数就近似与左边的函数了。</p><p>实际操作中，我们很多时候并不知道究竟应该惩罚哪一项，所以实际上除了$\theta_{0}​$（全是1），所有项都会惩罚。</p><p>回到逻辑回归算法上也是一样的</p><p>（2）公式：</p><p><img src="https://s2.ax1x.com/2019/04/03/AcD4PK.md.png" alt="AcD4PK.md.png"></p><p>对于线性回归算法也类似：</p><p><img src="https://s2.ax1x.com/2019/04/03/AcD7KH.png" alt="AcD7KH.png"></p><h4 id="代价函数：-1"><a href="#代价函数：-1" class="headerlink" title="代价函数："></a>代价函数：</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cost_reg</span><span class="params">(theta,X, y, lmd)</span>:</span></span><br><span class="line">    <span class="comment"># 不惩罚第一项</span></span><br><span class="line">    _theta = theta[<span class="number">1</span>:]</span><br><span class="line">    reg = (lmd / (<span class="number">2</span> * len(X))) * (_theta @ _theta)</span><br><span class="line"></span><br><span class="line">    first = (y) * np.log(Sigmoid.sigmoid(X @ theta))</span><br><span class="line">    second = (<span class="number">1</span> - y) * np.log(<span class="number">1</span> - Sigmoid.sigmoid(X @ theta))</span><br><span class="line">    final = -np.mean(first + second)</span><br><span class="line">    <span class="keyword">return</span> final + reg</span><br></pre></td></tr></table></figure><h4 id="梯度函数"><a href="#梯度函数" class="headerlink" title="梯度函数"></a>梯度函数</h4><p>（1）公式：<img src="https://s2.ax1x.com/2019/04/06/AWJzAf.md.png" alt="AWJzAf.md.png"></p><p>（2）向量化：$\theta_{j}:=\theta-\alpha[\frac{1}{m} X^{T} (g(X^{T}\theta)-y)+\frac{\lambda}{m}\theta_{j}]$</p><p>代码只需要计算蓝色括号中的内容，然后用<strong>优化算法</strong>迭代：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradient_reg</span><span class="params">(theta,X, y, lmd)</span>:</span></span><br><span class="line">    <span class="comment"># 因为不惩罚第一项，所以要分开计算</span></span><br><span class="line">    grad = (<span class="number">1</span> / len(X)) * (X.T @ (Sigmoid.sigmoid(X @ theta) - y))</span><br><span class="line">    grad[<span class="number">1</span>:] += (lmd / len(X)) * theta[<span class="number">1</span>:]</span><br><span class="line">    <span class="keyword">return</span> grad</span><br></pre></td></tr></table></figure><h4 id="优化算法："><a href="#优化算法：" class="headerlink" title="优化算法："></a>优化算法：</h4><p>用于迭代计算$\theta_{j}$值。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">result = opt.fmin_tnc(</span><br><span class="line">func=cost_reg,</span><br><span class="line">x0=theta,</span><br><span class="line">fprime=gradient_reg,</span><br><span class="line">args=(X, y, <span class="number">1</span>),</span><br><span class="line">)</span><br></pre></td></tr></table></figure><h4 id="画出决策边界"><a href="#画出决策边界" class="headerlink" title="画出决策边界"></a>画出决策边界</h4><p>不是代入假设函数来画！！在逻辑回归中假设函数时Sigmoid function，用于计算概率的！！</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_decision_boundary</span><span class="params">(theta)</span>:</span></span><br><span class="line">    x=np.linspace(<span class="number">-1</span>,<span class="number">1.5</span>,<span class="number">50</span>)</span><br><span class="line">    plot_x,plot_y=np.meshgrid(x,x) <span class="comment"># 先画网格</span></span><br><span class="line"></span><br><span class="line">    z=map_feature(plot_x,plot_y)  </span><br><span class="line">    z=z@theta <span class="comment"># 画出边界</span></span><br><span class="line">    z=z.reshape(plot_x.shape)</span><br><span class="line">    plt.contour(plot_x,plot_y,z,<span class="number">0</span>,colors=<span class="string">'yellow'</span>)</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;一个名为回归确用于解决分类的算法，吴恩达coursera第三周&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
      <category term="Machine Learning" scheme="http://yoursite.com/tags/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>信息熵</title>
    <link href="http://yoursite.com/2019/03/29/%E4%BF%A1%E6%81%AF%E7%86%B5/"/>
    <id>http://yoursite.com/2019/03/29/信息熵/</id>
    <published>2019-03-29T02:53:40.000Z</published>
    <updated>2019-03-29T02:53:40.260Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>我们在生活中常常听到“这句话信息量好大。”、“这都是废话，没什么信息量。“，而我们却很少思考，这里的信息量究竟是什么意思？是否可以给出精确定义，甚至，量化它呢？</p></blockquote><p><img src="https://s2.ax1x.com/2019/03/26/ANfKbT.png" alt=""></p><a id="more"></a><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>1984年香农提出了”信息熵“的概念，解决了这个问题。</p><h2 id="信息中的熵"><a href="#信息中的熵" class="headerlink" title="信息中的熵"></a>信息中的熵</h2><blockquote><p>因为熵不只在信息，还在物理等其他领域有定义，虽然他们本质一样，但表达上和所研究的问题上略有差异，这里特作说明。</p></blockquote><p>可以这么理解：当一件事情（宏观态）具有多种情况时，对于观察者而言，其具体情况（微观态）的<strong>不确定度</strong>，称为熵。</p><p>显然这个定义看上去肯定是懵逼的，没事，用一个例子来解释。</p><p>假设我们在做一道单项选择题，这道选择题的答案具有4个选项。那么对你这个观察者而言，这个选择题正确答案的可能选择情况（四种选择情况：选A、选B、选C、选D）就是宏观态，而其中的每一个选择情况就被称为微观态。我们目前并不能确定谁是正确答案，所以其微观态就具有不确定性。那么此时此刻，”选择正确选项”这件事情对你而言，熵为2bit。</p><p>（1）、而正在你陷入迷茫之际，你的好基友小明来到了你的身边，悄悄地对你说：”有50%可能性选C。“</p><p>这条消息中含有的<strong>信息量</strong>，帮助你调整了每个微观态的概率：从等概率（A：25%，B：25%，C：25%，D：25%）调整为（A：16.6%，B：16.6%，C：50%，D：16.6%），而这条消息中含有的信息量为0.21bit，所以才“使用”了这条消息之后，“选出正确选项”这件事情的熵为$2-0.21=1.79bit$</p><p>（2）、这时候，你突然想起来可以直接看参考答案啊，于是你打开了书的最后一页发现——这题的正确答案选C。同样，这条消息含有的信息量同样也帮你调整了概率，这次更直接的把C的概率改为了100%。那么这条消息的信息量为2bit，因为它帮助你完全消除了“选出正确选项”这件事情的不确定性。</p><p>从上面的例子中可以发现，信息量的作用，就是用来消除熵。</p><p>到这里，你应该对熵的概念有了大体的理解，但还有一个问题，就是信息量是如何计算的？</p><h2 id="信息量是如何计算的"><a href="#信息量是如何计算的" class="headerlink" title="信息量是如何计算的"></a>信息量是如何计算的</h2><p>我们都知道，在定义重量的单位时，我们先是选择了一个<strong>参照物</strong>，比如一块砖头，我们把这块砖头的重量定义为1千克，剩下的所有物体的重量都可以利用这块砖头来表示，比如把我和一堆砖头放在天平上，当达到水平状态时，只要数出砖头个数——65个，那么就可以知道我的重量是65kg。</p><p>基于同样的道理，我们先找一个参照物事件——扔硬币！它只有2种等概率情况，即50%正面50%反面（微观态）。它的熵（不确定性）我们记为1bit，那么再回顾一下你的选择题，在刚开始时，它的等概率情况有4种，所以它的熵就是2bit。</p><p>同样推断，如果一件事情的等概率情况有8种，那么它的熵就是4bit…那是不可能的。</p><p>你先仔细想想，什么事情具有8种等概率情况？对的，扔3枚硬币，所以熵是3bit而不是4bit。原因是，扔硬币结果的个数和硬币之间是<strong>指数关系</strong>而不是线性关系。</p><p>所以我们可以总结：<strong>在各个微观态之间是等概率的情况下，把宏观态的熵记为n，微观态的个数记为m，就可以得到 $n=log_{2}m​$</strong></p><p>但问题又来了，如果各个微观态之间不是等概率的情况呢？</p><p>显然无法直接用上面的公式，这里回到“选择题事件”的（1）中，当我们得知了小明给出的消息之后，熵的bit数为：$n=p_{A}<em>log_{2}(1/p_{A})+p_{B}</em>log_{2}(1/p_{B})+p_{C}<em>log_{2}(1/p_{C})+p_{D}</em>log_{2}(1/p_{D})$ </p><p>其中$p$为每个选项的概率。这里对数中概率取倒数的原因：概率的倒数等于其发生在等概率情况的个数。</p><p>计算后熵 $n=1.79bit$ ，则小明的消息中的信息量就是0.21bit 。</p><p><strong>以上的内容均来自 <a href="https://space.bilibili.com/344849038/video" target="_blank" rel="noopener">B站—YJango</a>中10~11期的内容。</strong></p><h2 id="计算熵的公式"><a href="#计算熵的公式" class="headerlink" title="计算熵的公式"></a>计算熵的公式</h2><p>那么可以做出一个总结，如果一个随机变量A有k中可能取值，其中第i种发生的概率为P(i)，那么信息熵的公式为：<img src="https://s2.ax1x.com/2019/03/26/ANwmT0.png" alt=""></p><p>由此也可见，一个取值的概率越低，他的熵就越高，你确定他所需要的信息量就越大。也可以证明，等概率情况下，信息熵的值最大$^{[1]}$。</p><p>而在等概率（每个情况发生的概率都为P(a)）情况下，我们需要了解一个随机变量所需要的信息量，为$L=log_{2}(1/P(a))$，这种说法由R.V.L.哈莱特与1928年提出$^{[2]}$，早于香农，实际上这里求的也就是熵，只不过那时候没这么叫。（见 信息量和信息熵的区别）</p><p>当时看到这里我有一个巨大的疑问，如果按这个公式计算，那么“选择题”中的（1），小明的消息提供的信息量应该为1bit啊怎么会是0.21bit。</p><p>后来我是这么理解的，如果<strong>单独</strong>把”50%的可能性选C”看成随机变量的话，你作为观察者所面对的情况实际上只有 <strong>2种</strong>：选C或者不选C。那么的确熵为1bit。</p><p>而放回到原来的事件中，情况为<strong>4种</strong>，这条信息只不过使得每个情况的概率都改变了，或者说，仅仅是4种情况之间的权重发生了改变，需要通过期望的方式来计算熵（1.79bit），再相减，得出结果（0.21bit）代表了这条信息在<strong>当前所讨论的宏观态下消除的不确定性。</strong></p><p>更进一步说，其实这里隐藏了一个条件，<strong>我们默认把”50%的可能性选C“这件条信息当成是确定的（熵为0而不是单独情况时的1bit）</strong>，并且在宏观态中根据信息调整了微观态的权重，而这时这条信息对这个宏观态产生了影响，使其熵减了0.21bit。</p><h2 id="信息量和信息熵"><a href="#信息量和信息熵" class="headerlink" title="信息量和信息熵"></a>信息量和信息熵</h2><blockquote><p>信息熵就是上面说的熵，用“信息熵”的叫法是为了和信息量对称</p></blockquote><p>信息熵：描述的是一个事件（宏观态）的不确定度，也就是说，信息熵是宏观态的一个<strong>属性</strong>，并不因为观察者而产生变化，比如你知道了一道选择题的答案，那么这道题的信息熵仅仅对你而言是0bit（也就是说你对这道题已经是确定了的），这时另外有人看到了这道选择题，那么对他而言，这道题的信息熵仍然为2bit。</p><p>信息量：你解开一个不确定事件的过程，在这个过程中你不断的获得<strong>信息量</strong>，来消除信息熵。</p><p>二者的本质上存在差异，但其实是角度不同导致的。</p><blockquote><p>比较好的表述是:</p><p>信息熵是</p><p><strong>对于事件A，我们对A不了解的程度</strong>。</p><p>换句话说，</p><p><strong>就是我们还需要多少信息量才能完全了解事件A</strong></p><p>所需要的信息量就是信息熵。</p><p><strong>而信息熵很大的意思是事件A本身所携带的信息量很大。</strong></p></blockquote><p>引用自<a href="https://www.zhihu.com/question/274997106/answer/383102744" target="_blank" rel="noopener">信息熵越大，信息量到底是越大还是越小？ - 捣衣的回答 - 知乎</a></p><h2 id="熵在物理中的运用"><a href="#熵在物理中的运用" class="headerlink" title="熵在物理中的运用"></a>熵在物理中的运用</h2><p>简单来说，热力学第二定律推导出熵增原理：孤立系统的熵永不减少。进而判断宇宙处在一个不断熵增的状态，因此总有一天会达到“热寂寞”</p><p>有兴趣可参考几个视频</p><p><a href="https://www.bilibili.com/video/av31931321?from=search&amp;seid=9987571277393916947" target="_blank" rel="noopener">热力学第二定律是什么？“麦克斯韦妖”是什么鬼？李永乐老师告诉你</a></p><p><a href="https://www.bilibili.com/video/av25408872/?spm_id_from=333.788.videocard.0" target="_blank" rel="noopener">熵到底是什么？一副牌中抽三张为同花的概率是多大？</a></p><p><a href="https://www.bilibili.com/video/av42589628" target="_blank" rel="noopener">【学习观12】阻碍人类永生的原因是？</a></p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p>[1].<a href="https://www.zhihu.com/question/22178202/answer/125040625" target="_blank" rel="noopener">信息熵是什么？ - 柯伟辰的回答 - 知乎</a></p><p>[2].<a href="https://baike.baidu.com/item/%E4%BF%A1%E6%81%AF%E9%87%8F" target="_blank" rel="noopener">信息量—百度百科</a></p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;我们在生活中常常听到“这句话信息量好大。”、“这都是废话，没什么信息量。“，而我们却很少思考，这里的信息量究竟是什么意思？是否可以给出精确定义，甚至，量化它呢？&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img src=&quot;https://s2.ax1x.com/2019/03/26/ANfKbT.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="Reading" scheme="http://yoursite.com/tags/Reading/"/>
    
  </entry>
  
  <entry>
    <title>YJango学习观笔记及感悟</title>
    <link href="http://yoursite.com/2019/03/25/YJango%E5%AD%A6%E4%B9%A0%E8%A7%82%E7%AC%94%E8%AE%B0%E5%8F%8A%E6%84%9F%E6%82%9F/"/>
    <id>http://yoursite.com/2019/03/25/YJango学习观笔记及感悟/</id>
    <published>2019-03-25T10:11:47.000Z</published>
    <updated>2019-03-29T02:54:03.283Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p><a href="https://space.bilibili.com/344849038/video" target="_blank" rel="noopener">b站-YJango</a>，个人还是对这位先生的</p><p>视频地址：<a href="https://space.bilibili.com/344849038/video" target="_blank" rel="noopener">b站-Yjango</a>，个人还是对他表示敬意的，他的视频挺具有启发性。</p><p>本文同样参考了<a href="https://www.zhihu.com/question/305705365/answer/621628417" target="_blank" rel="noopener"> 如何评价知乎用户 YJango 的公众号？ - Makise Chris的回答 - 知乎</a> </p></blockquote><a id="more"></a><h3 id="第一季"><a href="#第一季" class="headerlink" title="第一季"></a>第一季</h3><h4 id="01-何为学习"><a href="#01-何为学习" class="headerlink" title="01.何为学习"></a>01.何为学习</h4><p>学习就是从有限例子中找出规律的过程，而这个规律就是知识。</p><h4 id="02-如何学习"><a href="#02-如何学习" class="headerlink" title="02.如何学习"></a>02.如何学习</h4><p>因为人脑机能原因，单单<strong>记忆</strong>一个知识，显然远远达不到掌握它的水平。</p><p>而首先应该做的是明确<strong>问题（输入）</strong>和<strong>答案（输出）</strong>。</p><p>然后通过大量例子来理清二者的关系。接下来根据你的问题和答案，找出这些例子中符合问题及答案的规律。这两步也就是华罗庚的”先把书读厚（大量例子体会关系），再把书读薄（找出知识来压缩例子）。”</p><p>最后，需要用新的问题来验证你的例子是否正确。</p><h4 id="03-学习误区"><a href="#03-学习误区" class="headerlink" title="03.学习误区"></a>03.学习误区</h4><p>学习误区指什么？见图片黄字。</p><p>学习方式分为两类：（1）运动类：语言、运动（2）思考类：数学、逻辑</p><p>举了个语言的例子：</p><p>错误方式：</p><p><img src="https://s2.ax1x.com/2019/03/24/AYs9G8.png" alt=""></p><p>正确方式：</p><p><img src="https://s2.ax1x.com/2019/03/24/AYrzIP.png" alt=""></p><p>老实说我认为这个观点见仁见智，尤其是摒弃中文作为中间媒介这件事，姑且不论对错，可行性就值得怀疑了。</p><h4 id="04-运用误区"><a href="#04-运用误区" class="headerlink" title="04.运用误区"></a>04.运用误区</h4><p>运动类的学习可以同时进行，比如边走路边聊天；而思考类则不行，只能串行操作，所以当我们遇到一个大的问题（输入）时，如果无法直接得到输出，就需要将这个输入拆分，分别解决。</p><h4 id="05-思维导图"><a href="#05-思维导图" class="headerlink" title="05.思维导图"></a>05.思维导图</h4><p>其功能不在于记忆，而在于克服学习和运用中的误区。</p><p>在构建思维导图的过程中，你会先找到<strong>关键词</strong>，然后问自己它是什么？从而不断的联想起具体的例子，进而分析这些例子的共同输入和输出，找出规律；而当问自己它的作用或目的时，实际在思考输入是怎么变成输出的。输入输出可以代表<strong>一类</strong>事物中任何一种<strong>情况</strong>，因此被称为变量。而从输入变成输出的这一过程称为函数。而确定了这个“函数”，等下一次遇到未知的情况时，利用这个函数就可以解答出输出。随后你会用一个更好的关键词来代表你所找出的关系，一种是动宾结构，因为它描述了输入和函数，也就确定了输出，但当人们开始传播知识时，动宾结构也会名词化。</p><p>还有一种知识本身就是名词，会让你觉得没有输出，但这种知识的输出是分类任务中的类型，描述它的是主谓结构：是或不是/是否属于/属于哪个。</p><p>思维导图最强大的地方在于对知识的拆分，构建出知识网络，而知识网络中有些内容是你已经知道的，即具有重用性，学习新内容时，可以利用这些重用性，加快学习速度。拆分知识既可以用在学习未知知识上，也可以用在运用已有知识上（分而治之）。</p><p>而拆分知识的能力也是一种特殊的知识，称为二阶知识，不同于一般的知识用于描述信息与信息的关系，二阶知识用于描述知识与知识的关系。</p><p>视频中举的例子就是<strong>思维导图</strong>，假设我们需要构建一个有关于思维导图内容的思维导图。</p><p>那么首先，找出关键词——思维导图。然后问自己：思维导图是什么？因此联想出思维导图使用的具体例子：任务清单、知识体系、任务关系…，接着分析这些例子的共同输入——杂乱信息，共同输出——知识网络。而当你问它的作用时，也就是在思考输入是怎么变成输出的。一类事物，这个例子中，假设它代表的是任务清单这一类，那么输入的杂乱信息和输出的知识网络就是任务清单这一类事物的特定情况。而这个“函数”：可以理解为思维导图的画图方法。然后是找动宾结构，这里是压缩信息（其中压缩是函数，信息是输出），当人们开始传递知识时，压缩信息就被名词化成了思维导图。</p><p>知识本身属于名词的没说例子。</p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;&lt;a href=&quot;https://space.bilibili.com/344849038/video&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;b站-YJango&lt;/a&gt;，个人还是对这位先生的&lt;/p&gt;
&lt;p&gt;视频地址：&lt;a href=&quot;https://space.bilibili.com/344849038/video&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;b站-Yjango&lt;/a&gt;，个人还是对他表示敬意的，他的视频挺具有启发性。&lt;/p&gt;
&lt;p&gt;本文同样参考了&lt;a href=&quot;https://www.zhihu.com/question/305705365/answer/621628417&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt; 如何评价知乎用户 YJango 的公众号？ - Makise Chris的回答 - 知乎&lt;/a&gt; &lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
      <category term="Reading" scheme="http://yoursite.com/tags/Reading/"/>
    
  </entry>
  
  <entry>
    <title>关于记忆方法</title>
    <link href="http://yoursite.com/2019/03/25/%E5%85%B3%E4%BA%8E%E8%AE%B0%E5%BF%86%E6%96%B9%E6%B3%95/"/>
    <id>http://yoursite.com/2019/03/25/关于记忆方法/</id>
    <published>2019-03-25T09:52:12.000Z</published>
    <updated>2019-03-25T10:04:58.195Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>阅读了几篇关于记忆方法的文章，有所感悟，结合了一下自己经历，选了几个我认为有长期实践意义的方法，在此记录，此文可能会长期更新，毕竟记忆方法也是要不断调整的。</p></blockquote><h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>我认为针对不同问题，应该具有不同的记忆方法。比如：</p><p>单词记忆，那么也许联想类方法比较有用；</p><p>记忆某些抽象概念，如剩余价值的含义，也许更适合用关键词法；</p><p>…</p><p>所以记忆方法不能一概而论，犯教条主义错误。</p><a id="more"></a><h4 id="记忆整体框架"><a href="#记忆整体框架" class="headerlink" title="记忆整体框架"></a>记忆整体框架</h4><p>结构化，也就是常说的<strong>思维导图</strong>，从全局出发，有助于你理清思路，尤其当你面对一个比较庞杂而又相互关联的知识（比如政治一整个章节的内容）。现在很多书籍都会替你整理好，但是请务必要<strong>用自己的方式去整理一遍</strong>，也许有人会说，我自己思考的和书上整理的一样啊，那么也请合上书默写出来。</p><p>实际上你默写也好，自己写也罢，<strong>这里的关键是在这个过程是一定要去==主动思考==（敲重点！）这个庞杂的知识的脉络到底是什么样的。</strong></p><p>思维导图：</p><p><img src="https://s2.ax1x.com/2019/03/25/At22X8.png" alt=""></p><p><img src="https://s2.ax1x.com/2019/03/25/AtRC1x.png" alt=""></p><p>参考资料：<a href="https://zhuanlan.zhihu.com/YJango" target="_blank" rel="noopener">知乎专栏——Yjango</a></p><h4 id="记忆段落-句子内容"><a href="#记忆段落-句子内容" class="headerlink" title="记忆段落/句子内容"></a>记忆段落/句子内容</h4><p>一般来说，我们学习的总是不熟悉的内容，再加上作者和你在表述习惯上的差异，才会导致你无法理解作者想要传达的意思，才会导致只能死记硬背。</p><p>所以同样需要我们用自己的方式理解并记忆作者想要传达给你的意思。那么如何去用自己的语言表述出来呢？</p><p>引用一句话：记忆<strong>书本原有的关键词、知识点</strong>等<strong>“点”信息</strong>，<strong>而后自己去连“点”成“线</strong>”——指的是把这一块的“点”信息串成有逻辑的内容$^{[2]}$。</p><p>举个例子：</p><blockquote><p>类是对一群具有相同特征或者行为的事物的一个统称，是抽象的，不能直接使用。特征被称为”属性“，行为被称为”方法“。</p></blockquote><p>在这个句子中，对我而言，关键词是”类，特征=属性，行为=方法“。所以对我而言只需要记住这几个内容，剩下的用自己的话重复一遍：</p><blockquote><p>类是具有相同的特征和行为的事物的统称，是一个抽象概念。实际运用时，特征又叫做属性，行为又叫方法。</p></blockquote><p><strong>参考资料</strong>：[2].<a href="https://www.zhihu.com/question/50343728" target="_blank" rel="noopener">你有什么值得分享的高效学习方法？</a> 这个问题下最高赞的回答</p><p>我认为这个方法尤其适用于抽象概念——尤其是文科类——的记忆。</p><h4 id="记忆关键词"><a href="#记忆关键词" class="headerlink" title="记忆关键词"></a>记忆关键词</h4><p>联想记忆法，这个方法算是很常见了，但我认为可以看一下这个视频：·<a href="https://weibo.com/1974576991/HmiDNf2ps?type=comment" target="_blank" rel="noopener">TED画图记忆方法</a>。</p><p>视频中提到可以用画图的方式来替代文字记忆，但我认为在记忆大量内容的时候这种方法太低效。不过比起单纯的联想，画图具有更强的可操作性（有时候真是联想不到），而且动笔的过程中又进一步加深了你的思考。所以我认为可以在上一个记忆法中，记忆关键字时，根据需要使用这个方法。</p><p>还是上面的例子，当我们提取关键字后，可能还是会觉得抽象，类是什么？特征和行为又怎么理解？这时候把这几个词语通过联想的方式去理解记忆：</p><p>类，可以看做是制造一样东西（比如飞机）的设计图；属性，就好比飞机都有一对翅膀；方法，就好比飞机如何起飞。还可以接着拓展，比如用类创建一个对象，就相当于用设计图纸去制造一架飞机。</p><p><img src="https://s2.ax1x.com/2019/03/24/AYeagH.png" alt=""></p><p>其实无论哪种方法，归根结底都是要把新的知识转化为自己更熟悉的方式，加以理解并记忆。</p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;阅读了几篇关于记忆方法的文章，有所感悟，结合了一下自己经历，选了几个我认为有长期实践意义的方法，在此记录，此文可能会长期更新，毕竟记忆方法也是要不断调整的。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h4&gt;&lt;p&gt;我认为针对不同问题，应该具有不同的记忆方法。比如：&lt;/p&gt;
&lt;p&gt;单词记忆，那么也许联想类方法比较有用；&lt;/p&gt;
&lt;p&gt;记忆某些抽象概念，如剩余价值的含义，也许更适合用关键词法；&lt;/p&gt;
&lt;p&gt;…&lt;/p&gt;
&lt;p&gt;所以记忆方法不能一概而论，犯教条主义错误。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Reading" scheme="http://yoursite.com/tags/Reading/"/>
    
  </entry>
  
  <entry>
    <title>正向传播的向量化实现</title>
    <link href="http://yoursite.com/2019/03/23/%E6%AD%A3%E5%90%91%E4%BC%A0%E6%92%AD%E7%9A%84%E5%90%91%E9%87%8F%E5%8C%96%E5%AE%9E%E7%8E%B0/"/>
    <id>http://yoursite.com/2019/03/23/正向传播的向量化实现/</id>
    <published>2019-03-23T06:51:39.000Z</published>
    <updated>2019-04-06T10:13:09.799Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>神经网络中正向传播(Forward propagation)的向量化(Vectorized implementation)</p></blockquote><p><img src="https://s2.ax1x.com/2019/03/23/AGrwuR.png" alt=""></p><a id="more"></a><p>其实没什么难的，就是几个符号理解起来可能有点费劲，下面简单解释一下：</p><p>从这个式子入手：$z^{(2)}=\Theta^{(1)}*a^{(1)}$</p><p>$z^{(2)}$等于信号$a^{(1)}$在传输过程中乘上响应的权重$\Theta^{(1)}​$，展开来写的话就是左图中等号右边的灰色线框。</p><p>$a^{(1)}$（activation values of layer one）是指第一层中$x_{0}$~$x_{3}$构成的一维<strong>向量</strong>，上标1就是指第一层。</p><p>$\Theta^{(1)}$是指传递过程中不同信号所对应的权重，反应在公式上就是$\Theta^{(1)}_{10}$~$\Theta^{(1)}_{33}$所构成的<strong>矩阵</strong>。</p><p>$\Theta^{(1)}_{10}$下标的10是指，接收信号的神经元为下一层的第一个，发出信号的为这一层的第0个神经元。</p><p>这里额外说一下这个$\Theta^{(1)}​$的维度，如果在第$j​$层有$s_{j}​$个神经元(units)，在第$j+1​$层有$s_{j+1}​$个神经元，那么对应的权重的维度为$s_{j+1}*(s_{j}+1)​$。</p><p>其中$s_{j}+1$是因为上一层在传递时，默认会传递一个$s_{0}=1​$</p><p>以上是传递过程中的，$z^{(2)}$传递到神经元之后，就通过激励函数（图中是S函数）求出需要传递个下一层的信号$a^{(2)}$</p><p>即公式：$a^{(2)}=g(z^{(2)})$</p><p>依次类推，即可求出最终的假设函数$h_{\Theta}(x)$</p><blockquote><p>本文是从信号发出的角度去阐述，但似乎更多文章时站在信号接收的角度，但意思是一样的</p></blockquote><p>参考文章</p><p>[1]. <a href="https://zhuanlan.zhihu.com/p/28299430" target="_blank" rel="noopener">机器学习笔记（2）—— 神经网络</a></p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;神经网络中正向传播(Forward propagation)的向量化(Vectorized implementation)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img src=&quot;https://s2.ax1x.com/2019/03/23/AGrwuR.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="Machine Learning" scheme="http://yoursite.com/tags/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>sklearn中的函数classification_report</title>
    <link href="http://yoursite.com/2019/03/20/sklearn%E4%B8%AD%E7%9A%84%E5%87%BD%E6%95%B0classification-report/"/>
    <id>http://yoursite.com/2019/03/20/sklearn中的函数classification-report/</id>
    <published>2019-03-20T08:42:54.000Z</published>
    <updated>2019-03-24T01:51:45.678Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>classification_report函数主要用于显示主要分类指标的文本报告</p></blockquote><h4 id="1-前言"><a href="#1-前言" class="headerlink" title="1.前言"></a>1.前言</h4><p>在报告中显示每个类的精确度、召回率等信息（可以用来检测回归算法的准确度）。</p><a id="more"></a><h4 id="2-classification-report-参数"><a href="#2-classification-report-参数" class="headerlink" title="2.classification_report()参数"></a>2.classification_report()参数</h4><p>详见<a href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html" target="_blank" rel="noopener">官方文档</a>，这里只说几个重要参数</p><p><code>y_true</code>：1维数组，目标值</p><p><code>y_pred</code>：1维数组，分类器返回的估计值</p><p><code>label</code>：数组，报告中显示的类标签索引列表</p><p><code>target_names</code>：字符串列表，当和<code>label</code>匹配时作为<code>label</code>的名称</p><h4 id="3-classification-report-返回值"><a href="#3-classification-report-返回值" class="headerlink" title="3.classification_report()返回值"></a>3.classification_report()返回值</h4><blockquote><p>解释一下两个名词</p><p>正样本：与你所要研究的目的相关</p><p>负样本：与你所要研究的目的无关</p><p>举个例子：如果你要做一间教室里的人脸识别，那么正样本就是人脸，负样本就是课桌、门窗之类的</p></blockquote><p>TP(True Positive): 预测为正样本， 实际为正样本（预测正确）</p><p>FP(False Positive): 预测为正样本，  实际为负样本 （预测错误）</p><p>FN(False Negative): 预测为负样本，实际为正样本 （预测错误）</p><p>TN(True Negative): 预测为负样本， 实际为负样本 （预测正确）</p><p>精确度(precision)=正确预测的个数(TP)/预测为正样本的个数(TP+FP)</p><blockquote><p>检索结果中，都是你认为应该为正的样本（第二个字母都是P），但是其中有你判断正确的和判断错误的（第一个字母有T ，F）。</p></blockquote><p>召回率(recall)=正确预测值的个数(TP)/实际为正样本的个数(TP+FN)</p><blockquote><p>检索结果中，你判断为正的样本也确实为正的，以及那些没在检索结果中被你判断为负但是事实上是正的，或者说你没预测到的（FN）。</p></blockquote><p>F1值=2*精度*召回率/(精度+召回率)</p><p>不明白的话参考以下代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">4</span>]: <span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> classification_report</span><br><span class="line">   ...: y_true = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>]</span><br><span class="line">   ...: y_pred = [<span class="number">1</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">2</span>]</span><br><span class="line">   ...: labels =[<span class="number">1</span>,<span class="number">3</span>,<span class="number">2</span>]</span><br><span class="line">   ...: target_names = [<span class="string">'labels_1'</span>,<span class="string">'labels_2'</span>,<span class="string">'labels_3'</span>,<span class="string">'labels-4'</span>]</span><br><span class="line">   ...: print(classification_report(y_true,y_pred,labels=labels,target_names= t</span><br><span class="line">   ...: arget_names,digits=<span class="number">3</span>))</span><br><span class="line">   ...:</span><br><span class="line">              precision    recall  f1-score   support</span><br><span class="line"></span><br><span class="line">    labels_1      <span class="number">0.500</span>     <span class="number">1.000</span>     <span class="number">0.667</span>         <span class="number">1</span></span><br><span class="line">    labels_2      <span class="number">1.000</span>     <span class="number">0.667</span>     <span class="number">0.800</span>         <span class="number">3</span></span><br><span class="line">    labels_3      <span class="number">0.000</span>     <span class="number">0.000</span>     <span class="number">0.000</span>         <span class="number">1</span></span><br><span class="line"></span><br><span class="line">   micro avg      <span class="number">0.600</span>     <span class="number">0.600</span>     <span class="number">0.600</span>         <span class="number">5</span></span><br><span class="line">   macro avg      <span class="number">0.500</span>     <span class="number">0.556</span>     <span class="number">0.489</span>         <span class="number">5</span></span><br><span class="line">weighted avg      <span class="number">0.700</span>     <span class="number">0.600</span>     <span class="number">0.613</span>         <span class="number">5</span></span><br></pre></td></tr></table></figure><p>最右边<code>support</code>列为每个标签的出现次数(权重)。</p><p><code>micro avg</code>：计算所有数据中预测正确的值，比如这里是3/5=0.6</p><p><code>macro avg</code>：每个类别指标中的未加权平均值(一列)，比如准确率(precision)的<code>macro avg</code>是：$(0.5+1.0+0)/3=0.5$</p><p><code>weighted avg</code>：每个类别指标中的加权平均，比如准确率(precision)的<code>weighted avg</code>是：$(0.5<em>1+1.0</em>3+0*1)/3=0.7$  </p><h4 id="4-参考资料"><a href="#4-参考资料" class="headerlink" title="4.参考资料"></a>4.参考资料</h4><p>[1] <a href="http://www.cnblogs.com/akrusher/articles/6442839.html" target="_blank" rel="noopener">博客园</a></p><p>[2] <a href="https://blog.csdn.net/genghaihua/article/details/81155200" target="_blank" rel="noopener">CSDN博客</a></p><p>[3] <a href="https://blog.csdn.net/kancy110/article/details/74937469" target="_blank" rel="noopener">CSDN博客</a></p><p>[4] <a href="https://blog.csdn.net/guyubit/article/details/52276013" target="_blank" rel="noopener">CSDN博客——TP、TN、FP、FN解释说明</a></p><p>[5] <a href="https://www.libinx.com/2018/understanding-sklearn-classification-report/" target="_blank" rel="noopener">读懂 sklearn 的 classification_report</a></p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;classification_report函数主要用于显示主要分类指标的文本报告&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4 id=&quot;1-前言&quot;&gt;&lt;a href=&quot;#1-前言&quot; class=&quot;headerlink&quot; title=&quot;1.前言&quot;&gt;&lt;/a&gt;1.前言&lt;/h4&gt;&lt;p&gt;在报告中显示每个类的精确度、召回率等信息（可以用来检测回归算法的准确度）。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Python" scheme="http://yoursite.com/tags/Python/"/>
    
      <category term="Machine Learning" scheme="http://yoursite.com/tags/Machine-Learning/"/>
    
      <category term="sklearn" scheme="http://yoursite.com/tags/sklearn/"/>
    
  </entry>
  
  <entry>
    <title>Numpy中的矩阵乘法</title>
    <link href="http://yoursite.com/2019/03/19/Numpy%E4%B8%AD%E7%9A%84%E7%9F%A9%E9%98%B5%E4%B9%98%E6%B3%95/"/>
    <id>http://yoursite.com/2019/03/19/Numpy中的矩阵乘法/</id>
    <published>2019-03-19T03:38:01.000Z</published>
    <updated>2019-03-29T08:07:58.601Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>简单介绍一下Numpy中<code>.dot()</code>、<code>*</code>、<code>multiply()</code>和<code>@</code>的区别</p></blockquote><a id="more"></a><h4 id="1-np-multiply-函数"><a href="#1-np-multiply-函数" class="headerlink" title="1. np.multiply()函数"></a>1. np.multiply()函数</h4><p>数组/矩阵对应的位置相乘。</p><h4 id="2-np-dot-函数"><a href="#2-np-dot-函数" class="headerlink" title="2. np.dot()函数"></a>2. np.dot()函数</h4><p>2.1.  当数组/矩阵秩为1(即向量)时，执行点积。</p><p>2.2.  当数组/矩阵秩大于2时，执行矩阵乘法。</p><p>2.3.  .dot可以被数组对象调用，也可以通过numpy库调用（被matrix调用可以执行，但会报错）。</p><h4 id="3-星号-乘法运算"><a href="#3-星号-乘法运算" class="headerlink" title="3.  星号(*)乘法运算"></a>3.  星号(*)乘法运算</h4><p>3.1.  对数组执行对应位置相乘。</p><p>3.2.  对矩阵执行矩阵乘法。</p><h4 id="4-乘法运算"><a href="#4-乘法运算" class="headerlink" title="4. @乘法运算"></a>4. @乘法运算</h4><p>4.1.  对于矩阵乘法而言，完全等价于<code>.dot()</code>。</p><p>4.2.  区别在于，当a和b中有一个是标量时，只能用<code>.dot()</code>否则会报错。</p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;简单介绍一下Numpy中&lt;code&gt;.dot()&lt;/code&gt;、&lt;code&gt;*&lt;/code&gt;、&lt;code&gt;multiply()&lt;/code&gt;和&lt;code&gt;@&lt;/code&gt;的区别&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
      <category term="Python" scheme="http://yoursite.com/tags/Python/"/>
    
      <category term="Machine Learning" scheme="http://yoursite.com/tags/Machine-Learning/"/>
    
      <category term="Numpy" scheme="http://yoursite.com/tags/Numpy/"/>
    
  </entry>
  
  <entry>
    <title>Numpy中的Meshgrid</title>
    <link href="http://yoursite.com/2019/03/16/Numpy%E4%B8%AD%E7%9A%84Meshgrid/"/>
    <id>http://yoursite.com/2019/03/16/Numpy中的Meshgrid/</id>
    <published>2019-03-16T07:42:50.000Z</published>
    <updated>2019-04-26T08:06:55.959Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>Numpy是在利用Python进行科学计算和数据处理时肯定会使用到的模块(moudle)。</p><p>本文用比较通俗的语言讲解一下其中的Meshgrid函数</p></blockquote><h4 id="1-Meshgrid前言"><a href="#1-Meshgrid前言" class="headerlink" title="1.Meshgrid前言"></a>1.Meshgrid前言</h4><p>简单来说，<code>Meshgrid</code>就是在用两个坐标轴上的点在平面上画网格，当然也可以用三个坐标，但是为了方便理解，下面都用两个坐标轴举例。</p><a id="more"></a><h4 id="2-Meshgrid参数"><a href="#2-Meshgrid参数" class="headerlink" title="2.Meshgrid参数"></a>2.Meshgrid参数</h4><p>详情咨询<a href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.meshgrid.html" target="_blank" rel="noopener">官方文档</a>，最常用的就是传入两个一维数组</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x = np.linspace(<span class="number">-10</span>,<span class="number">10</span>,<span class="number">2</span>)</span><br><span class="line">y = np.linspace(<span class="number">-10</span>,<span class="number">10</span>,<span class="number">2</span>)</span><br><span class="line">xv,yv = meshgrid(x,y)</span><br></pre></td></tr></table></figure><h4 id="3-Meshgrid返回值"><a href="#3-Meshgrid返回值" class="headerlink" title="3.Meshgrid返回值"></a>3.Meshgrid返回值</h4><p>xv的返回值，列相等：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[[-10,10],</span><br><span class="line"> [-10,10]]</span><br></pre></td></tr></table></figure><p>yv的返回值，行相等：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[[-10,-10],</span><br><span class="line"> [10,10]]</span><br></pre></td></tr></table></figure><p>从上面很容易看出，Meshgrid实际上是返回两个<strong>矩阵</strong>，两个矩阵不同之处，下面用一张图来表示一目了然。</p><p><img src="https://s2.ax1x.com/2019/03/15/AEfxHJ.jpg" alt=""></p><p>可以看出就是通过两个矩阵的方式完成了网格的绘制。</p><h4 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h4><p>[1] <a href="https://zhuanlan.zhihu.com/p/33579211" target="_blank" rel="noopener">Python-Numpy模块Meshgrid函数</a></p><p>[2] <a href="https://zhuanlan.zhihu.com/p/33579211" target="_blank" rel="noopener">Numpy中Meshgrid函数介绍及2种应用场景</a></p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;Numpy是在利用Python进行科学计算和数据处理时肯定会使用到的模块(moudle)。&lt;/p&gt;
&lt;p&gt;本文用比较通俗的语言讲解一下其中的Meshgrid函数&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4 id=&quot;1-Meshgrid前言&quot;&gt;&lt;a href=&quot;#1-Meshgrid前言&quot; class=&quot;headerlink&quot; title=&quot;1.Meshgrid前言&quot;&gt;&lt;/a&gt;1.Meshgrid前言&lt;/h4&gt;&lt;p&gt;简单来说，&lt;code&gt;Meshgrid&lt;/code&gt;就是在用两个坐标轴上的点在平面上画网格，当然也可以用三个坐标，但是为了方便理解，下面都用两个坐标轴举例。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Python" scheme="http://yoursite.com/tags/Python/"/>
    
      <category term="Machine Learning" scheme="http://yoursite.com/tags/Machine-Learning/"/>
    
  </entry>
  
</feed>
